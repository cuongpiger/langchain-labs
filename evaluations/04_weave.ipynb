{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "vectordb_path = \"./\"\n",
    "resumes_path = \"docs\"\n",
    "huggingface_api = os.getenv(\"hugging_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stackops/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### In this code first we are loading the documents from Folder. Here you have\n",
    "### to provide the folder path and it will load the all documents.\n",
    "docs_loader = DirectoryLoader(\"/home/stackops/langchain-labs/data/vks/pdf/vi\")\n",
    "documents = docs_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### After Loading the documents we are splitting the documents into chunk with the help of RecursiveCharacterTextSplitter.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25178/3208077349.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=\"paraphrase-multilingual-mpnet-base-v2\")\n"
     ]
    }
   ],
   "source": [
    "### In this code we are using embeddings model to convert the chunk of document to vectors\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"paraphrase-multilingual-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents stored in the database\n"
     ]
    }
   ],
   "source": [
    "### This code will convert the docs to vectors and store the vectors into vector db.\n",
    "### Here we are using Chroma db as a vector database.\n",
    "### The Persist function will help us to store the vector db in local disk.\n",
    "vectordb = Chroma.from_documents(documents= docs,\n",
    "                    embedding= embeddings,\n",
    "                    persist_directory= vectordb_path)\n",
    "print(\"Documents stored in the database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import weave\n",
    "import asyncio\n",
    "import json\n",
    "import warnings\n",
    "from groq import Groq\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from weave import Model\n",
    "from langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = (\n",
    "    \"/home/stackops/secret/work/vngcloud/ai-platform/vertex-ai-credential.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: cuongpigerr.\n",
      "View Weave data at https://wandb.ai/cuongpigerr-vng-cloud/genai-application/weave\n"
     ]
    }
   ],
   "source": [
    "weave.init(project_name=\"genai-application\")\n",
    "persist_directory = \"./\"\n",
    "client = ChatVertexAI(model=\"gemini-1.5-flash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def generate_solution_with_llm(query: str, vector_db_content: str, model: str ):\n",
    "    user_input = f\"User query: {query}. Retrieved relevant data: {vector_db_content}\"\n",
    "\n",
    "    chat_completion = client.invoke(\n",
    "        [\n",
    "            SystemMessage(\"You are a helpful assistant. Based on the query and content, provide the solution.\"),\n",
    "            HumanMessage(user_input),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return chat_completion.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGModel(Model):\n",
    "    model_name: str = \"llama3-8b-8192\"\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str) -> dict:\n",
    "        embeddings = SentenceTransformerEmbeddings(model_name=\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "        vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "        vectorstore_retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "        retrieved_documents = vectorstore_retriever.get_relevant_documents(question)\n",
    "        retrieved_content = \" \".join([doc.page_content for doc in retrieved_documents])\n",
    "        answer = generate_solution_with_llm(\n",
    "            query=question,\n",
    "            vector_db_content=retrieved_content,\n",
    "            model=self.model_name,\n",
    "        )\n",
    "        return {'answer': answer, 'context': retrieved_content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your scoring function\n",
    "@weave.op()\n",
    "async def context_precision_score(question, model_output):\n",
    "    context_precision_prompt = \"\"\"Given the question, answer, and context, verify if the context was useful in arriving at the given answer. Give a verdict as \"1\" if useful and \"0\" if not, with RAW-TEXT JSON output.\n",
    "Output in only valid RAW-TEXT JSON format.\n",
    "\n",
    "question: {question}\n",
    "context: {context}\n",
    "answer: {answer}\n",
    "verdict: \"\"\"\n",
    "\n",
    "    prompt = context_precision_prompt.format(\n",
    "        question=question,\n",
    "        context=model_output['context'],\n",
    "        answer=model_output['answer'],\n",
    "    )\n",
    "\n",
    "    response = client.invoke(\n",
    "        [\n",
    "            HumanMessage(prompt),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    response_message = response.content.strip()\n",
    "    print(response_message)\n",
    "\n",
    "    try:\n",
    "        response_json = json.loads(response_message)\n",
    "        verdict = int(response_json[\"verdict\"]) == 1\n",
    "    except (json.JSONDecodeError, KeyError, ValueError):\n",
    "        verdict = False\n",
    "\n",
    "    return {\"verdict\": verdict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 'model_output' key for compatibility with older scorers. Please update scorers to use 'output' parameter.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    {\"question\": \"VKS l√† g√¨?\"},\n",
    "]\n",
    "\n",
    "model = RAGModel()\n",
    "\n",
    "evaluation = weave.Evaluation(dataset=questions, scorers=[context_precision_score])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/cuongpigerr-vng-cloud/genai-application/r/call/01953634-ecf6-7431-98f8-3a88c9b2bf86\n",
      "üç© https://wandb.ai/cuongpigerr-vng-cloud/genai-application/r/call/01953634-ecf3-74d2-8e45-6c03e95f98c2\n",
      "üç© https://wandb.ai/cuongpigerr-vng-cloud/genai-application/r/call/01953634-ecf4-76a0-af83-e79f78a341c5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25178/1554766186.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
      "/tmp/ipykernel_25178/1554766186.py:9: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_documents = vectorstore_retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/cuongpigerr-vng-cloud/genai-application/r/call/01953634-fbd6-7793-9849-26fc710337d8\n",
      "üç© https://wandb.ai/cuongpigerr-vng-cloud/genai-application/r/call/01953634-fbfb-7321-9264-d1115fffe53a\n",
      "üç© https://wandb.ai/cuongpigerr-vng-cloud/genai-application/r/call/01953634-fbfd-7753-b8be-ccaefe7b385b\n",
      "üç© https://wandb.ai/cuongpigerr-vng-cloud/genai-application/r/call/01953635-0320-7030-85b0-4d3ba44cb679\n",
      "üç© https://wandb.ai/cuongpigerr-vng-cloud/genai-application/r/call/01953635-0321-7262-8b0c-3e4a30262bdf\n",
      "{\"verdict\": \"1\"}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m1\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'context_precision_score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'verdict'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.674119472503662</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'context_precision_score'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'verdict'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m5.674119472503662\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'context_precision_score': {'verdict': {'true_count': 1,\n",
       "   'true_fraction': 1.0}},\n",
       " 'model_latency': {'mean': 5.674119472503662}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/cuongpigerr-vng-cloud/genai-application/r/call/01953635-0512-7c63-aee9-e982436358c8\n"
     ]
    }
   ],
   "source": [
    "await evaluation.evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./tmp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
