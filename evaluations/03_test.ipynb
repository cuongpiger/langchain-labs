{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision\n",
    ")\n",
    "from ragas.integrations.llama_index import evaluate\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.vertex import Vertex\n",
    "from google.oauth2 import service_account\n",
    "from ragas import SingleTurnSample, EvaluationDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/stackops/secret/work/vngcloud/ai-platform/vertex-ai-credential.json\"\n",
    "credentials: service_account.Credentials = (\n",
    "    service_account.Credentials.from_service_account_file(filename)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "Settings.llm = Vertex(\n",
    "    model=\"gemini-1.5-flash\", project=credentials.project_id, credentials=credentials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/home/stackops/langchain-labs/data/vks/pdf/vi\"\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(dir_path).load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    \"Can you provide a concise description of the TinyLlama model?\",\n",
    "    \"I would like to know the speed optimizations that TinyLlama has made.\",\n",
    "    \"Why TinyLlama uses Grouped-query Attention?\",\n",
    "    \"Is the TinyLlama model open source?\",\n",
    "    \"Tell me about starcoderdata dataset\",\n",
    "]\n",
    "eval_answers = [\n",
    "    \"TinyLlama is a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes.\",\n",
    "    \"During training, our codebase has integrated FSDP to leverage multi-GPU and multi-node setups efficiently. Another critical improvement is the integration of Flash Attention, an optimized attention mechanism. We have replaced the fused SwiGLU module from the xFormers (Lefaudeux et al., 2022) repository with the original SwiGLU module, further enhancing the efficiency of our codebase. With these features, we can reduce the memory footprint, enabling the 1.1B model to fit within 40GB of GPU RAM.\",  \n",
    "    \"To reduce memory bandwidth overhead and speed up inference, we use grouped-query attention in our model. We have 32 heads for query attention and use 4 groups of key-value heads. With this technique, the model can share key and value representations across multiple heads without sacrificing much performance\",\n",
    "    \"Yes, TinyLlama is open-source\",\n",
    "    \"This dataset was collected to train StarCoder (Li et al., 2023), a powerful opensource large code language model. It comprises approximately 250 billion tokens across 86 programming languages. In addition to code, it also includes GitHub issues and text-code pairs that involve natural languages.\",\n",
    "]\n",
    "eval_answers = [[a] for a in eval_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1\n",
    "sample1 = SingleTurnSample(\n",
    "    user_input=\"What is the capital of Germany?\",\n",
    "    retrieved_contexts=[\"Berlin is the capital and largest city of Germany.\"],\n",
    "    response=\"The capital of Germany is Berlin.\",\n",
    "    reference=\"Berlin\",\n",
    ")\n",
    "\n",
    "# Sample 2\n",
    "sample2 = SingleTurnSample(\n",
    "    user_input=\"Who wrote 'Pride and Prejudice'?\",\n",
    "    retrieved_contexts=[\"'Pride and Prejudice' is a novel by Jane Austen.\"],\n",
    "    response=\"'Pride and Prejudice' was written by Jane Austen.\",\n",
    "    reference=\"Jane Austen\",\n",
    ")\n",
    "\n",
    "# Sample 3\n",
    "sample3 = SingleTurnSample(\n",
    "    user_input=\"What's the chemical formula for water?\",\n",
    "    retrieved_contexts=[\"Water has the chemical formula H2O.\"],\n",
    "    response=\"The chemical formula for water is H2O.\",\n",
    "    reference=\"H2O\",\n",
    ")\n",
    "\n",
    "dataset = EvaluationDataset(samples=[sample1, sample2, sample3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Query Engine: 100%|██████████| 3/3 [00:00<00:00,  4.89it/s]\n",
      "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]Exception raised in Job[2]: ValueError(Unknown field for GenerationConfig: n)\n",
      "Evaluating:   8%|▊         | 1/12 [00:44<08:10, 44.59s/it]Exception raised in Job[4]: ValueError(Unknown field for GenerationConfig: n)\n",
      "Evaluating:  17%|█▋        | 2/12 [00:57<04:20, 26.02s/it]Exception raised in Job[11]: ValueError(Unknown field for GenerationConfig: n)\n",
      "Evaluating:  25%|██▌       | 3/12 [01:04<02:34, 17.18s/it]Exception raised in Job[9]: ValueError(Unknown field for GenerationConfig: n)\n",
      "Evaluating:  33%|███▎      | 4/12 [01:12<01:48, 13.58s/it]Exception raised in Job[1]: ValueError(Unknown field for GenerationConfig: n)\n",
      "Evaluating:  42%|████▏     | 5/12 [01:33<01:55, 16.47s/it]Exception raised in Job[8]: ValueError(Unknown field for GenerationConfig: n)\n",
      "Evaluating:  50%|█████     | 6/12 [01:40<01:18, 13.08s/it]Exception raised in Job[6]: ValueError(Unknown field for GenerationConfig: n)\n",
      "Evaluating:  58%|█████▊    | 7/12 [01:43<00:48,  9.74s/it]Exception raised in Job[5]: ValueError(Unknown field for GenerationConfig: n)\n",
      "Evaluating:  67%|██████▋   | 8/12 [01:46<00:30,  7.57s/it]Exception raised in Job[10]: ValueError(Unknown field for GenerationConfig: n)\n",
      "Evaluating:  75%|███████▌  | 9/12 [01:56<00:25,  8.51s/it]Exception raised in Job[0]: ValueError(Unknown field for GenerationConfig: n)\n",
      "Evaluating:  83%|████████▎ | 10/12 [02:02<00:14,  7.50s/it]Exception raised in Job[3]: ValueError(Unknown field for GenerationConfig: n)\n",
      "Evaluating:  92%|█████████▏| 11/12 [02:04<00:06,  6.09s/it]Exception raised in Job[7]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 12/12 [03:00<00:00, 15.00s/it]\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(dataset=dataset, metrics=metrics, query_engine=query_engine, llm=Vertex(\n",
    "    model=\"gemini-1.5-flash\", project=credentials.project_id, credentials=credentials\n",
    "), embeddings=HuggingFaceEmbedding(model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"))\n",
    "result.to_pandas().to_csv('./test.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': nan, 'answer_relevancy': nan, 'context_precision': nan, 'context_recall': nan}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
