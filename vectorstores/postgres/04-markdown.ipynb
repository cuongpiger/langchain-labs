{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_path = \"example.md\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "\n",
    "data = loader.load()\n",
    "assert len(data) == 1\n",
    "assert isinstance(data[0], Document)\n",
    "readme_content = data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VKS là gì?\\n\\nVKS (VNGCloud Kubernetes Service) là một dịch vụ được quản lý trên VNGCloud giúp bạn đơn giản hóa quá trình triển khai và quản lý các ứng dụng dựa trên container. Kubernetes là một nền tảng mã nguồn mở được phát triển bởi Google, được sử dụng rộng rãi để quản lý và triển khai các ứng dụng container trên môi trường phân tán.\\n\\nNhững điểm nổi bật của VKS\\n\\nQuản lý Control Plane hoàn toàn tự động (Fully Managed control plane): VKS sẽ giải phóng bạn khỏi gánh nặng quản lý Control Plane của Kubernetes, giúp bạn tập trung vào việc phát triển ứng dụng.\\n\\nHỗ trợ các phiên bản Kubernetes mới nhất: VKS luôn cập nhật những phiên bản Kubernetes mới nhất (minor version từ 1.27, 1.28, 1.29) để đảm bảo bạn luôn tận dụng được những tính năng tiên tiến nhất.\\n\\nKubernetes Networking: VKS tích hợp Calico CNI, mang lại tính hiệu quả và bảo mật cao.\\n\\nUpgrade seamlessly: VKS hỗ trợ nâng cấp giữa các phiên bản Kubernetes một cách dễ dàng và nhanh chóng, giúp bạn luôn cập nhật những cải tiến mới nhất.\\n\\nScaling & Healing Automatically: VKS tự động mở rộng Node group khi cần thiết và tự động sửa lỗi khi node gặp vấn đề, giúp bạn tiết kiệm thời gian và công sức quản lý.\\n\\nGiảm chi phí và nâng cao độ tin cậy: VKS triển khai Control Plane của Kubernetes ở chế độ sẵn sàng cao và hoàn toàn miễn phí, giúp bạn tiết kiệm chi phí và nâng cao độ tin cậy cho hệ thống.\\n\\nTích hợp Blockstore Native (Container Storage Interface - CSI): VKS cho phép bạn quản lý Blockstore thông qua YAML của Kubernetes, cung cấp lưu trữ bền vững cho container và hỗ trợ các tính năng quan trọng như thay đổi kích thước, thay đổi IOPS và snapshot volume.\\n\\nTích hợp Load Balancer (Network Load Balancer, Application Load Balancer) thông qua các driver được tích hợp sẵn như VNGCloud Controller Mananger, VNGCloud Ingress Controller: VKS cung cấp khả năng quản lý NLB/ALB thông qua YAML của Kubernetes, giúp bạn dễ dàng expose Service trong Kubernetes ra bên ngoài.\\n\\nNâng cao bảo mật: VKS cho phép bạn tạo Private Node Group với chỉ Private IP và kiểm soát quyền truy cập vào cluster thông qua tính năng Whitelist IP, đảm bảo an toàn cho hệ thống của bạn.\\n\\nNgoài ra, VKS còn có các ưu điểm sau:\\n\\nDễ sử dụng: VKS cung cấp giao diện đơn giản và dễ sử dụng.\\n\\nChi phí hợp lý: VKS cung cấp mức giá cạnh tranh cho các dịch vụ của mình.\\n\\nRegion\\n\\nHiện tại, trên VKS, chúng tôi đang cung cấp cho bạn 2 cơ sở hạ tầng riêng biệt được đặt tại Hà Nội và Hồ Chí Minh. Bạn có thể lựa chọn sử dụng VKS trên mỗi region tùy thuộc vào vị trí và nhu cầu thực tế của bạn. Đối với 2 farm HCM03, HAN01, các thông số cụ thể cho mỗi region được chúng tôi cung cấp như sau:\\n\\nFarm Domain HCM03 https://vks.console.vngcloud.vn HAN01 https://vks-han-1.console.vngcloud.vn'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./../../data/vks/security-group.md', './../../data/vks/README.md', './../../data/vks/vks-la-gi.md', './../../data/vks/cach-tinh-gia.md', './../../data/vks/mo-hinh-hoat-dong.md', './../../data/vks/su-dung-vks-voi-terraform.md', './../../data/vks/tham-khao-them/README.md', './../../data/vks/tham-khao-them/danh-sach-system-image-dang-ho-tro.md', './../../data/vks/tham-khao-them/danh-sach-flavor-dang-ho-tro.md', './../../data/vks/node-groups/README.md', './../../data/vks/node-groups/upgrading-node-group-version.md', './../../data/vks/node-groups/lable-va-taint.md', './../../data/vks/node-groups/auto-healing.md', './../../data/vks/node-groups/auto-scaling.md', './../../data/vks/clusters/README.md', './../../data/vks/clusters/whitelist.md', './../../data/vks/clusters/public-cluster-va-private-cluster.md', './../../data/vks/clusters/upgrading-control-plane-version.md', './../../data/vks/clusters/stop-poc.md', './../../data/vks/migration/migrate-cluster-from-vks-to-vks.md', './../../data/vks/migration/README.md', './../../data/vks/migration/migrate-cluster-from-other-to-vks.md', './../../data/vks/migration/gioi-han-va-han-che.md', './../../data/vks/migration/migration-cluster-from-vcontainer-to-vks.md', './../../data/vks/storage/README.md', './../../data/vks/storage/lam-viec-voi-container-storage-interface-csi/integrate-with-container-storage-interface-csi.md', './../../data/vks/storage/lam-viec-voi-container-storage-interface-csi/README.md', './../../data/vks/storage/lam-viec-voi-container-storage-interface-csi/gioi-han-va-han-che-csi.md', './../../data/vks/bat-dau-voi-vks/integrate-with-container-storage-interface-csi.md', './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-cluster-thong-qua-vi-poc.md', './../../data/vks/bat-dau-voi-vks/expose-mot-service-thong-qua-vlb-layer7.md', './../../data/vks/bat-dau-voi-vks/README.md', './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-private-cluster.md', './../../data/vks/bat-dau-voi-vks/lam-viec-voi-nvidia-gpu-nodegroups.md', './../../data/vks/bat-dau-voi-vks/huong-dan-cai-dat-va-cau-hinh-cong-cu-kubectl-trong-kubernetes.md', './../../data/vks/bat-dau-voi-vks/su-dung-terraform-de-khoi-tao-cluster-va-node-group.md', './../../data/vks/bat-dau-voi-vks/expose-mot-service-thong-qua-vlb-layer7/tu-dong-quan-ly-certificate-trong-vks-voi-nginx-ingress-controller-cert-manager-va-lets-encrypt.md', './../../data/vks/bat-dau-voi-vks/expose-mot-service-thong-qua-vlb-layer4/README.md', './../../data/vks/bat-dau-voi-vks/expose-mot-service-thong-qua-vlb-layer4/preserve-source-ip-khi-su-dung-vlb-layer4-va-nginx-ingress-controller.md', './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-public-cluster/README.md', './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-public-cluster/khoi-tao-mot-public-cluster-voi-public-node-group.md', './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-public-cluster/khoi-tao-mot-public-cluster-voi-private-node-group/README.md', './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-public-cluster/khoi-tao-mot-public-cluster-voi-private-node-group/pfsense-as-a-nat-gateway.md', './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-public-cluster/khoi-tao-mot-public-cluster-voi-private-node-group/palo-alto-as-a-nat-gateway.md', './../../data/vks/network/README.md', './../../data/vks/network/lam-viec-voi-network-load-balancing-nlb/README.md', './../../data/vks/network/lam-viec-voi-network-load-balancing-nlb/integrate-with-network-load-balancer.md', './../../data/vks/network/lam-viec-voi-network-load-balancing-nlb/cau-hinh-cho-mot-network-load-balancer.md', './../../data/vks/network/lam-viec-voi-network-load-balancing-nlb/gioi-han-va-han-che-nlb.md', './../../data/vks/network/cni/README.md', './../../data/vks/network/cni/su-dung-cni-cilium-vpc-native-routing.md', './../../data/vks/network/cni/su-dung-cni-cilium-overlay.md', './../../data/vks/network/cni/su-dung-cni-calico-overlay.md', './../../data/vks/network/lam-viec-voi-application-load-balancer-alb/ingress-for-an-application-load-balancer.md', './../../data/vks/network/lam-viec-voi-application-load-balancer-alb/cau-hinh-cho-mot-application-load-balancer.md', './../../data/vks/network/lam-viec-voi-application-load-balancer-alb/README.md', './../../data/vks/network/lam-viec-voi-application-load-balancer-alb/gioi-han-va-han-che-alb.md', './../../data/vks/thong-bao-va-cap-nhat/README.md', './../../data/vks/thong-bao-va-cap-nhat/release-notes.md', './../../data/vks/upgrade-kubernetes-version/manually-upgrade.md', './../../data/vks/upgrade-kubernetes-version/README.md', './../../data/vks/upgrade-kubernetes-version/automatically-upgrade.md', './../../data/vks/upgrade-kubernetes-version/phien-ban-ho-tro-kubernetes.md', './../../data/vks/giam-sat/README.md', './../../data/vks/giam-sat/metrics.md']\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"./../../data/vks\"  # Change this to your directory\n",
    "md_files = []\n",
    "\n",
    "for root, _, files in os.walk(directory_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".md\"):\n",
    "            md_files.append(os.path.join(root, file))\n",
    "\n",
    "print(md_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for file in md_files:\n",
    "    loader = UnstructuredMarkdownLoader(file)\n",
    "\n",
    "    data = loader.load()\n",
    "    docs.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './../../data/vks/security-group.md'}, page_content='Security Group\\n\\nSecurity Group đóng vai trò như một firewall giúp bạn kiểm soát lưu lượng truy cập ra vào máy chủ (VM). Trên hệ thống VKS, để đảm bảo cluster hoạt động an toàn và hiệu quả, các Security Group mặc định được thiết lập để cho phép các truy cập cần thiết cho hoạt động nội bộ của cluster. Việc tự động tạo Security Group giúp đơn giản hóa quá trình triển khai cluster và đảm bảo rằng cluster được bảo vệ ngay từ đầu. Cụ thể, khi bạn thực hiện khởi tạo một Cluser, chúng tôi sẽ tự động khởi tạo một vài Security Group với các thông số như sau:\\n\\nSecurity group mặc định được tạo tự động cho tất cả Cluster\\n\\nMỗi Cluster được tạo ra trong hệ thống VKS, chúng tôi sẽ tự động tạo một Security Group. Security group này sẽ bao gồm:\\n\\nInbound:\\n\\nProtocol Ether type Port range Source Ý nghĩa TCP IPv4 30000-32767 CIDR của VPC mà bạn sử dụng cho Cluster. Security group rule sử dụng cho TCP Node Port Services UDP IPv4 30000-32767 CIDR của VPC mà bạn sử dụng cho Cluster. Security group rule sử dụng cho UDP Node Port Services TCP IPv4 10250 External IP của Load Balancer sử dụng cho Cluster. Security group rule sử dụng cho Kubelet API control-plane TCP IPv4 10250 CIDR của VPC mà bạn sử dụng cho Cluster. Security group rule sử dụng cho Kubelet API control-plane TCP IPv4 179 CIDR của VPC mà bạn sử dụng cho Cluster. Security group rule sử dụng cho Kubelet API control-plane 4 IPv4 1-65535 CIDR của VPC mà bạn sử dụng cho Cluster. Security group rule sử dụng cho Calico IP-in-IP TCP IPv4 5473 CIDR của VPC mà bạn sử dụng cho Cluster. Security group rule sử dụng cho Calico Typha\\n\\nOutbound\\n\\nProtocol Ether type Port range Destination Ý nghĩa ANY IPv4 0-65535 0.0.0.0/0 Rule mặc định của tất cả Security group ANY IPv6 0-65535 ::/0 Rule mặc định của tất cả Security group\\n\\nSecurity group được tạo tự động bởi VNGCLOUD LoadBalancer Controller\\n\\nKhi bạn sử dụng VNGCloud LoadBalancer Controller để tích hợp Network Load Balancer với Cluster trên hệ thống VKS, chúng tôi sẽ tự động tạo một Security Group. Security group này sẽ bao gồm:\\n\\nInbound:\\n\\nProtocol Ether type Port range Source TCP, UDP hoặc ICMP IPv4 Port của Service Subnet Mask của Subnet mà bạn sử dụng cho Cluster.\\n\\nOutbound:\\n\\nProtocol Ether type Port range Destination Ý nghĩa ANY IPv4 0-65535 0.0.0.0/0 Rule mặc định của tất cả Security group ANY IPv6 0-65535 ::/0 Rule mặc định của tất cả Security group\\n\\nSecurity group được tạo tự động bởi VNGCLOUD Ingress Controller\\n\\nKhi bạn sử dụng VNGCloud Ingress Controller để tích hợp Application Load Balancer với Cluster trên hệ thống VKS, chúng tôi sẽ tự động tạo một Security Group. Security group này sẽ bao gồm:\\n\\nInbound:\\n\\nProtocol Ether type Port range Source TCP IPv4 Port của Service Subnet Mask của Subnet mà bạn sử dụng cho Cluster.\\n\\nOutbound:\\n\\nProtocol Ether type Port range Destination Ý nghĩa ANY IPv4 0-65535 0.0.0.0/0 Rule mặc định của tất cả Security group ANY IPv6 0-65535 ::/0 Rule mặc định của tất cả Security group\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nCác Security Group mặc định được thiết lập để đáp ứng các nhu cầu bảo mật cơ bản của cluster. Nếu bạn sửa hoặc xóa các Security Group được tạo sẵn cho cluster, có thể dẫn đến các vấn đề về kết nối và truy cập giữa các node trong cluster hoặc cluster có thể không hoạt động chính xác hoặc thậm chí không thể khởi động được. Để đảm bảo tính ổn định và bảo mật của cluster, hệ thống sẽ tự động reset các Security Group về cài đặt mặc định sau mỗi khoảng thời gian cố định. {% endhint %}'),\n",
       " Document(metadata={'source': './../../data/vks/README.md'}, page_content='VKS\\n\\nVKS (VNGCloud Kubernetes Service) là một dịch vụ được quản lý trên VNGCloud giúp bạn đơn giản hóa quá trình triển khai và quản lý các ứng dụng dựa trên container. Kubernetes là một nền tảng mã nguồn mở được phát triển bởi Google, được sử dụng rộng rãi để quản lý và triển khai các ứng dụng container trên môi trường phân tán.\\n\\n{% embed url=\"https://www.youtube.com/watch?v=t272u0uU8dU\" %}'),\n",
       " Document(metadata={'source': './../../data/vks/vks-la-gi.md'}, page_content='VKS là gì?\\n\\nVKS (VNGCloud Kubernetes Service) là một dịch vụ được quản lý trên VNGCloud giúp bạn đơn giản hóa quá trình triển khai và quản lý các ứng dụng dựa trên container. Kubernetes là một nền tảng mã nguồn mở được phát triển bởi Google, được sử dụng rộng rãi để quản lý và triển khai các ứng dụng container trên môi trường phân tán.\\n\\nNhững điểm nổi bật của VKS\\n\\nQuản lý Control Plane hoàn toàn tự động (Fully Managed control plane): VKS sẽ giải phóng bạn khỏi gánh nặng quản lý Control Plane của Kubernetes, giúp bạn tập trung vào việc phát triển ứng dụng.\\n\\nHỗ trợ các phiên bản Kubernetes mới nhất: VKS luôn cập nhật những phiên bản Kubernetes mới nhất (minor version từ 1.27, 1.28, 1.29) để đảm bảo bạn luôn tận dụng được những tính năng tiên tiến nhất.\\n\\nKubernetes Networking: VKS tích hợp Calico CNI, mang lại tính hiệu quả và bảo mật cao.\\n\\nUpgrade seamlessly: VKS hỗ trợ nâng cấp giữa các phiên bản Kubernetes một cách dễ dàng và nhanh chóng, giúp bạn luôn cập nhật những cải tiến mới nhất.\\n\\nScaling & Healing Automatically: VKS tự động mở rộng Node group khi cần thiết và tự động sửa lỗi khi node gặp vấn đề, giúp bạn tiết kiệm thời gian và công sức quản lý.\\n\\nGiảm chi phí và nâng cao độ tin cậy: VKS triển khai Control Plane của Kubernetes ở chế độ sẵn sàng cao và hoàn toàn miễn phí, giúp bạn tiết kiệm chi phí và nâng cao độ tin cậy cho hệ thống.\\n\\nTích hợp Blockstore Native (Container Storage Interface - CSI): VKS cho phép bạn quản lý Blockstore thông qua YAML của Kubernetes, cung cấp lưu trữ bền vững cho container và hỗ trợ các tính năng quan trọng như thay đổi kích thước, thay đổi IOPS và snapshot volume.\\n\\nTích hợp Load Balancer (Network Load Balancer, Application Load Balancer) thông qua các driver được tích hợp sẵn như VNGCloud Controller Mananger, VNGCloud Ingress Controller: VKS cung cấp khả năng quản lý NLB/ALB thông qua YAML của Kubernetes, giúp bạn dễ dàng expose Service trong Kubernetes ra bên ngoài.\\n\\nNâng cao bảo mật: VKS cho phép bạn tạo Private Node Group với chỉ Private IP và kiểm soát quyền truy cập vào cluster thông qua tính năng Whitelist IP, đảm bảo an toàn cho hệ thống của bạn.\\n\\nNgoài ra, VKS còn có các ưu điểm sau:\\n\\nDễ sử dụng: VKS cung cấp giao diện đơn giản và dễ sử dụng.\\n\\nChi phí hợp lý: VKS cung cấp mức giá cạnh tranh cho các dịch vụ của mình.\\n\\nRegion\\n\\nHiện tại, trên VKS, chúng tôi đang cung cấp cho bạn 2 cơ sở hạ tầng riêng biệt được đặt tại Hà Nội và Hồ Chí Minh. Bạn có thể lựa chọn sử dụng VKS trên mỗi region tùy thuộc vào vị trí và nhu cầu thực tế của bạn. Đối với 2 farm HCM03, HAN01, các thông số cụ thể cho mỗi region được chúng tôi cung cấp như sau:\\n\\nFarm Domain HCM03 https://vks.console.vngcloud.vn HAN01 https://vks-han-1.console.vngcloud.vn'),\n",
       " Document(metadata={'source': './../../data/vks/cach-tinh-gia.md'}, page_content='Cách tính giá\\n\\nĐối với VKS chi phí Managed Control Plane là hoàn toàn miễn phí. Bạn chỉ cần chi trả cho các resource khác mà thực tế bạn sử dụng bao gồm:\\n\\nĐối với Public Cluster, bạn cần chi trả cho:\\n\\nTất cả các node có trong Cluster (VM). Chi tiết cách tính giá của vServer bạn có thể tham khảo thêm tại đây.\\n\\nLoad Balancer được integrated vào Cluster của bạn. Chi tiết cách tính giá của Load Balancer bạn có thể tham khảo thêm tại đây.\\n\\nPersistent Volume, Snapshot được integrated vào Cluster của bạn. Chi tiết cách tính giá của Volume bạn có thể tham khảo tại đây.\\n\\nĐối với Private Cluster, bạn cần chi trả cho:\\n\\nTất cả các node có trong Cluster (VM). Chi tiết cách tính giá của vServer bạn có thể tham khảo thêm tại đây.\\n\\nLoad Balancer được integrated vào Cluster của bạn. Chi tiết cách tính giá của Load Balancer bạn có thể tham khảo thêm tại đây.\\n\\nPersistent Volume, Snapshot được integrated vào Cluster của bạn. Chi tiết cách tính giá của Volume bạn có thể tham khảo tại đây.\\n\\nNgoài ra, khi bạn tạo một private cluster, hệ thống tự động tạo 4 endpoints giúp kết nối với các dịch vụ khác trên VNG Cloud bao gồm:\\n\\nEndpoint để kết nối tới dịch vụ IAM (Endpoint Name: vks-iam-endpoint-...)\\n\\nEndpoint để kết nối tới dịch vụ vCR (Endpoint Name: vks-vcr-endpoint-...)\\n\\nEndpoint để kết nối tới dịch vụ vServer (Endpoint Name: vks-vserver-endpoint-...)\\n\\nEndpoint để kết nối tới dịch vụ vStorage (Endpoint Name: vks-vstorage-endpoint-...)\\n\\nViệc sử dụng private cluster sẽ phát sinh thêm chi phí cho 4 private service endpoint này, nhưng nó mang lại nhiều lợi ích về bảo mật cho dự án của bạn. Bạn hãy cân nhắc kỹ lưỡng các yếu tố để đưa ra quyết định sử dụng public hay private cho cluster của mình.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nĐể đảm bảo Cluster của bạn hoạt động ổn định, chúng tôi đã tự động thiết lập Auto-renew cho tất cả các resource trên Cluster của bạn. Trước ngày hết hạn của các resource, hãy đảm bảo số dư credit của bạn đủ để hệ thống có thể thực hiện Auto-renew thành công. {% endhint %}'),\n",
       " Document(metadata={'source': './../../data/vks/mo-hinh-hoat-dong.md'}, page_content='Mô hình hoạt động\\n\\nBên dưới là các concepts hiện tại VKS đang cung cấp cho bạn:\\n\\n1. Public Cluster\\n\\nKhi bạn khởi tạo một Public Cluster với Public Node Group, hệ thống VKS sẽ:\\n\\nTạo VM có Floating IP ( tức có IP Public). Lúc này các VM (Node) này có thể join trực tiếp vào cụm K8S thông qua Public IP này. Bằng cách sử dụng Public Cluster và Public Node Group, bạn có thể dễ dàng tạo các cụm Kubernetes và thực hiện expose service mà không cần sử dụng Load Balancer. Việc này sẽ góp phần tiết kiệm chi phí cho cụm của bạn.\\n\\nKhi bạn khởi tạo một Public Cluster với Private Node Group, hệ thống VKS sẽ:\\n\\nTạo VM không có Floating IP ( tức không có IP Public). Lúc này các VM (Node) này không thể join trực tiếp vào cụm K8S. Để các VM này có thể join vào cụm K8S, bạn cần phải sử dụng một NAT Gateway (NATGW). NATGW hoạt động như một trạm chuyển tiếp, cho phép các VM kết nối với cụm K8S mà không cần IP Public. Với VNG Cloud, chúng tôi khuyến cáo bạn sử dụng Pfsense hoặc Palo Alto như một NATGW cho Cluster của bạn. Pfsense sẽ giúp bạn quản lý lưu lượng mạng đến và đi (inbound và outbound traffic) một cách hiệu quả, đảm bảo an ninh mạng và quản lý truy cập. Bên cạnh đó, việc sử dụng Private Node Group sẽ giúp bạn kiểm soát các ứng dụng trong cụm được bảo mật hơn, cụ thể bạn có thể thực hiện giới hạn quyền truy cập control plane thông qua tính năng Whitelist IP.\\n\\n2. Private Cluster\\n\\nKhi bạn khởi tạo một Private Cluster với Public/ Private Node Group, hệ thống VKS sẽ:\\n\\nĐể nâng cao bảo mật cho cluster của bạn, chúng tôi đã cho ra mắt mô hình private cluster. Tính năng Private Cluster giúp cho cụm K8S của bạn được bảo mật nhất có thể, mọi kết nối hoàn toàn là private từ kết nối giữa nodes tới control plane, kết nối từ client tới control plane, hay kết nối từ nodes tới các sản phẩm dịch vụ khác trong VNG Cloud như: vStorage, vCR, vMonitor, VNGCloud APIs,...Private Cluster là lựa chọn lý tưởng cho các dịch vụ yêu cầu kiểm soát truy cập chặt chẽ, đảm bảo tuân thủ các quy định về bảo mật và quyền riêng tư dữ liệu.\\n\\n3. So sánh giữa việc sử dụng Public Cluster và Private Cluster\\n\\nDưới đây là bảng so sánh giữa việc tạo và sử dụng Public Cluster và Private Cluster trên hệ thống VKS:\\n\\nTiêu chí Public Cluster Private Cluster Kết nối Sử dụng địa chỉ Public IP để giao tiếp giữa nodes và control plane, giữa client và control plane, giữa nodes và các dịch vụ khác trong VNG Cloud. Sử dụng địa chỉ Private IP để giao tiếp giữa nodes và control plane, giữa client và control plane, giữa nodes và các dịch vụ khác trong VNG Cloud. Bảo mật Bảo mật trung bình do các kết nối sử dụng Public IP. Bảo mật cao hơn với tất cả kết nối đều private và giới hạn truy cập. Quản lý truy cập Khó kiểm soát hơn, có thể quản lý truy cập thông qua tính năng Whitelist Kiểm soát truy cập chặt chẽ, mọi kết nối đều nằm trong mạng private của VNG Cloud, từ đó giảm thiểu nguy cơ từ các cuộc tấn công mạng từ bên ngoài. Khả năng mở rộng (AutoScaling) Dễ dàng mở rộng thông qua tính năng Auto Scaling . Dễ dàng mở rộng thông qua tính năng Auto Scaling . Khả năng tự hồi phục (AutoHealing) Tự động phát hiện lỗi và khởi động lại node ( Auto Healing ) Tự động phát hiện lỗi và khởi động lại node ( Auto Healing ) Khả năng truy cập từ bên ngoài Dễ dàng truy cập từ bất kỳ đâu với internet. Truy cập từ bên ngoài phải qua các giải pháp bảo mật khác. Cấu hình và triển khai Đơn giản hơn do không yêu cầu thiết lập mạng nội bộ. Phức tạp hơn, yêu cầu cấu hình mạng private và bảo mật. Chi phí Thường thấp hơn do không cần thiết lập cơ sở hạ tầng bảo mật phức tạp. Chi phí cao hơn do yêu cầu thêm các thành phần bảo mật và quản lý. Cụ thể, khi sử dụng private cluster, bạn cần chi trả chi phí cho 4 private service endpoint được tạo tự động để kết nối tới các dịch vụ trên VNG Cloud. Tính linh hoạt Cao, dễ dàng thay đổi và truy cập các dịch vụ. Linh hoạt hơn trong các ứng dụng yêu cầu bảo mật, nhưng ít linh hoạt hơn cho các ứng dụng yêu cầu truy cập từ bên ngoài.\\n\\nDo đó:\\n\\nPublic Cluster: Phù hợp cho các ứng dụng không yêu cầu bảo mật cao và cần sự linh hoạt, truy cập từ nhiều địa điểm. Dễ dàng triển khai và quản lý nhưng có rủi ro bảo mật cao hơn.\\n\\nPrivate Cluster: Phù hợp cho các ứng dụng yêu cầu bảo mật cao, tuân thủ nghiêm ngặt các quy định về bảo mật và quyền riêng tư. Mang lại kết nối ổn định và bảo mật, nhưng yêu cầu cấu hình và quản lý phức tạp hơn, cũng như chi phí cao hơn.'),\n",
       " Document(metadata={'source': './../../data/vks/su-dung-vks-voi-terraform.md'}, page_content='Sử dụng VKS với Terraform\\n\\nTerraform là gì?\\n\\nTerraform là một cơ sở hạ tầng nguồn mở dưới dạng công cụ mã cho phép người dùng quản lý cơ sở hạ tầng của họ một cách dễ dàng và hiệu quả trên các nền tảng đám mây khác nhau, chẳng hạn như VNG Cloud, AWS, Google Cloud và Azure. Máy chủ Terraform đề cập đến phiên bản của công cụ Terraform đang chạy trên một máy chủ hoặc máy cụ thể. Đây là nơi mã cơ sở hạ tầng được viết và thực thi, cho phép người dùng tạo, sửa đổi và hủy tài nguyên trên nền tảng đám mây.\\n\\nBản thân Terraform không có giao diện người dùng đồ họa, thay vào đó người dùng tương tác với nó bằng giao diện dòng lệnh. Terraform yêu cầu tài khoản và khóa của nhà cung cấp đám mây được định cấu hình cùng với tệp cấu hình Terraform để thực thi cơ sở hạ tầng dưới dạng mã. Ngoài ra, Terraform có thể hoạt động trong môi trường nhóm nơi nhiều người dùng có thể cộng tác trên cùng một cơ sở mã cơ sở hạ tầng, khiến nó trở thành một công cụ mạnh mẽ và linh hoạt để quản lý cơ sở hạ tầng đám mây.\\n\\nCác bước thực hiện\\n\\nĐể khởi tạo một Cluster Kubernetes bằng Terraform, bạn cần thực hiện các bước sau:\\n\\nTruy cập IAM Portal tại đây, thực hiện tạo Service Account với quyền hạn VKS Full Access. Cụ thể, tại trang IAM, bạn có thể:\\n\\nChọn \"Create a Service Account\", điền tên cho Service Account và nhấn Next Step để gắn quyền cho Service Account.\\n\\nTìm và chọn Policy: VKSFullAccess sau đó nhấn \"Create a Service Account\" để tạo Service Account, Policy: VKSFullAccess do VNG Cloud tạo ra, bạn không thể xóa các policy này.\\n\\nSau khi tạo thành công bạn cần phải lưu lại Client_ID và Secret_Key của Service Account để thực hiện bước tiếp theo.\\n\\nTruy cập VKS Portal tại đây, thực hiện Activate dịch vụ VKS ở tab Overview. Hãy chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn.\\n\\nCài đặt Terraform:\\n\\nTải xuống và cài đặt Terraform cho hệ điều hành của bạn từ https://developer.hashicorp.com/terraform/install.\\n\\nKhởi tạo cấu hình Terraform:\\n\\nTạo tệp variable.tf và khai báo thông tin Service Account trong file này.\\n\\nTạo tệp main.tf và định nghĩa các tài nguyên Kubernetes Cluster mà bạn muốn tạo.\\n\\nVí dụ:\\n\\nTệp variable.tf:bạn cần thay thế Client ID và Client Secret đã khởi tạo ở bước 1 ở file này.\\n\\nvariable \"client_id\" { type = string default = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" } variable \"client_secret\" { type = string default = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }\\n\\nTrên file main.tf, bạn cần có thể thêm resource để tạo Cluster/ Node Group:\\n\\nTạo Cluster my-vks-cluster và Node Group my-nodegroup độc lập:\\n\\n```markup resource \"vngcloud_vks_cluster\" \"primary\" { name = \"my-cluster\" cidr = \"172.16.0.0/16\" vpc_id = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" }\\n\\nresource \"vngcloud_vks_cluster_node_group\" \"primary\" { name= \"my-nodegroup\" ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" cluster_id= vngcloud_vks_cluster.primary.id } ```\\n\\nTạo Cluster với Default Node Group\\n\\nmarkup resource \"vngcloud_vks_cluster\" \"primary\" { name = \"my-cluster\" cidr = \"172.16.0.0/16\" vpc_id = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" node_group { name= \"my-nodegroup\" ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" } }\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nChúng tôi khuyên bạn nên tạo và quản lý các Cluster, Node Group dưới dạng resource riêng biệt, như trong ví dụ bên dưới. Điều này cho phép bạn thêm hoặc xóa các Node Group mà không cần tạo lại toàn bộ Cluster. Nếu bạn khai báo trực tiếp Node Group Default trong tài nguyên vngcloud_vks_cluster, bạn không thể xóa chúng mà không tạo lại chính Cluster đó.\\n\\nTrong file main.tf, để khởi tạo một cluster với một node group thành công, bạn bắt buộc cần nhập thông tin của 4 field sau:\\n\\nvpc_id = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" {% endhint %}\\n\\nCác ví dụ tham khảo\\n\\nExample Usage 1 - Create a Cluster with Network type CALICO OVERLAY and a Node Group with AutoScale Mode\\n\\n``` terraform { required_providers { vngcloud = { source = \"vngcloud/vngcloud\" version = \"1.2.7\" } } } resource \"vngcloud_vks_cluster\" \"primary\" { name = \"cluster-demo\" description = \"Cluster create via terraform\" version = \"v1.29.1\" cidr = \"172.16.0.0/16\" enable_private_cluster = false network_type = \"CALICO\" vpc_id = \"net-70ef12d4-d619-43fc-88f0-1c1511683123\" subnet_id = \"sub-0725ef54-a32e-404c-96f2-34745239c123\" enabled_load_balancer_plugin = true enabled_block_store_csi_plugin = true }\\n\\nresource \"vngcloud_vks_cluster_node_group\" \"primary\" { cluster_id = vngcloud_vks_cluster.primary.id name = \"nodegroup1\" num_nodes = 3 auto_scale_config { min_size = 0 max_size = 5 } upgrade_config { strategy = \"SURGE\" max_surge = 1 max_unavailable = 0 } image_id = \"img-108b3a77-ab58-4000-9b3e-190d0b4b07fc\" flavor_id = \"flav-9e88cfb4-ec31-4ad4-8ba5-243459f6d123\" disk_size = 50 disk_type = \"vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018\" enable_private_nodes = false ssh_key_id= \"ssh-f923c53c-cba7-4131-9f86-175d04ae2123\" security_groups = [\"secg-faf05344-fbd6-4f10-80a2-cda08d15ba5e\"] labels = { \"test\" = \"terraform\" } taint { key = \"key1\" value = \"value1\" effect = \"PreferNoSchedule\" } } ```\\n\\nExample Usage 2 - Create a Cluster with Network type CILIUM OVERLAY and a Node Group with AutoScale Mode\\n\\n``` terraform { required_providers { vngcloud = { source = \"vngcloud/vngcloud\" version = \"1.2.7\" } } } resource \"vngcloud_vks_cluster\" \"primary\" { name = \"cluster-demo\" description = \"Cluster create via terraform\" version = \"v1.29.1\" cidr = \"172.16.0.0/16\" enable_private_cluster = false network_type = \"CILIUM_OVERLAY\" vpc_id = \"net-70ef12d4-d619-43fc-88f0-1c1511683123\" subnet_id = \"sub-0725ef54-a32e-404c-96f2-34745239c123\" enabled_load_balancer_plugin = true enabled_block_store_csi_plugin = true }\\n\\nresource \"vngcloud_vks_cluster_node_group\" \"primary\" { cluster_id = vngcloud_vks_cluster.primary.id name = \"nodegroup1\" num_nodes = 3 auto_scale_config { min_size = 0 max_size = 5 } upgrade_config { strategy = \"SURGE\" max_surge = 1 max_unavailable = 0 } image_id = \"img-108b3a77-ab58-4000-9b3e-190d0b4b07fc\" flavor_id = \"flav-9e88cfb4-ec31-4ad4-8ba5-243459f6d123\" disk_size = 50 disk_type = \"vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018\" enable_private_nodes = false ssh_key_id= \"ssh-f923c53c-cba7-4131-9f86-175d04ae2123\" security_groups = [\"secg-faf05344-fbd6-4f10-80a2-cda08d15ba5e\"] labels = { \"test\" = \"terraform\" } taint { key = \"key1\" value = \"value1\" effect = \"PreferNoSchedule\" } } ```\\n\\nExample Usage 3 - Create a Cluster with Network type CILIUM VPC Native Routing and a Node Group with AutoScale Mode\\n\\n``` terraform { required_providers { vngcloud = { source = \"vngcloud/vngcloud\" version = \"1.2.7\" } } } resource \"vngcloud_vks_cluster\" \"primary\" { name = \"cluster-demo\" description = \"Cluster create via terraform\" version = \"v1.29.1\" enable_private_cluster = false network_type = \"CILIUM_NATIVE_ROUTING\" vpc_id = \"net-70ef12d4-d619-43fc-88f0-1c1511683123\" subnet_id = \"sub-0725ef54-a32e-404c-96f2-34745239c123\" secondary_subnets = [\"10.200.27.0/24\", \"10.200.28.0/24\"] node_netmask_size = 25 enable_service_endpoint = false enabled_load_balancer_plugin = true enabled_block_store_csi_plugin = true }\\n\\nresource \"vngcloud_vks_cluster_node_group\" \"primary\" { cluster_id = vngcloud_vks_cluster.primary.id name = \"nodegroup1\" num_nodes = 3 auto_scale_config { min_size = 0 max_size = 5 } upgrade_config { strategy = \"SURGE\" max_surge = 1 max_unavailable = 0 } image_id = \"img-108b3a77-ab58-4000-9b3e-190d0b4b07fc\" flavor_id = \"flav-9e88cfb4-ec31-4ad4-8ba5-243459f6d123\" subnet_id = \"sub-cddd7ffa-be05-4698-9b3d-794e1adfcbce\" secondary_subnets = [\"10.200.27.0/24\", \"10.200.28.0/24\"] disk_size = 50 disk_type = \"vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018\" enable_private_nodes = false ssh_key_id= \"ssh-f923c53c-cba7-4131-9f86-175d04ae2123\" security_groups = [\"secg-faf05344-fbd6-4f10-80a2-cda08d15ba5e\"] labels = { \"test\" = \"terraform\" } taint { key = \"key1\" value = \"value1\" effect = \"PreferNoSchedule\" } } ```\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nĐể lấy image_id bạn mong muốn sử dụng, bạn có thể truy cập vào VKS Portal, chọn menu System Image và lấy ID mà bạn mong muốn hoặc lấy thông tin này tại đây.\\n\\nĐể lấy flavor_id bạn mong muốn sử dụng cho Node group của bạn, vui lòng lấy ID tại đây. {% endhint %}\\n\\nKhởi chạy Terraform command\\n\\nSau khi hoàn tất các thông tin trên, thực hiện chạy lệnh bên dưới:\\n\\nterraform init\\n\\nSau đó, bạn để xem những thay đổi sẽ được áp dụng trên những resource mà terraform đang quản lý bạn có thể chạy:\\n\\nterraform plan\\n\\nCuối cùng bạn chọn chạy dòng lệnh:\\n\\nterraform apply\\n\\nChọn YES để thực hiện việc khởi tạo Cluster, Node Group thông qua Terraform\\n\\nKiểm tra Cluster vừa tạo trên giao diện VNG Cloud Portal\\n\\nSau khi khởi tạo thành công Terraform, bạn có thể lên VKS Portal để xem thông tin Cluster vừa tạo.\\n\\nTham khảo thêm về cách sử dụng Terraform để làm việc với VKS tại đây.\\n\\nMột số lưu ý khi sử dụng VKS với Terraform:\\n\\nKhi sử dụng Terraform để khởi tạo Cluster và Node Group trên hệ thống VKS, nếu bạn thay đổi một trong các field sau, hệ thống sẽ tự động xóa Node Group/ Cluster và thực hiện khởi tạo lại Node Group/ Cluster theo thông số mới tương ứng. Việc xóa sẽ được thực hiện trước khi tạo Node Group/ Cluster mới.\\n\\nĐỗi với resource vngcloud_vks_cluster, các field khi bạn thay đổi hệ thống sẽ xóa Cluster và tạo lại bao gồm:\\n\\nname\\n\\ndescription\\n\\nenable_private_cluster\\n\\nnetwork_type\\n\\nvpc_id\\n\\nsubnet_id\\n\\ncidr\\n\\nenabled_load_balancer_plugin\\n\\nenabled_block_store_csi_plugin\\n\\nnode_group\\n\\nsecondary_subnets\\n\\nnode_netmask_size\\n\\nĐỗi với resource vngcloud_vks_cluster_node_group, các field khi bạn thay đổi hệ thống sẽ xóa Cluster và tạo lại bao gồm:\\n\\ncluster_id\\n\\nname\\n\\nflavor_id\\n\\ndisk_size\\n\\ndisk_type\\n\\nenable_private_nodes\\n\\nssh_key_id\\n\\nsecondary_subnets\\n\\nenabled_encryption_volume\\n\\nsubnet_id\\n\\nĐể chỉ định hệ thống tạo cluster/node group mới rồi mới thực hiện xóa cluster/ node group cũ, bạn có thể thêm tham số lifecycle { create_before_destroy = true }vào file main.tf của bạn. Cụ thể:\\n\\nĐỗi với resource vngcloud_vks_cluster\\n\\n``` resource \"vngcloud_vks_cluster\" \"example\" { # ...\\n\\nlifecycle { create_before_destroy = true } } ```\\n\\nĐỗi với resource vngcloud_vks_cluster_node_group\\n\\n``` resource \"vngcloud_vks_cluster_node_group\" \"example\" { # ...\\n\\nlifecycle { create_before_destroy = true } } ```'),\n",
       " Document(metadata={'source': './../../data/vks/tham-khao-them/README.md'}, page_content='Tham khảo thêm'),\n",
       " Document(metadata={'source': './../../data/vks/tham-khao-them/danh-sach-system-image-dang-ho-tro.md'}, page_content='Danh sách System Image đang hỗ trợ\\n\\nBên dưới là danh sách System Image mà hệ thống VKS đang hỗ trợ:\\n\\nKubernetes version Image name Image ID v1.27.12 1_Ubuntu-22.kube_v1-27-12 img-36ee0a61-863d-4d40-a768-9b41269b8a62 v1.28.8 1_Ubuntu-22.kube_v1-28-8 img-983d55cf-9b5b-44cf-aa72-23f3b25d43ce v1.29.1 1_Ubuntu-22.kube_v1-29-1 img-108b3a77-ab58-4000-9b3e-190d0b4b07fc'),\n",
       " Document(metadata={'source': './../../data/vks/tham-khao-them/danh-sach-flavor-dang-ho-tro.md'}, page_content='Danh sách Flavor đang hỗ trợ\\n\\nBên dưới là danh sách các Flavor đang hỗ trợ bởi VKS:\\n\\nGeneral Code A\\n\\nFlavor Name CPU Memory Flavor ID a-general-2x4 2 4 flav-305a67bf-1825-4e3f-bc72-d41afa160d93 a-general-4x8 4 8 flav-9e88cfb4-ec31-4ad4-8ba5-243459f6dc4b a-general-8x16 8 16 flav-2fec3902-4b71-489c-a294-19bad1df3a70 a-general-12x24 12 24 flav-ea60643c-36f0-4fd2-be8d-e42198d6320e a-general-12x24-n10 12 24 flav-0db2d13d-530b-4312-b2be-cdb3b80c73e4 a-general-8x16-n10 8 16 flav-a04a8e9c-8def-4fde-9bfc-2ba738576463 a-general-16x32-n10 16 32 flav-65701e64-471f-4747-a837-500651b4c67d a1-standard-2x8 2 8 flav-edd3a4bd-ce5a-4b72-9323-468d58615b68 a1-standard-4x16 4 16 flav-4cb926dc-63be-4edc-8a21-4a66d963b45b a1-standard-8x32 8 32 flav-b834f30b-8dad-4d7e-8b71-952824bfef23 a1-standard-16x64 16 64 flav-7f6e15db-191c-4c89-80da-70c74ab5f786 a1-standard-32x128 32 128 flav-67617e54-48a5-4aed-9213-4560dc0cd9c1 a1-standard-48x192 48 192 flav-778bfd0f-ad5a-4c83-b44a-7a1f2f855240 a1-standard-8x32-n10 8 32 flav-f31884f4-e8b9-4f9a-9de5-41307afa5960 a1-standard-16x64-n10 16 64 flav-c398e0a9-a153-4826-8bd5-4d715d3e5032 a1-standard-32x128-n10 32 128 flav-2ac60541-046e-4761-b40c-990012597e09 a1-standard-48x192-n10 48 192 flav-bde154c4-52c9-4e92-9495-7fd7af3080c8 a1-highmem-2x16 2 16 flav-7615377f-909c-4e1d-9512-bc3b639a3777 a1-highmem-4x32 4 32 flav-dc274c6a-ef9a-4aff-9966-adecfac3731f a1-highmem-8x64 8 64 flav-3e3eb1c2-9f38-47f3-b2a1-878ce28c160b a1-highmem-16x128 16 128 flav-618a1e2c-c3eb-48e8-af13-17661ce8e37c a1-highmem-32x256 32 256 flav-9f9421c3-8956-43fb-a9c5-2563d5316fb9 a1-highmem-8x64-n10 8 64 flav-f51b4591-7e2d-4a2f-93ce-7a0b2a4827db a1-highmem-16x128-n10 16 128 flav-f9e6b4a4-a802-4f86-9adb-fcb746fe62e6 a1-highmem-32x256-n10 32 256 flav-3f52df69-638b-4363-836c-4b8ff9622080 a1-highcpu-4x4 4 4 flav-3bb2088e-5c17-4780-93c8-edea2b5bc1d6 a1-highcpu-8x8 8 8 flav-4c4cb707-3e85-4a17-8891-485985b10c0a a1-highcpu-16x16 16 16 flav-67489c98-a620-466a-9449-5f08dfdebec3 a1-highcpu-32x32-n10 32 32 flav-5f84e226-833b-4ab8-a797-7653dae9a764 a1-highcpu-16x16-n10 16 16 flav-9ef49395-9628-4a50-94fd-56c3aeef5065 a1-highcpu-32x64-n10 32 64 flav-bf77e8d1-8ec4-42e1-aca4-bb4a703b67d8\\n\\nGeneral Code S\\n\\nFlavor Name CPU Memory Flavor ID eci.ins.s-general-2x4 2 4 flav-152e00ea-5412-44c5-868f-0b30e37e4f90 eci.ins.s-general-4x8 4 8 flav-f5e0201d-606f-4480-be72-de11f46d48ff eci.ins.s-general-8x16 8 16 flav-aece130a-2a09-4d0d-b5eb-0c79589f1547 eci.ins.s-general-16x32 16 32 flav-4b949be1-e3ef-4d73-8ce4-cd264fee86df s-general-2x4 2 4 flav-8066e9ff-5d80-4e8f-aeae-9e8a934bfc44 s-general-4x8 4 8 flav-3929c073-9da9-486f-a96f-9282dbb8d83f s-general-8x16 8 16 flav-578e2030-154e-45db-ac5a-72c7c7f70e47 s-general-16x32 16 32 flav-4b686b68-a141-4a1a-a8f6-e54f710706bf s-general-16x32-n10 16 32 flav-903a8d4a-066a-449f-b537-2fd889d79de4 s-general-8x16-n10 8 16 flav-65870741-34cd-4d78-89f5-63c4354ccfd8 s1-standard-2x8 2 8 flav-198daa13-4829-43b4-9e7e-445906388ffb s1-standard-4x16 4 16 flav-2d84e9ce-ad64-4e1d-a623-b11326f229b4 s1-standard-8x32 8 32 flav-80681f16-3202-47c5-8387-011285199e64 s1-standard-12x48 12 48 flav-54007569-0943-47b7-adaf-7ab672bd96a5 s1-standard-16x64 16 64 flav-eab7d53c-f711-40fe-90fc-2430d529742f s1-standard-32x128 32 128 flav-b0f73b54-06b3-4743-9e11-734d0fab0a37 s1-standard-64x256-n10 64 256 flav-e55076b2-6c75-446f-a35c-51c70e6344f6 s1-standard-48x192-n10 48 192 flav-3519fdf6-7387-4ff4-9323-3c0d4b3ca431 s1-standard-32x128-n10 32 128 flav-1271e7a6-c15b-4002-9aba-077d81126902 s1-standard-16x64-n10 16 64 flav-59f8bb49-1ef1-4425-be92-414eb80465d8 s1-standard-8x32-n10 8 32 flav-3c0a65d2-775f-4c15-826e-3f41ff8c7900 s1-highmem-2x16 2 16 flav-e3ded003-043f-4fdf-8d2f-2d98fba7bb9f s1-highmem-4x32 4 32 flav-23385d8e-1abe-4897-acf5-f2dc25f9d04a s1-highmem-8x64 8 64 flav-bcae1721-a43c-4752-a3da-9f53ee33b297 s1-highmem-16x128 16 128 flav-6afd6448-48e8-4f3e-a7e7-12f0fc357564 s1-highmem-32x256 32 256 flav-c0986513-5863-456a-8e85-5fb217e37934 s1-highmem-32x256-n10 32 256 flav-1df93b8c-a39f-4298-ad0a-6cacd0ba86e4 s1-highmem-16x128-n10 16 128 flav-3bc652a4-99a6-4b1c-8676-dc8a35ef3a70 s1-highmem-8x64-n10 8 64 flav-34bc77ca-3887-4a52-a068-ac0160be7cfd s1-highcpu-4x4 4 4 flav-6d08591d-292f-4220-9ac2-6db0448c78ba s1-highcpu-8x8 8 8 flav-9ef5b92c-40fc-46a3-b8d0-81f6d602c997 s1-highcpu-32x32 32 32 flav-2b9063fc-799f-4dc6-ad25-6f279b212785 s1-highcpu-32x64-n10 32 64 flav-f50dddce-3ab9-426f-8fa1-0d4f84b4e4db s1-highcpu-16x16-n10 16 16 flav-5da432a8-8b26-4db4-bcb3-14236e3e1521 s1-highcpu-8x8-n10 8 8 flav-bc0a9310-dbef-4206-9d3e-44c4780a222a eci.ins.s1-standard-2x8 2 8 flav-afef7157-38ac-49e0-8329-327f1559733e eci.ins.s1-standard-4x16 4 16 flav-bb0bc0db-2710-44a1-933e-2b9e1fb99d6e eci.ins.s1-standard-8x32 8 32 flav-d83e3d9d-83a6-4935-aca7-e7891a415a51 eci.ins.s1-standard-16x64 16 64 flav-4ee7c1db-ad75-4b0a-9f8a-3e43f0b27df3 eci.ins.s1-standard-32x128 32 128 flav-12f11d02-ab75-4157-8a14-f7be6f14095e s1-highcpu-16x16 16 16 flav-a3be1208-a778-4cc5-9777-30d09fc41179 s1-highcpux2-32x64 32 64 flav-be97bcad-9b37-4570-ad06-ae7d9786e4fb eci.ins.s1-highmem-2x16 2 16 flav-49528db1-71d2-452f-bf75-e48c7ae0922f eci.ins.s1-highmem-4x32 4 32 flav-a3b67f38-dd45-47e3-8755-273b6b1e506f eci.ins.s1-highmem-8x64 8 64 flav-54a945d5-a811-4c55-a22f-6b3338223794 eci.ins.s1-highmem-16x128 16 128 flav-bf0dc4ef-3262-4730-a1fb-14eb27ba1fdb eci.ins.s1-highmem-32x256 32 256 flav-50d0f16b-cc73-4301-ae6e-07ef3b538a6c eci.ins.s1-highcpu-4x4 4 4 flav-4988e043-2cee-4e64-bf8b-e97171a66045 eci.ins.s1-highcpu-8x8 8 8 flav-e87c9a15-e1bc-4ee1-a25d-389dbadc4325 eci.ins.s1-highcpu-16x16 16 16 flav-4d67306c-6fd2-4363-85ac-98e4319d7eb6\\n\\nGPU Code G\\n\\nFlavor Name CPU Memory CPU Flavor ID g1-standard-4x16-1rtx2080ti 4 16 1 flav-ae002cb3-41ac-417f-ad40-fb0af73b184a g1-standard-8x32-1rtx2080ti 8 32 1 flav-f73d71b9-e269-47e0-af78-3be6404c52b4 g1-standard-8x32-2rtx2080ti 8 32 2 flav-c8313a57-b53a-4970-b4a0-63440a388869 g1-standard-16x64-2rtx2080ti 16 64 2 flav-c8f87213-8a35-47d3-aec1-29ab5207b8db g1-standard-16x64-4rtx2080ti 16 64 4 flav-3bd899f0-0725-4a78-a5a1-9d0cab0a16af g1-standard-32x128-8rtx2080ti 32 128 8 flav-c370e9ab-5c74-4e86-982b-0d91723cd445 g1-standard-8x64-1rtx2080ti 8 64 1 flav-b9febd61-07fd-46eb-9ead-f084e420febf g1-standard-8x64-2rtx2080ti 8 64 2 flav-52a090f3-5a64-48c3-a47e-4e1f26c4ebfb\\n\\nGPU Code RTX4090\\n\\nFlavor Name CPU Memory GPU Flavor ID g2-standard-32x128-4rtx4090 32 128 4 flav-79d5efbf-91bd-442b-9919-f7d7dd59761a g2-standard-16x128-2rtx4090 16 128 2 flav-a9bfd432-0b9f-481a-a6ce-8c924dd08ec3 g2-standard-16x64-2rtx4090 16 64 2 flav-fc2c0e0b-8ecb-4039-aa98-11bccc982d35 g2-standard-32x64-2rtx4090 32 64 2 flav-6b11c1c0-2631-4ce8-8184-1f94859518cb g2-standard-16x64-1rtx4090 16 64 1 flav-2a2286db-503d-4e0e-8cdf-b994475865d3'),\n",
       " Document(metadata={'source': './../../data/vks/node-groups/README.md'}, page_content='Node Groups\\n\\nNode Group là một khái niệm quan trọng trong Kubernetes, dùng để quản lý nhóm các node (VM) có cùng chung cấu hình trong một cluster. Đối với một Node Group, bạn có thể:\\n\\nTạo một Node Group\\n\\nĐể khởi tạo một Node Group, hãy làm theo các bước bên dưới:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại Cluster đã khởi tạo trước đó, hãy chọn Create a Node group.\\n\\nBước 3: Tại màn hình khởi tạo Node Group, chúng tôi đã thiết lập thông tin cho Node Group của bạn. Bạn có thể giữ các giá trị mặc định này hoặc điều chỉnh các thông số mong muốn cho Node Group của bạn tại:\\n\\nNode Group Information:\\n\\nNode Group Name: Tên gợi nhớ cho Node Group của bạn.\\n\\nNumber of nodes: Nhập vào số lượng Worker node cho Cluster của bạn, lưu ý số lượng node cần lớn hơn hoặc bằng 1 và nhỏ hơn hoặc bằng 100.\\n\\nNode Group Automation Setting:\\n\\nAuto Healing: Mặc định chúng tôi sẽ bật tính năng HA trong Cluster của bạn. Khi có node hoặc pod bị lỗi, Kubernetes sẽ tự động khởi động lại hoặc tạo mới pod để thay thế, đảm bảo ứng dụng của bạn luôn hoạt động mà không bị gián đoạn.\\n\\nAuto Scaling: Bật tính năng tự động mở rộng trong Cluster của bạn. Auto scaling giúp tự động điều chỉnh số lượng pod (đơn vị triển khai ứng dụng) dựa trên nhu cầu sử dụng thực tế, tránh tình trạng lãng phí tài nguyên khi nhu cầu thấp hoặc quá tải khi nhu cầu cao.\\n\\nMinimum node: số node tối thiểu mà Cluster cần có.\\n\\nMaximum node: số node tối đa mà Cluster có thể scale tới.\\n\\nNode Group upgrade stratetry: chiến lược upgrade Node Group. Khi bạn thiết lập Node Group Upgrade Strategy thông qua phương thức Surge upgrade cho một Node Group trong VKS, hệ thống VKS sẽ cập nhật tuần tự để nâng cấp các node, theo thứ tự không xác định.\\n\\nMax surge: giới hạn số lượng node được nâng cấp đồng thời (số lượng node mới (surge) có thể được tạo ra cùng một lúc). Mặc định Max surge = 1 - chỉ nâng cấp một node tại một thời điểm. với maxUnavailable\\n\\nMax unavailable: giới hạn số lượng node không thể truy cập được trong quá trình nâng cấp (số lượng node hiện tại có thể bị gián đoạn cùng một lúc). Mặc định Max unavailable = 0 - đảm bảo tất cả các node đều có thể truy cập được trong quá trình nâng cấp.\\n\\nNode Group Setting:\\n\\nImage: mặc định chúng tôi cung cấp 1 loại Image là Ubuntu with containerd.\\n\\nInstance type: chọn loại phiên bản cấu hình phù hợp cho Worker node theo nhu cầu sử dụng của bạn.\\n\\nNode Group Volume Setting: Cấu hình Boot Volume – Các thông số được cài đặt mặc định bởi hệ thống giúp tối ưu cho Cluster của bạn\\n\\nNode Group Network Setting: Bạn có thể lựa chọn Public Node Group hoặc Private Node Group tùy theo nhu cầu sử dụng Cluster của bạn.\\n\\nNode Group Security Setting: Bạn có thể chọn Security Group và SSH Key cho Node Group của bạn.\\n\\nNode Group Metadata Setting: Bạn có thể nhập Metadata tương ứng cho Node Group.\\n\\nBước 5: Chọn Create Node Group. Hãy chờ vài phút để chúng tôi khởi tạo Node Group của bạn, trạng thái của Node Group lúc này là Creating.\\n\\nBước 6: Khi trạng thái Node Group là Active, bạn có thể xem thông tin Node Group bằng cách chọn vào Node Group Name tại màn hình chính.\\n\\nChỉnh sửa một Node Group\\n\\nĐối với Node Group, bạn có thể chỉnh sửa các thông số: Number of Nodes, Auto Scaling, Upgrade Strategy, Security Group trong từng lần chỉnh sửa riêng biệt. Cụ thể, bạn có thể thực hiện theo các bước sau đây:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại Cluster đã khởi tạo trước đó, hãy chọn vào Cluster bạn muốn chỉnh sửa Node group.\\n\\nBước 3: Tại màn hình chứa danh sách Node Group đang có, tại Node Group bạn muốn chỉnh sửa chọn một trong các phương án:\\n\\nTính năng Resize: bạn có thể thay đổi\\\\\\n\\nNumber of nodes: Nhập vào số lượng Worker node cho Cluster của bạn, lưu ý số lượng node cần lớn hơn hoặc bằng 1 và nhỏ hơn hoặc bằng 100.\\n\\nTính năng Edit Auto Scaling: bạn có thể thay đổi\\\\\\n\\nAuto Scaling: Bật tính năng tự động mở rộng trong Cluster của bạn. Auto scaling giúp tự động điều chỉnh số lượng pod (đơn vị triển khai ứng dụng) dựa trên nhu cầu sử dụng thực tế, tránh tình trạng lãng phí tài nguyên khi nhu cầu thấp hoặc quá tải khi nhu cầu cao.\\n\\nMinimum node: số node tối thiểu mà Cluster cần có.\\n\\nMaximum node: số node tối đa mà Cluster có thể scale tới.\\n\\nTính năng Edit Upgrade Stratetry: bạn có thể thay đổi\\n\\nNode Group upgrade stratetry: chiến lược upgrade Node Group. Khi bạn thiết lập Node Group Upgrade Strategy thông qua phương thức Surge upgrade cho một Node Group trong VKS, hệ thống VKS sẽ cập nhật tuần tự để nâng cấp các node, theo thứ tự không xác định.\\n\\nMax surge: giới hạn số lượng node được nâng cấp đồng thời (số lượng node mới (surge) có thể được tạo ra cùng một lúc). Mặc định Max surge = 1 - chỉ nâng cấp một node tại một thời điểm. với maxUnavailable\\n\\nMax unavailable: giới hạn số lượng node không thể truy cập được trong quá trình nâng cấp (số lượng node hiện tại có thể bị gián đoạn cùng một lúc). Mặc định Max unavailable = 0 - đảm bảo tất cả các node đều có thể truy cập được trong quá trình nâng cấp.\\n\\nTính năng Edit Security Group: bạn có thể thay đổi\\n\\nNode Group Security Setting: Bạn có thể chọn Security Group và SSH Key cho Node Group của bạn.\\n\\nXóa một Node Group\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi không còn nhu cầu sử dụng Node Group, bạn hãy thực hiện xóa chúng để tiết kiệm chi phí. Khi xoá Node Group, các tài nguyên sau sẽ bị xóa:\\n\\nTất cả các node có trong Node Group (VM) {% endhint %}\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn menu Kubernetes Cluster.\\n\\nBước 3: Tại Cluster đã tạo thành công, chọn Node Group bạn muốn xóa và chọn Delete.\\n\\nBước 4: Chọn Delete để xóa hoàn toàn Node Group của bạn.'),\n",
       " Document(metadata={'source': './../../data/vks/node-groups/upgrading-node-group-version.md'}, page_content='Upgrading Node Group Version\\n\\nHiện tại, hệ thống VKS của chúng tôi đã hỗ trợ bạn nâng cấp Node Group Version, bạn có thể nâng cấp Node Group Version lên:\\n\\nControl Plane Version (Ví dụ nâng cấp từ 1.24 (Node Group version hiện tại) lên 1.25 (Control Plane Version hiện tại), nhưng không thể nâng cấp lên các phiên bản khác.\\n\\nĐể thực hiện nâng cấp phiên bản Node Group Version, bạn có thể thực hiện theo hướng dẫn sau:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn menu Kubernetes Cluster. Chọn vào một Cluster mà bạn muốn nâng cấp Node Group Version.\\n\\nBước 3: Chọn biểu tượng và chọn Upgrade Node Group version để thực hiện nâng cấp version node group.\\n\\nBước 4: Bạn có thể lựa chọn phiên bản mới cho tất cả các Node Group. Phiên bản mới cần hợp lệ và tương thích với phiên bản hiện tại của cluster. Cụ thể: bạn có thể chọn:\\n\\nNâng cấp Node Group sao cho về cùng version với Control Plane Version (ví dụ: 1.24 lên 1.25)\\n\\nBước 5: Hệ thống VKS sẽ thực hiện nâng cấp tất cả các Node Group lên version của Control Plane. Sau khi việc nâng cấp hoàn tất, trạng thái Node Group trở về ACTIVE.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nViệc nâng cấp Node Group Version là không bắt buộc và độc lập với việc nâng cấp Control Plane Version. Tuy nhiên tất cả các Node Group trong một Cluster sẽ được nâng cấp trong cùng một lần, cũng như Control Plane Version và Node Group Version trong cùng một Cluster không được lệch quá 1 minor version. Bên cạnh đó, hệ thống VKS tự động nâng cấp Node Group Version khi phiên bản K8S Version hiện tại đang sử dụng cho Cluster của bạn quá thời hạn được nhà cung cấp hỗ trợ.\\n\\nTrong quá trình nâng cấp Node Group Version, bạn không thể thực hiện các hành động khác trên Node Group của bạn.\\n\\nBên dưới là một vài lưu ý trước, trong và sau quá trình nâng cấp, vui lòng tham khảo thêm:\\n\\nTrước khi thực hiện:\\n\\nKiểm tra phiên bản hiện tại: Truy cập Releases để tham khảo danh sách các phiên bản được hỗ trợ. Chọn phiên bản mới hợp lệ và tương thích với phiên bản hiện tại của cluster.\\n\\nĐảm bảo tính sẵn sàng của Node Group: Node Group phải đang ở trạng thái hoạt động (ACTIVE) và tất cả các node phải HEALTHY.\\n\\nNgừng các tác vụ đang chạy: Ngừng các tác vụ đang chạy trên cluster để tránh ảnh hưởng đến quá trình nâng cấp.\\n\\nTrong khi thực hiện:\\n\\nTheo dõi trạng thái Node Group: Theo dõi trạng thái Node Group trong quá trình nâng cấp. Trạng thái Node Group sẽ chuyển sang UPDATING và sau khi hoàn tất sẽ trở về ACTIVE.\\n\\nKiểm tra nhật ký hệ thống: Kiểm tra nhật ký hệ thống để phát hiện bất kỳ lỗi hoặc cảnh báo nào trong quá trình nâng cấp.\\n\\nSau khi thực hiện:\\n\\nKiểm tra tính sẵn sàng của Node Group: Xác nhận rằng Node Group đã được nâng cấp thành công và tất cả các node đang hoạt động bình thường.\\n\\nKiểm tra các ứng dụng: Kiểm tra các ứng dụng đang chạy trên cluster để đảm bảo chúng hoạt động bình thường sau khi nâng cấp.\\n\\nLưu ý:\\n\\nViệc nâng cấp Node Group Version có thể mất một khoảng thời gian tùy thuộc vào kích thước và độ phức tạp của Node Group.\\n\\nTrong một số trường hợp hiếm gặp, việc nâng cấp Node Group Version có thể thất bại. Nếu điều này xảy ra, hệ thống VKS sẽ tự động rollback cluster về phiên bản hiện tại. {% endhint %}\\n\\nNgoài ra, bạn cần chú ý thêm trường thông tin:\\n\\nNode Group upgrade stratetry: chiến lược upgrade Node Group. Khi bạn thiết lập Node Group Upgrade Strategy thông qua phương thức Surge upgrade cho một Node Group trong VKS, hệ thống VKS sẽ cập nhật tuần tự để nâng cấp các node, theo thứ tự không xác định.\\n\\nMax surge: giới hạn số lượng node được nâng cấp đồng thời (số lượng node mới (surge) có thể được tạo ra cùng một lúc). Mặc định Max surge = 1 - chỉ nâng cấp một node tại một thời điểm. với maxUnavailable\\n\\nMax unavailable: giới hạn số lượng node không thể truy cập được trong quá trình nâng cấp (số lượng node hiện tại có thể bị gián đoạn cùng một lúc). Mặc định Max unavailable = 0 - đảm bảo tất cả các node đều có thể truy cập được trong quá trình nâng cấp.'),\n",
       " Document(metadata={'source': './../../data/vks/node-groups/lable-va-taint.md'}, page_content='Lable và Taint\\n\\nLable\\n\\nLabel là một tính năng quan trọng trong Kubernetes, được sử dụng để tổ chức và quản lý các đối tượng một cách hiệu quả. Bạn có thể gán các cặp key-value cho các đối tượng Kubernetes như Pod, Node, Service, Deployment, v.v. Cụ thể:\\n\\nMỗi Lable là một cặp key-value: Key (khoá) là một chuỗi ký tự dùng để xác định tên của lable. Value (giá trị) là một chuỗi ký tự tùy chọn, cung cấp thông tin chi tiết về lable.\\n\\nKey và value phải tuân theo các quy tắc đặt tên: Key và value không được chứa dấu khoảng trắng, ký tự đặc biệt ngoài (-, _,.).\\n\\nLable có thể được sử dụng cho nhiều mục đích khác nhau, bao gồm:\\n\\nPhân loại các đối tượng dựa trên các tiêu chí như environment, version, status, v.v.\\n\\nTheo dõi và quản lý các đối tượng trong cụm Kubernetes.\\n\\nVí dụ:\\n\\napp: nginx - Lable này cho biết đối tượng có liên quan đến ứng dụng Nginx.\\n\\nenvironment: production - Lable này cho biết đối tượng thuộc về môi trường production.\\n\\nversion: 1.7.2 - Lable này cho biết đối tượng có liên quan đến phiên bản 1.7.2.\\n\\nTạo Lable\\n\\nĐể tạo Lable cho một Node Group, bạn hãy thực hiện theo hướng dẫn sau:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại Cluster đã khởi tạo trước đó, hãy chọn Create a Node group.\\n\\nBước 3: Tại màn hình khởi tạo Node Group, chúng tôi đã thiết lập thông tin cho Node Group của bạn. Bạn có thể giữ các giá trị mặc định này hoặc điều chỉnh các thông số mong muốn cho Node Group của bạn. Tại mục Node Group Metadata Setting, bạn cần:\\n\\nNhập key cho lable của bạn. Key phải bắt đầu và kết thúc bằng chữ hoặc số và bao gồm các ký tự a-z, A-Z, 0-9, -, _, . tối đa 253 ký tự. Ngoài ra bạn có thể nhập key là một DNS subdomain ví dụ: example.com/my-app\\n\\nNhập value cho key tương ứng này.\\n\\nBước 5: Chọn Create Node Group. Hãy chờ vài phút để chúng tôi khởi tạo Node Group của bạn, trạng thái của Node Group lúc này là Creating.\\n\\nBước 6: Khi trạng thái Node Group là Active, bạn có thể xem thông tin Node Group bằng cách chọn vào Node Group Name tại màn hình chính.\\n\\nHoặc bạn có thể tạo Lable thông qua kubectl theo câu lệnh:\\n\\nkubectl label nodes my-node1 disktype=ssd\\n\\nBạn có thể kiểm tra lại lable vừa tạo qua lệnh:\\n\\nkubectl get nodes --show-labels\\n\\nVí dụ kết quả cho lệnh này sẽ như sau:\\n\\nshell NAME STATUS ROLES AGE VERSION LABELS worker0 Ready <none> 1d v1.13.0 ...,disktype=ssd,kubernetes.io/hostname=worker0 worker1 Ready <none> 1d v1.13.0 ...,kubernetes.io/hostname=worker1 worker2 Ready <none> 1d v1.13.0 ...,kubernetes.io/hostname=worker2\\n\\nSử dụng Lable với nodeSelector\\n\\nnodeSelector là một tham số được sử dụng trong PodSpec để chỉ định rằng Pod chỉ nên được lên lịch trên các Node có lable cụ thể. Điều này hữu ích khi bạn muốn chạy Pod trên các Node có tài nguyên hoặc thuộc tính cụ thể.\\n\\nTạo tệp tin my-pod.yaml chứa nội dung như sau:\\n\\napiVersion: v1 kind: Pod metadata: name: my-pod spec: nodeSelector: disktype: ssd region: hcm03\\n\\nTrong ví dụ này, Pod my-pod chỉ được lên lịch trên các Node có lable disktype: ssd và region: hcm03.\\n\\nTriển khai Pod trên Cluster của bạn:\\n\\nkubectl -f apply my-pod.yaml\\n\\nTaint\\n\\nTaint là một tính năng quan trọng trong Kubernetes, đóng vai trò như một cơ chế để đánh dấu các Node và kiểm soát việc lên lịch Pod trên những Node đó. Khác với Label thông thường, Taint được sử dụng để chỉ định các thuộc tính đặc biệt của Node và thực thi các hành động cụ thể khi Pod không đáp ứng các điều kiện được xác định bởi Taint. Cụ thể:\\n\\nCụ thể:\\n\\nMỗi Taint bao gồm:\\n\\nKey (khoá) là một chuỗi ký tự dùng để xác định tên của taint.\\n\\nValue (giá trị) là một chuỗi ký tự tùy chọn, cung cấp thông tin chi tiết về taint.\\n\\nEffect:\\n\\nNoSchedule: Ngăn Pod không có Toleration tương ứng được lên lịch trên Node.\\n\\nNoExecute: Cho phép Pod được lên lịch trên Node nhưng Pod sẽ không được thực thi.\\n\\nPreferNoSchedule: Kubernetes sẽ cố gắng ưu tiên không lên lịch Pod lên Node có Taint này.\\n\\nKey và value phải tuân theo các quy tắc đặt tên: Key và value không được chứa dấu khoảng trắng, ký tự đặc biệt ngoài (-, _,.).\\n\\nToleration: Để Pod có thể được lên lịch và chạy trên Node có Taint, Pod cần có Toleration tương ứng. Toleration được khai báo trong PodSpec bằng cách sử dụng tolerations field. Ví dụ:\\n\\ntolerations: - key: node.role.kubernetes.io/master effect: NoSchedule\\n\\nMối quan hệ giữa Taint và Toleration: Khi Kubernetes lên lịch Pod, Kubernetes sẽ so khớp các Taint của Node với các Toleration của Pod. Pod chỉ được lên lịch trên Node nếu có Toleration cho tất cả các Taint của Node đó.\\n\\nVí dụ:\\n\\nnode.role.kubernetes.io/master:NoSchedule - ngăn các Pod thông thường được chạy trên Node này.\\n\\nTạo Taint\\n\\nĐể tạo Taint cho một Node Group, bạn hãy thực hiện theo hướng dẫn sau:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại Cluster đã khởi tạo trước đó, hãy chọn Create a Node group.\\n\\nBước 3: Tại màn hình khởi tạo Node Group, chúng tôi đã thiết lập thông tin cho Node Group của bạn. Bạn có thể giữ các giá trị mặc định này hoặc điều chỉnh các thông số mong muốn cho Node Group của bạn. Tại mục Node Group Metadata Setting, bạn cần:\\n\\nNhập key cho taint của bạn. Key phải bắt đầu và kết thúc bằng chữ hoặc số và bao gồm các ký tự a-z, A-Z, 0-9, -, _, . tối đa 253 ký tự. Ngoài ra bạn có thể nhập key là một DNS subdomain ví dụ: example.com/my-app\\n\\nNhập value cho key tương ứng này.\\n\\nChọn 1 trong 3 loại effect: NoSchedule, NoExecute, PreferNoSchedule.\\n\\nBước 5: Chọn Create Node Group. Hãy chờ vài phút để chúng tôi khởi tạo Node Group của bạn, trạng thái của Node Group lúc này là Creating.\\n\\nBước 6: Khi trạng thái Node Group là Active, bạn có thể xem thông tin Node Group bằng cách chọn vào Node Group Name tại màn hình chính.\\n\\nHoặc bạn có thể tạo Taint thông qua kubectl theo câu lệnh:\\n\\nkubectl taint node my-node node.role.kubernetes.io/master:NoSchedule.\\n\\nVí dụ sử dụng Taint:\\n\\nGiả sử bạn có một Node master được sử dụng cho mục đích quản lý và bạn muốn ngăn các Pod thông thường được chạy trên Node này. Bạn có thể sử dụng Taint như sau:\\n\\nkubectl taint node my-master node.role.kubernetes.io/master:NoSchedule\\n\\nĐể Pod có thể chạy trên Node master, Pod cần có Toleration tương ứng:\\n\\napiVersion: v1 kind: Pod metadata: name: my-pod spec: tolerations: - key: node.role.kubernetes.io/master effect: NoSchedule'),\n",
       " Document(metadata={'source': './../../data/vks/node-groups/auto-healing.md'}, page_content='Auto Healing\\n\\nTổng quan\\n\\nTrên hệ thống VKS, tính năng Auto Healing được apply cho mỗi Node Group và luôn luôn ở trạng thái bật. Việc bật tính năng tự phục hồi (self-healing) trong Kubernetes mang lại nhiều lợi ích quan trọng, giúp đảm bảo tính sẵn sàng và độ tin cậy cao cho ứng dụng của bạn.\\n\\nTính năng Auto Healing có những điểm nổi bật sau:\\n\\nTự động phát hiện lỗi: Kubernetes có thể tự động phát hiện các node bị lỗi hoặc gặp sự cố thông qua việc theo dõi trạng thái của các node. Một số dấu hiệu cho thấy node bị lỗi bao gồm: node báo cáo trạng thái \"NotReady\", node không thể ping được, node bị lỗi phần cứng, v.v.\\n\\nTự động khởi động lại node: Khi một node bị phát hiện lỗi, Kubernetes sẽ tự động khởi động lại node. Việc khởi động lại node có thể giúp khắc phục các lỗi tạm thời và đưa node trở lại trạng thái hoạt động bình thường.\\n\\nGiảm thiểu sự can thiệp thủ công: Auto Healing giúp giảm thiểu sự can thiệp thủ công của người quản trị viên hệ thống, tiết kiệm thời gian và công sức.\\n\\nCải thiện hiệu quả hoạt động: Auto Healing giúp cải thiện hiệu quả hoạt động của hệ thống bằng cách đảm bảo rằng các node luôn hoạt động bình thường.\\n\\nCơ chế hoạt động\\n\\nCơ chế Auto Healing: hệ thống VKS thực hiện kích hoạt auto healing khi\\n\\nNode báo cáo trạng thái NotReady trong các lần kiểm tra liên tiếp trong khoảng thời gian 10 phút.\\n\\nNếu thỏa mãn điều kiện trên, hệ thống sẽ ngay lập tức thực hiện auto healing. Quá trình này được thực hiện theo 2 bước:\\n\\nBước 1: Hệ thống VKS thực hiện drain node, tức là di chuyển tất cả các pod đang chạy trên node NotReady này sang các node khác trong node group trước khi gỡ bỏ node đó khỏi node group.\\n\\nBước 2: Hệ thống sẽ tạo lại node mới với cấu hình đã được thiết lập trên node group và thực hiện join node này vào cụm. Nếu sau khi khởi động lại, node vẫn báo cáo trạng thái \"NotReady\", hệ thống sẽ tiếp tục khởi động lại node cho đến khi node trở lại trạng thái hoạt động bình thường.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi hệ thống thực hiện Auto Healing, việc tạo ra node mới có thể gặp lỗi nếu bạn không có đủ credit hoặc bạn đã hết quota để tạo VM trên hệ thống vServer. Lúc này, mỗi 30 phút thì hệ thống sẽ thực hiện khởi động lại node cho đến khi node trở lại trạng thái hoạt động bình thường. Để tránh gặp lỗi bên trên, bạn cần:\\n\\nĐảm bảo bạn có đủ credit: Nếu bạn là người dùng trả trước, hãy nạp thêm credit vào tài khoản của bạn.\\n\\nYêu cầu tăng quota: Bạn có thể yêu cầu tăng quota cho tài khoản của mình tại đây. {% endhint %}\\n\\nBật Auto Healing\\n\\nHiện tại, tính năng Auto Healing được apply cho mỗi Node Group và luôn luôn ở trạng thái bật. Bạn không cần thao tác bật thủ công khi khởi tạo Cluster cũng như Node Group.'),\n",
       " Document(metadata={'source': './../../data/vks/node-groups/auto-scaling.md'}, page_content='Auto Scaling\\n\\nTổng quan\\n\\nAuto Scaling cho Cluster là một tính năng trong Kubernetes cho phép tự động điều chỉnh kích thước của cụm (Cluster) cụ thể là số lượng các node trong cụm để đáp ứng nhu cầu sử dụng.\\n\\nTính năng Auto Scaling có những điểm nổi bật sau:\\n\\nTối ưu hóa hiệu suất: Auto Scaling cho phép cụm tự động mở rộng tài nguyên khi có nhu cầu. Khi khối lượng công việc cao hơn, cụm sẽ tự động tạo thêm các node để đảm bảo các ứng dụng hoạt động với hiệu suất tốt nhất.\\n\\nTiết kiệm chi phí: Auto Scaling cho phép cụm tự động giảm tài nguyên khi không cần thiết. Nếu khối lượng công việc giảm đi, cụm sẽ tự động thu hồi các tài nguyên không sử dụng để tiết kiệm chi phí.\\n\\nĐảm bảo tính sẵn sàng: Auto Scaling giúp đảm bảo rằng cụm có sẵn để đáp ứng nhu cầu sử dụng và tránh tình trạng quá tải hoặc thiếu tài nguyên.\\n\\nTự động phục hồi: Auto Scaling giúp tự động phục hồi từ các sự cố hoặc lỗi bằng cách tạo ra các node mới để thay thế các node bị hỏng.\\n\\nKhi triển khai các ứng dụng trong môi trường cloud, việc sử dụng tính năng Auto Scaling giúp tối ưu hóa việc sử dụng tài nguyên, cải thiện tính sẵn sàng và hiệu suất của ứng dụng, và giúp quản lý cụm trở nên dễ dàng và hiệu quả hơn.\\n\\nCơ chế hoạt động\\n\\nCơ chế Scale up: hệ thống VKS thực hiện scale up khi\\n\\nCác pods không thể scheduling trên bất kỳ node hiện tại nào vì lý do thiếu resource.\\n\\nViệc tăng thêm 1 node giống với cấu hình node group hiện tại là hữu ích và có thể xử lý vấn đề thiếu resource này.\\n\\nMinh họa:\\n\\nNếu thỏa mãn 2 điều kiện trên, hệ thống sẽ tăng số node (một hoặc nhiều nodes) để đáp ứng toàn bộ pods đang unscheduling. Quá trình này sẽ được thực hiện ngay lập tức theo 2 bước:\\n\\nBước 1: Hệ thống VKS tạo node mới theo cấu hình node group hiện tại.\\n\\nBước 2: Hệ thống VKS sẽ deploy các pods đang unscheduling này lên các node mới.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi hệ thống thực hiện Auto Scaling, việc tạo ra node mới có thể gặp lỗi nếu bạn không có đủ credit hoặc bạn đã hết quota để tạo VM trên hệ thống vServer. Để tránh gặp lỗi bên trên, bạn cần:\\n\\nĐảm bảo bạn có đủ credit: Nếu bạn là người dùng trả trước, hãy nạp thêm credit vào tài khoản của bạn.\\n\\nYêu cầu tăng quota: Bạn có thể yêu cầu tăng quota cho tài khoản của mình tại đây. {% endhint %}\\n\\nCơ chế Scale down: hệ thống VKS thực hiện scale down khi\\n\\nMột hoặc nhiều node có tải thấp liên tục trong một khoảng thời gian. Cụ thể node có utilization (độ khả dụng) bao gồm cả request CPU và memory của pod thấp ở mức < 50%.\\n\\nTất cả các pod hiện tại của node đó, có thể được di chuyển qua node khác mà không gặp vấn đề gì.\\n\\nNếu thỏa mãn 2 điều kiện trên, mặc định là trong khoảng 10 phút, node đó sẽ bị xóa đi khỏi Cluster. Quá trình xóa này sẽ bao gồm 3 bước:\\n\\nBước 1: Hệ thống VKS sẽ đánh dấu là node đó là unschedulable.\\n\\nBước 2: Hệ thống di chuyển (move) toàn bộ pod qua node khác.\\n\\nBước 3: Sau khi di chuyển tất cả các pod qua node khác thành công, hệ thống VKS sẽ xóa node được đánh dấu.\\n\\nBật Auto Scaling\\n\\nTrên hệ thống VKS, bạn có thể bật Auto Scaling khi:\\n\\nKhởi tạo một Cluster\\n\\nKhởi tạo một Node Group\\n\\nChỉnh sửa một Node Group\\n\\nĐể bật Auto Scaling cho Kubernetes Cluster của bạn vui lòng bật lựa chọn Enable Auto Scaling, khi bật lựa chọn này bạn cần nhập:.\\n\\nMinimum node: số node tối thiểu mà Cluster phải có.\\n\\nMaximum node: số node tối đa mà Cluster có thể scale tới.\\n\\nNode Group upgrade stratetry: chiến lược upgrade Node Group. Khi bạn thiết lập Node Group Upgrade Strategy thông qua phương thức Surge upgrade cho một Node Group trong VKS, hệ thống VKS sẽ cập nhật tuần tự để nâng cấp các node, theo thứ tự không xác định.\\n\\nMax surge: giới hạn số lượng node được nâng cấp đồng thời (số lượng node mới (surge) có thể được tạo ra cùng một lúc). Mặc định Max surge = 1 - chỉ nâng cấp một node tại một thời điểm.\\n\\nMax unavailable: giới hạn số lượng node không thể truy cập được trong quá trình nâng cấp (số lượng node hiện tại có thể bị gián đoạn cùng một lúc). Mặc định Max unavailable = 0 - đảm bảo tất cả các node đều có thể truy cập được trong quá trình nâng cấp.\\n\\nVí dụ như hình bên dưới: tôi đã khởi tạo một Node Group với:\\n\\nNumber of nodes: 3 nodes\\n\\nMinimum node: 1 nodes\\n\\nMaximum node: 5 nodes\\n\\nLúc này:\\n\\nNếu có một hoặc nhiều pod không thể scheduling trên bất kỳ node trên cụm vì lý do thiếu resource và việc thêm 1 node giống với cấu hình node group này có thể xử lý vấn đề thì hệ thống sẽ thực hiện scale up. Với thiết lập này thì hệ thống có thể scale up tối đa lên tới 5 node.\\n\\nNếu có 1 node có low utilization (khả dụng) thấp ở mức < 50% và tất cả các pod của node đó có thể scheduling trên node khác thì hệ thống sẽ thực hiện scale up. Với thiết lập này thì hệ thống có thể scale down xuống mức tối thiểu là 1 node.'),\n",
       " Document(metadata={'source': './../../data/vks/clusters/README.md'}, page_content='Clusters\\n\\nCluster trong Kubernetes là một tập hợp gồm một hoặc nhiều máy ảo (VM) được kết nối lại với nhau để chạy các ứng dụng được đóng gói dạng container. Cluster cung cấp một môi trường thống nhất để triển khai, quản lý và vận hành các container trên quy mô lớn.\\n\\nĐiều kiện cần:\\n\\nĐể có thể khởi tạo một Cluster và Deploy một Workload, bạn cần:\\n\\nCó ít nhất 1 VPC và 1 Subnet đang ở trạng thái ACTIVE. Nếu bạn chưa có VPC, Subnet nào, vui lòng khởi tạo VPC, Subnet theo hướng dẫn tại đây.\\n\\nCó ít nhất 1 SSH key đang ở trạng thái ACTIVE. Nếu bạn chưa có SSH key nào, vui lòng khởi tạo SSH key theo hướng dẫn tại đây.\\n\\nĐã cài đặt và cấu hình kubectl trên thiết bị của bạn. vui lòng tham khảo tại đây nếu bạn chưa rõ cách cài đặt và sử dụng kuberctl. Ngoài ra, bạn không nên sử dụng phiên bản kubectl quá cũ, chúng tôi khuyến cáo bạn nên sử dụng phiên bản kubectl sai lệch không quá một phiên bản với version của cluster.\\n\\nKhởi tạo Cluster\\n\\nĐể khởi tạo một Cluster, hãy làm theo các bước bên dưới:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn Activate.\\n\\nBước 3: Chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn. Sau khi Activate thành công, bạn hãy chọn Create a Cluster.\\n\\nBước 4: Tại màn hình khởi tạo Cluster, chúng tôi đã thiết lập thông tin cho Cluster và một Default Node Group cho bạn, bạn có thể giữ các giá trị mặc định này hoặc điều chỉnh các thông số mong muốn cho Cluster và Node Group của bạn tại Cluster Configuration, Default Node Group Configuration, Plugin.\\n\\nCluster Configuration:\\n\\nCluster Information:\\n\\nCluster Name: Tên cho Cluster của bạn. Tên chỉ có thể chứa các ký tự chữ và số (a-z, A-Z, 0-9, \\'_\\', \\'-\\'). Độ dài dữ liệu đầu vào của bạn phải từ 5 đến 50. Tên phải là duy nhất trong Khu vực và tài khoản VNG Cloud mà bạn đang tạo Cluster.\\n\\nKubernetes Version: Phiên bản Kubernetes sẽ sử dụng cho Cluster của bạn. Chúng tôi khuyên bạn nên chọn phiên bản mới nhất, trừ khi bạn cần phiên bản cũ hơn.\\n\\nDescription: Nhập vào thông tin bạn muốn ghi chú cho Cluster nhằm tạo dấu hiệu riêng cho việc quản lý chúng dễ dàng hơn trong tương lai.\\n\\nNetwork Setting:\\n\\nNetwork type: lựa chọn loại network mà bạn mong muốn sử dụng cho Cluster của bạn. Hiện tại, VKS cung câp cho bạn chọn sử dụng 1 trong 3 loại network Calico Overlay, Cilium Overlay, Cilium VPC Native Routing.\\n\\nĐối với loại network Calico Overlay, Cilium Overlay, Encapsulation Mode được tự chọn mặc định bởi hệ thống và bạn không thể thay đổi chúng, tuy nhiên bạn có thể nhập lại thông số Calico CIDR: Dải mạng ảo mà các pod sẽ sử dụng (lưu ý IP phải là riêng tư và có thể chọn theo các tùy chọn sau (10.0.0.0 - 10.255.0.0 / 172.16.0.0 - 172.24.0.0 / 192.168.0.0)\\n\\nĐối với loại network Cilium VPC Native Routing:\\n\\nDefault Pod IP range là dải địa chỉ IP thứ cấp được sử dụng cho các pod. Nó được gọi là Secondary IP range vì nó không trùng với dải IP chính của node (Primary IP range). Các pod trong Cluster sẽ được gán IP từ dải này. Bạn cần lựa chọn ít nhất 1 dải Secondary IP range đã tạo từ vServer.\\n\\nNode CIDR mask size: Kích thước của CIDR dành cho các node. Thông số này cho biết mỗi node sẽ được gán bao nhiêu địa chỉ IP từ dải pod IP range. Kích thước này cần được chọn sao cho đảm bảo có đủ địa chỉ IP cho tất cả các pod trên mỗi node. Bạn có thể tham khảo bảng bên dưới để hiểu các tính số lượng IP có thể sử dụng để cấp phát cho node, pod trong cluster của bạn. Bạn cần lựa chọn một giá trị trong danh sách mà chúng tôi cung cấp phù hợp với nhu cầu của bạn.\\n\\nVPC: Chọn một VPC hiện có đáp ứng các yêu cầu của K8S để tạo Cluster của bạn. Trước khi chọn một VPC, chúng tôi khuyên bạn nên làm quen với tất cả các yêu cầu và cân nhắc trong VPC cũng như các yêu cầu và cân nhắc về Subnet. Bạn không thể thay đổi VPC nào bạn muốn sử dụng sau khi tạo Cluster. Nếu không có VPC nào được liệt kê, trước tiên bạn cần tạo một VPC. Để biết thêm thông tin, hãy xem Tạo VPC.\\n\\nSubnet: Theo mặc định, tất cả các mạng con khả dụng trong VPC được chỉ định trong trường trước đó sẽ được chọn ngẫu nhiên ở thứ tự đầu tiên, bạn có thể chọn lại Subnet khác, tuy nhiên chỉ được chọn duy nhất 1.\\n\\nDefault Node group Configuration:\\n\\nNode Group Information:\\n\\nNode Group Name: Tên gợi nhớ cho Node Group của bạn.\\n\\nNumber of nodes: Nhập vào số lượng Worker node cho Cluster của bạn, lưu ý số lượng node cần lớn hơn hoặc bằng 1 và nhỏ hơn hoặc bằng 100.\\n\\nNode Group Automation Setting:\\n\\nAuto Healing: Mặc định chúng tôi sẽ bật tính năng HA trong Cluster của bạn. Khi có node hoặc pod bị lỗi, Kubernetes sẽ tự động khởi động lại hoặc tạo mới pod để thay thế, đảm bảo ứng dụng của bạn luôn hoạt động mà không bị gián đoạn.\\n\\nAuto Scaling: Bật tính năng tự động mở rộng trong Cluster của bạn. Auto scaling giúp tự động điều chỉnh số lượng pod (đơn vị triển khai ứng dụng) dựa trên nhu cầu sử dụng thực tế, tránh tình trạng lãng phí tài nguyên khi nhu cầu thấp hoặc quá tải khi nhu cầu cao.\\n\\nMinimum node: số node tối thiểu mà Cluster cần có.\\n\\nMaximum node: số node tối đa mà Cluster có thể scale tới.\\n\\nNode Group upgrade stratetry: chiến lược upgrade Node Group. Khi bạn thiết lập Node Group Upgrade Strategy thông qua phương thức Surge upgrade cho một Node Group trong VKS, hệ thống VKS sẽ cập nhật tuần tự để nâng cấp các node, theo thứ tự không xác định.\\n\\nMax surge: giới hạn số lượng node được nâng cấp đồng thời (số lượng node mới (surge) có thể được tạo ra cùng một lúc). Mặc định Max surge = 1 - chỉ nâng cấp một node tại một thời điểm. với maxUnavailable\\n\\nMax unavailable: giới hạn số lượng node không thể truy cập được trong quá trình nâng cấp (số lượng node hiện tại có thể bị gián đoạn cùng một lúc). Mặc định Max unavailable = 0 - đảm bảo tất cả các node đều có thể truy cập được trong quá trình nâng cấp.\\n\\nNode Group Setting:\\n\\nImage: mặc định chúng tôi cung cấp 1 loại Image là Ubuntu with containerd.\\n\\nInstance type: chọn loại phiên bản cấu hình phù hợp cho Worker node theo nhu cầu sử dụng của bạn.\\n\\nNode Group Volume Setting: Cấu hình Boot Volume – Các thông số được cài đặt mặc định bởi hệ thống giúp tối ưu cho Cluster của bạn\\n\\nNode Group Network Setting: Bạn có thể lựa chọn Public Node Group hoặc Private Node Group tùy theo nhu cầu sử dụng Cluster của bạn.\\n\\nNode Group Security Setting: Bạn có thể chọn Security Group và SSH Key cho Node Group của bạn.\\n\\nNode Group Metadata Setting: Bạn có thể nhập Metadata tương ứng cho Node Group.\\n\\nPlugin\\n\\nEnable BlockStore Persistent Disk CSI Driver: bật để chúng tôi tự động cài đặt CSI Controller trên Cluster của bạn.\\n\\nEnable vLB Native Integration Driver: bật để chúng tôi tự động cài đặt LB Controller trên Cluster của bạn.\\n\\nBước 5: Chọn Create Kubernetes cluster. Hãy chờ vài phút để chúng tôi khởi tạo Cluster của bạn, trạng thái của Cluster lúc này là Creating.\\n\\nBước 6: Khi trạng thái Cluster là Active, bạn có thể xem thông tin Cluster, thông tin Node Group bằng cách chọn vào Cluster Name tại cột Name.\\n\\nTải xuống tệp tin Kube Config\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn menu Kubernetes Cluster.\\n\\nBước 3: Tại Cluster đã tạo thành công, chọn biểu tượng và chọn Download Config File.\\n\\nBước 4: File config sẽ được lưu về máy, lúc này bạn có thể sử dụng Kubectl để quản lý Cluster của bạn trên thiết bị cá nhân của mình.\\n\\nXóa một Cluster\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi không còn nhu cầu sử dụng Kubernetes Cluster, bạn nên xóa các tài nguyên được liên kết với cụm đó để không phải chịu bất kỳ chi phí không cần thiết nào. Khi xoá Kubernetes Cluster các tài nguyên sau sẽ bị xóa:\\n\\nControl Plane Resource của Cluster.\\n\\nTất cả các node có trong Cluster (VM)\\n\\nTất cả các Pod nào đang chạy trên các node.\\n\\nSecurity Group mặc định tạo cho Cluster đó.\\n\\nLoad Balancer mặc định tạo cho Cluster đó.\\n\\nETCD.\\n\\nHệ thống có thể không xóa các tài nguyên sau:\\n\\nLoad Balancer được integrated vào Cluster bởi bạn.\\n\\nPersistent Volume được integrated vào Cluster bởi bạn. {% endhint %}\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn menu Kubernetes Cluster.\\n\\nBước 3: Tại Cluster đã tạo thành công, chọn cluster muốn xóa và chọn Delete\\n\\nBước 4: Chọn Delete để xóa hoàn toàn Cluster của bạn.'),\n",
       " Document(metadata={'source': './../../data/vks/clusters/whitelist.md'}, page_content='Whitelist\\n\\nTổng quan\\n\\nTính năng Whitelist IP trên chế độ Private Node Group của VKS cho phép bạn chỉ cho phép các địa chỉ IP cụ thể kết nối đến Cluster của bạn. Điều này giúp tăng cường bảo mật cho các ứng dụng và dữ liệu nhạy cảm bằng cách hạn chế truy cập từ các nguồn không xác định.\\n\\nLợi ích\\n\\nTăng cường bảo mật: Whitelist IP giúp bảo vệ dữ liệu và ứng dụng của bạn khỏi các mối đe dọa tiềm ẩn trên mạng công cộng, chẳng hạn như tấn công mạng và vi phạm dữ liệu.\\n\\nGiảm thiểu rủi ro: Bằng cách hạn chế truy cập vào các node nhạy cảm, Whitelist IP giúp giảm thiểu rủi ro lây lan vi phạm dữ liệu sang các phần khác của mạng của bạn.\\n\\nKiểm soát tốt hơn: Whitelist IP cho phép bạn kiểm soát chặt chẽ quyền truy cập vào các node của mình, đảm bảo chỉ những người dùng và ứng dụng được ủy quyền mới có thể truy cập.\\n\\n{% hint style=\"info\" %} Khuyến nghị về Sử dụng Whitelist trong Các Mô Hình Cluster:\\n\\n1. Public Cluster Chỉ Bao Gồm Public Node Group\\n\\nKhuyến nghị: Không khuyến khích sử dụng whitelist.\\n\\nNếu bạn có nhu cầu sử dụng Whitelist IP vì security, vui lòng allow danh sách IP Range Public của vServer theo danh sách sau:\\n\\nbash 103.245.249.0/24 103.245.251.0/24 116.118.95.0/24 58.84.1.0/24 58.84.2.0/24 61.28.226.0/24 61.28.227.0/24 61.28.229.0/24 61.28.230.0/24 61.28.231.0/24 180.93.182.0/24 61.28.233.0/24 61.28.235.0/24 61.28.236.0/24 61.28.238.0/24 180.93.183.0/24\\n\\n2. Public Cluster Bao Gồm Private Node Group Đi Qua NAT Gateway (Pfsense, PaloAlto)\\n\\nKhuyến nghị: Có thể sử dụng tính năng whitelist.\\n\\nCần thực hiện Whitelist thêm IP của NAT Gateway.\\n\\n3. Private Cluster Bao Gồm Public Node Group hoặc Private Node Group\\n\\nKhuyến nghị: Có thể sử dụng tính năng whitelist. {% endhint %}\\n\\nChỉnh sửa Whitelist\\n\\nĐể sử dụng tính năng Whitelist IP trên chế độ Private Node Group, bạn cần thực hiện các bước sau:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn menu Kubernetes Cluster.\\n\\nBước 3: Chọn biểu tượng Action và chọn Edit Whitelist hoặc chọn biểu tượng Edit khi xem chi tiết một Cluster để thực hiện thêm Whitelist cho Cluster của bạn.\\n\\nBước 4: Lúc này, màn hình Edit Whitelist hiển thị, bạn có thể nhập địa chỉ IP mà bạn muốn cho phép truy cập vào Cluster sau đó chọn Add.\\n\\nBước 5: Lặp lại bước 4 nếu bạn muốn thêm nhiều Whitelist IP cho Cluster của bạn. Bạn cũng có thể chọn Delete để xóa Whitelist IP mà bạn đã thêm trước đó.\\n\\nBước 6: Chọn Save để lưu thông tin hoặc Cancel để hủy bỏ việc lưu các thông số này.'),\n",
       " Document(metadata={'source': './../../data/vks/clusters/public-cluster-va-private-cluster.md'}, page_content='Public Cluster và Private Cluster\\n\\n1. Public Cluster\\n\\nKhi bạn khởi tạo một Public Cluster với Public Node Group, hệ thống VKS sẽ:\\n\\nTạo VM có Floating IP ( tức có IP Public). Lúc này các VM (Node) này có thể join trực tiếp vào cụm K8S thông qua Public IP này. Bằng cách sử dụng Public Cluster và Public Node Group, bạn có thể dễ dàng tạo các cụm Kubernetes và thực hiện expose service mà không cần sử dụng Load Balancer. Việc này sẽ góp phần tiết kiệm chi phí cho cụm của bạn.\\n\\nKhi bạn khởi tạo một Public Cluster với Private Node Group, hệ thống VKS sẽ:\\n\\nTạo VM không có Floating IP ( tức không có IP Public). Lúc này các VM (Node) này không thể join trực tiếp vào cụm K8S. Để các VM này có thể join vào cụm K8S, bạn cần phải sử dụng một NAT Gateway (NATGW). NATGW hoạt động như một trạm chuyển tiếp, cho phép các VM kết nối với cụm K8S mà không cần IP Public. Với VNG Cloud, chúng tôi khuyến cáo bạn sử dụng Pfsense hoặc Palo Alto như một NATGW cho Cluster của bạn. Pfsense sẽ giúp bạn quản lý lưu lượng mạng đến và đi (inbound và outbound traffic) một cách hiệu quả, đảm bảo an ninh mạng và quản lý truy cập. Bên cạnh đó, việc sử dụng Private Node Group sẽ giúp bạn kiểm soát các ứng dụng trong cụm được bảo mật hơn, cụ thể bạn có thể thực hiện giới hạn quyền truy cập control plane thông qua tính năng Whitelist IP.\\n\\n2. Private Cluster\\n\\nKhi bạn khởi tạo một Public Cluster với Public/ Private Node Group, hệ thống VKS sẽ:\\n\\nĐể nâng cao bảo mật cho cluster của bạn, chúng tôi đã cho ra mắt mô hình private cluster. Tính năng Private Cluster giúp cho cụm K8S của bạn được bảo mật nhất có thể, mọi kết nối hoàn toàn là private từ kết nối giữa nodes tới control plane, kết nối từ client tới control plane, hay kết nối từ nodes tới các sản phẩm dịch vụ khác trong VNG Cloud như: vStorage, vCR, vMonitor, VNGCloud APIs,...Private Cluster là lựa chọn lý tưởng cho các dịch vụ yêu cầu kiểm soát truy cập chặt chẽ, đảm bảo tuân thủ các quy định về bảo mật và quyền riêng tư dữ liệu.'),\n",
       " Document(metadata={'source': './../../data/vks/clusters/upgrading-control-plane-version.md'}, page_content='Upgrading Control Plane Version\\n\\nHiện tại, hệ thống VKS của chúng tôi đã hỗ trợ bạn nâng cấp Control Plane Version, bạn có thể:\\n\\nNâng cấp Minor Version mới hơn (ví dụ: 1.24 lên 1.25)\\n\\nNâng cấp Patch Version mới hơn (ví dụ: 1.24.2-VKS.100 lên 1.24.5-VKS.200)\\n\\nĐể thực hiện nâng cấp phiên bản Control Plane, bạn có thể thực hiện theo hướng dẫn sau:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn menu Kubernetes Cluster.\\n\\nBước 3: Chọn biểu tượng và chọn Upgrade control plane version để thực hiện nâng cấp version control plane.\\n\\nBước 4: Bạn có thể lựa chọn phiên bản mới cho control plane. Phiên bản mới cần hợp lệ và tương thích với phiên bản hiện tại của cluster. Cụ thể: bạn có thể chọn:\\n\\nNâng cấp Minor Version mới hơn (ví dụ: 1.24 lên 1.25)\\n\\nNâng cấp Patch Version mới hơn (ví dụ: 1.24.2-VKS.100 lên 1.24.5-VKS.200)\\n\\nBước 4: Hệ thống VKS sẽ thực hiện nâng cấp các thành phần Control Plane của Cluster lên phiên bản mới. Sau khi việc nâng cấp hoàn tất, trạng thái Cluster trở về ACTIVE.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nViệc nâng cấp Control Plane Version là không bắt buộc và độc lập với việc nâng cấp Node Group Version. Tuy nhiên Control Plane Version và Node Group Version trong cùng một Cluster không được lệch quá 1 minor version. Bên cạnh đó, hệ thống VKS tự động nâng cấp Control Plane Version khi phiên bản K8S Version hiện tại đang sử dụng cho Cluster của bạn quá thời hạn được nhà cung cấp hỗ trợ.\\n\\nTrong quá trình nâng cấp Control Plane Version, bạn không thể thực hiện các hành động khác trên Cluster của bạn.\\n\\nBên dưới là một vài lưu ý trước, trong và sau quá trình nâng cấp, vui lòng tham khảo thêm:\\n\\nTrước khi thực hiện:\\n\\nSao lưu dữ liệu: Nên sao lưu dữ liệu của cluster trước khi nâng cấp để đảm bảo an toàn trong trường hợp nâng cấp thất bại.\\n\\nKiểm tra phiên bản hiện tại: Truy cập Releases để tham khảo danh sách các phiên bản được hỗ trợ. Chọn phiên bản mới hợp lệ và tương thích với phiên bản hiện tại của cluster.\\n\\nĐảm bảo tính sẵn sàng của cluster: Cluster phải đang ở trạng thái hoạt động (ACTIVE) và tất cả các node phải HEALTHY.\\n\\nNgừng các tác vụ đang chạy: Ngừng các tác vụ đang chạy trên cluster để tránh ảnh hưởng đến quá trình nâng cấp.\\n\\nTrong khi thực hiện:\\n\\nTheo dõi trạng thái cluster: Theo dõi trạng thái cluster trong quá trình nâng cấp. Trạng thái cluster sẽ chuyển sang UPDATING và sau khi hoàn tất sẽ trở về ACTIVE.\\n\\nKiểm tra nhật ký hệ thống: Kiểm tra nhật ký hệ thống để phát hiện bất kỳ lỗi hoặc cảnh báo nào trong quá trình nâng cấp.\\n\\nSau khi thực hiện:\\n\\nKiểm tra tính sẵn sàng của cluster: Xác nhận rằng cluster đã được nâng cấp thành công và tất cả các node đang hoạt động bình thường.\\n\\nKiểm tra các ứng dụng: Kiểm tra các ứng dụng đang chạy trên cluster để đảm bảo chúng hoạt động bình thường sau khi nâng cấp.\\n\\nLưu ý:\\n\\nViệc nâng cấp Control Plane Version có thể mất một khoảng thời gian tùy thuộc vào kích thước và độ phức tạp của cluster.\\n\\nTrong một số trường hợp hiếm gặp, việc nâng cấp Control Plane Version có thể thất bại. Nếu điều này xảy ra, hệ thống VKS sẽ tự động rollback cluster về phiên bản hiện tại. {% endhint %}'),\n",
       " Document(metadata={'source': './../../data/vks/clusters/stop-poc.md'}, page_content='POC và Stop POC\\n\\nTrước khi tìm hiểu cách Stop POC cho tài nguyên của bạn trên VKS, bạn nên hiểu rõ các khái niệm cũng các hành động mà bạn có thể thao tác đối với tài nguyên POC. Chi tiết tham khảo thêm tại đây.\\n\\nKhởi tạo tài nguyên VKS thông qua ví POC\\n\\nĐể khởi tạo Cluster thông qua ví POC, hãy thực hiện các bước như sau;\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn Activate.\\n\\nBước 3: Chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn. Sau khi Activate thành công, bạn hãy chọn Create a Cluster\\n\\nBước 4: Tại màn hình khởi tạo Cluster, chúng tôi đã thiết lập thông tin cho Cluster và một Default Node Group cho bạn, bạn có thể giữ các giá trị mặc định này hoặc điều chỉnh các thông số mong muốn cho Cluster và Node Group của bạn tại Cluster Configuration, Default Node Group Configuration, Plugin. Mặc định chúng tôi sẽ khởi tạo cho bạn một Public Cluster với Public Node Group.\\n\\nBước 5: Chọn POC như hình bên dưới\\n\\nBước 6: Chọn Create Kubernetes cluster. Hãy chờ vài phút để chúng tôi khởi tạo Cluster của bạn, trạng thái của Cluster lúc này là Creating.\\n\\nBước 7: Khi trạng thái Cluster là Active, bạn có thể xem thông tin Cluster, thông tin Node Group bằng cách chọn vào Cluster Name tại cột Name.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi bạn khởi tạo Cluster và chọn sử dụng ví POC, chúng tôi đã tự động tạo Control Plane, Node, Volume và Private Service Endpoint (nếu bạn chọn sử dụng) thông qua ví POC. Đối với các tài nguyên khác như\\n\\nPVC: khi thực hiện khởi tạo qua yaml, bạn vui lòng thêm tham số isPOC: \"true\" vào file yaml này. Tham khảo ví dụ bên dưới.\\n\\nLoadBalancer: khi thực hiện khởi tạo qua yaml, bạn vui lòng thêm annotation vks.vngcloud.vn/is-poc: \"true\" vào file yaml này. Tham khảo ví dụ bên dưới.\\n\\nDo các resource Load Balancer và PVC được quản lý thông qua YAML, sau khi Stop POC, nếu trong file YAML của bạn vẫn có tham số isPOC : true hoặc is-poc : true, trong trường hợp bạn xóa Load Balancer từ Portal vLB và xóa tham sốload-balancer-id trong yaml, lúc này hệ thống sẽ tự động tạo lại các resource này thông qua ví POC. Để tạo Load Balancer và PVC khác bằng tiền thật, vui lòng thay đổi tham số isPOC thành false. (isPOC : false hoặc is-poc : false). Chúng tôi khuyến cáo bạn nên thực hiện điều chỉnh tham số này trước khi thực hiện Stop POC cho Cluster của bạn. {% endhint %}\\n\\nBên dưới là yaml mẫu để tạo Load Balancer thông qua số dư ví POC:\\n\\n```bash apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: default spec: replicas: 2 selector: matchLabels: unique-label: app1 template: metadata: labels: unique-label: app1 same-label: vngcloud spec: containers: - name: nginx-deployment image: nginx ports: - containerPort: 80 name: http\\n\\napiVersion: apps/v1 kind: Deployment metadata: name: python-http-server spec: replicas: 2 selector: matchLabels: app: python-http-server template: metadata: labels: app: python-http-server same-label: vngcloud spec: containers: - name: python-http-server image: python:3.9-slim command: [\"python\", \"-m\", \"http.server\", \"8080\"] ports: - containerPort: 8080 name: http\\n\\napiVersion: v1 kind: Service metadata: name: nginx-service namespace: default annotations: vks.vngcloud.vn/is-poc: \"true\" spec: ports: - port: 80 targetPort: http protocol: TCP name: http-server selector: same-label: vngcloud type: LoadBalancer\\n\\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: webapp-02 namespace: default annotations: vks.vngcloud.vn/is-poc: \"true\" spec: ingressClassName: vngcloud defaultBackend: service: name: nginx-service port: name: http-server ```\\n\\nVà đây là yaml mẫu để tạo PVC thông qua số dư ví POC:\\n\\n```bash apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: my-expansion-storage-class # [1] The StorageClass name, CAN be changed provisioner: bs.csi.vngcloud.vn # The VNG-CLOUD CSI driver name parameters: type: vtype-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx # The volume type UUID isPOC: \"true\" allowVolumeExpansion: true # MUST set this value to turn on volume expansion feature\\n\\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-expansion-pvc # [2] The PVC name, CAN be changed spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi # [3] The PVC size, CAN be changed, this value MUST be in the valid range of the proper volume type storageClassName: my-expansion-storage-class # [4] The StorageClass name, MUST be the same as [1]\\n\\napiVersion: v1 kind: Pod metadata: name: nginx # [5] The Pod name, CAN be changed spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP volumeMounts: - mountPath: /var/lib/www/html name: my-volume-name # MUST be the same as [6] volumes: - name: my-volume-name # [6] The volume name, CAN be changed persistentVolumeClaim: claimName: my-expansion-pvc # MUST be the same as [2] readOnly: false ```\\n\\nStop POC\\n\\nTrước khi hết hạn sử dụng ví POC cho Cluster, bạn có hai lựa chọn chính:\\n\\nXóa Cluster đang POC này và tạo lại Cluster bình thường khác.\\n\\nThực hiện Stop POC để gia hạn Cluster đang POC thành Cluster bình thường.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nĐể thực hiện Stop POC cho một Cluster và các thành phần liên quan của Cluster, bạn cần thực hiện:\\n\\nBước 1: Thực hiện cập nhật tham số isPOC : true hoặc is-poc : true về isPOC : false hoặc is-poc : false trong các yaml cho Load Balancer, PVC nếu có.\\n\\nBước 2: Thực hiện Stop POC cho Cluster thông qua nút Stop POC trên VKS Portal theo hướng dẫn bên dưới.\\n\\nBước 3: Thực hiện Thanh toán cho các tài nguyên thông qua tiền thật. {% endhint %}\\n\\nĐể tiếp tục sử dụng tài nguyên vừa dừng POC như một tài nguyên bình thường (với mục đích giữ nguyên cấu hình), người dùng có thể thực hiện:\\n\\nBước 1: Truy cập vào VKS Portal, chọn Cluster mà bạn muốn Stop POC.\\n\\nBước 2: Chọn nút Stop POC phía trên góc phải màn hình.\\n\\nBước 3: Lúc này, màn hình hiển thị danh sách tất cả các Server và Volume (bao gồm cả Boot Volume và PVC mà bạn attach vào node trong Cluster của bạn) , Load Balancer, Endpoint thuộc Cluster đang có trạng thái POC. Bạn có thể kiểm tra thông tin sau đó chọn Stop POC\\n\\nBước 4: Tiến hành thanh toán tài nguyên bằng tiền thật, bạn có thể lựa chọn Chu kỳ sử dụng mong muốn, bật tắt Tự động gia hạn, nhập Coupon nếu có và chọn Continue để thực hiện Thanh toán tài nguyên\\n\\nBước 5: Thực hiện thanh toán bằng số dư credit hoặc qua các hình thức thanh toán khác nếu có.\\n\\nĐể đảm bảo VKS hoạt động chính xác, việc thực hiện Stop POC cần được tiến hành trên VKS Portal thay vì thực hiện riêng lẻ trên vServer Portal hoặc vConsole. Nếu bạn đã thực hiện stop POC riêng lẻ cho từng resource trên vServer Portal trước đó, bạn vẫn cần thực hiện Stop POC cho Cluster tại VKS Portal, lúc này, màn hình sẽ hiển thị như sau. Bạn hãy nhẫn Stop để tắt lựa chọn POC cho Cluster của bạn.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nSau khi stop POC trên VKS, nút \"Stop POC\" sẽ tiếp tục hiển thị nếu chúng tôi thấy vẫn còn resource chưa được Stop POC sau khi thực hiện trên VKS Portal. Bạn có thể tiếp tục chọn và thực hiện Stop POC cho đến khi tất cả các resource được chuyển về resource thật.\\n\\nĐối với loại resource Snapshot, bạn không thể chỉ định snapshot sử dụng ví POC từ VKS. Để thực hiện tạo Snapshot qua ví POC, tại vServer Portal, vui lòng chọn Activate Snapshot, sau đó tại màn hình Checkout, vui lòng chọn sử dụng ví POC. Lúc này tất cả các resource snapshot của bạn sẽ được tạo qua ví POC. Do đó, việc stop POC cần được bạn thực hiện thông qua vConsole hoặc vServer Portal. Tham khảo thêm hình bên dưới. {% endhint %}'),\n",
       " Document(metadata={'source': './../../data/vks/migration/migrate-cluster-from-vks-to-vks.md'}, page_content='Migrate Cluster from VKS to VKS\\n\\nĐể migrate một Cluster từ hệ thống VKS tới hệ thống VKS, hãy thực hiện theo các bước theo tài liệu này.\\n\\nĐiều kiện cần\\n\\nThực hiện tải xuống helper bash script và grand execute permission cho file này (velero_helper.sh)\\n\\n(Optional) Triển khai một vài service để kiểm tra tính đúng đắn của việc migrate. Giả sử, tại Cluster nguồn, tôi đã triển khai một service nginx như sau:\\n\\nFile triển khai:\\n\\n```yaml apiVersion: v1 kind: Service metadata: name: nginx namespace: mynamespace labels: app: nginx spec: ports: - port: 80 name: web selector: app: nginx type: NodePort\\n\\napiVersion: apps/v1 kind: StatefulSet metadata: name: web namespace: mynamespace spec: selector: matchLabels: app: nginx serviceName: \"nginx\" replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: web volumeMounts: - name: disk-ssd mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: disk-ssd namespace: mynamespace spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: ssd-3000 resources: requests: storage: 40Gi ```\\n\\nbash kubectl exec -n mynamespace -it web-0 bash cd /usr/share/nginx/html echo -e \"<html>\\\\n<head>\\\\n <title>MyVNGCloud</title>\\\\n</head>\\\\n<body>\\\\n <h1>Hello, MyVNGCloud</h1>\\\\n</body>\\\\n</html>\" > index.html\\n\\nLúc này, khi bạn truy cập vào Public IP của Node, bạn sẽ thấy \"Hello, MyVNGCloud\".\\n\\nChuẩn bị cluster đích (Prepare target resource)\\n\\nTrên hệ thống VKS, bạn cần thực hiện khởi tạo một Cluster theo hướng dẫn tại đây. Đảm bảo rằng cấu hình của cluster đích giống với cấu hình của cluster nguồn.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nĐể việc migrate thành công, trên Cluster đích, bạn cần đảm bảo các yêu cầu sau:\\n\\nLượng resource cần thiết như số lượng node, cấu hình instance của node,...\\n\\nNode labels và node taints giống cluster cũ.\\n\\nStorage Class tương ứng hoặc thay thế. {% endhint %}\\n\\n[Optional] Migrate resources private outside cluster\\n\\nMigrating resources private outside cluster (di chuyển tài nguyên riêng tư bên ngoài cụm) là quá trình di chuyển tài nguyên riêng tư nằm ngoài Cluster nguồn sang một nơi mà Cluster đích có thể sử dụng. Ví dụ, bạn có thể có những tài nguyên riêng tư như image, database,... Lúc này, trước khi bắt đầu migrate, bạn cần tự thực hiện việc migrate các tài nguyên này. Ví dụ, nếu bạn cần:\\n\\nMigrate Container Images: bạn có thể migrate image tới VNGCloud Container Registry thông qua hướng dẫn tại đây.\\n\\nMigrate Databases: bạn có thể sử dụng Relational Database Service (RDS) và Object Storage Service (OBS) tùy theo nhu cầu sử dụng của bạn. Sau khi việc migration hoàn tất, hãy nhớ config lại database cho applications của bạn trên VKS Cluster.\\n\\nMigrate Storage: bạn có thể sử dụng NFS Server của vServer.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nSau khi bạn thực hiện migrate các resource ngoài Cluster, bạn cần đảm bảo Cluster đích kết nối được tới các resource đã migrate này. {% endhint %}\\n\\nCài đặt Velero trên cả 2 cluster nguồn và cluster đích (Install Velero tool)\\n\\nSau khi bạn đã thực hiện migrate các tài nguyên private ngoài cluster, bạn có thể sử dụng công cụ migration để sao lưu (backup) và khôi phục (restore) application trên cluster nguồn và cluster đích.\\n\\nTạo một vStorage Project, Container làm nơi nhận dữ liệu backup của cụm theo hướng dẫn tại đây.\\n\\nKhởi tạo S3 key tương ứng với vStorage Project này theo hướng dẫn tại đây.\\n\\nVí dụ, tôi đã khởi tạo một vStorage Project, Container có thông tin sau: Region: HCM03, Container: mycontainer, Endpoint: https://hcm03.vstorage.vngcloud.vn.\\n\\nTrên cả 2 Cluster (source and target)\\n\\nTạo file credentials-velero với nội dung sau:\\n\\nyaml [default] aws_access_key_id=________________________ aws_secret_access_key=________________________\\n\\nCài đặt Velero CLI:\\n\\nbash curl -OL https://github.com/vmware-tanzu/velero/releases/download/v1.13.2/velero-v1.13.2-linux-amd64.tar.gz tar -xvf velero-v1.13.2-linux-amd64.tar.gz cp velero-v1.13.2-linux-amd64/velero /usr/local/bin\\n\\nCài đặt Velero trên 2 cụm của bạn theo lệnh:\\n\\nbash velero install --provider aws \\\\ --plugins velero/velero-plugin-for-aws:v1.9.0,velero/velero-plugin-for-csi:v0.7.0 \\\\ --secret-file ./credentials-velero \\\\ --bucket ________________________ \\\\ --backup-location-config region=hcm03,s3ForcePathStyle=\"true\",s3Url=https://hcm03.vstorage.vngcloud.vn \\\\ --use-node-agent \\\\ --features=EnableCSI\\n\\nbash velero client config set features=EnableCSI\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi bạn thực hiện migrate cluster từ VKS tới VKS, chúng tôi khuyến cáo bạn sử dụng Snapshot để migrate Volume của bạn từ cluster nguồn qua cluster đích. {% endhint %}\\n\\nCài đặt plugin VNGCloud Snapshot Controller trên 2 cluster theo lệnh:\\n\\nbash helm repo add vks-helm-charts https://vngcloud.github.io/vks-helm-charts helm repo update helm install vngcloud-snapshot-controller vks-helm-charts/vngcloud-snapshot-controller \\\\ --replace --namespace kube-system\\n\\nTại Cluster nguồn\\n\\nAnnotate các Persistent Volume cần backup. Mặc định velero sẽ không backup volume. Bạn có thể chạy lệnh dưới để annotate backup tất cả volume.\\n\\nbash ./velero_helper.sh mark_volume -c\\n\\nNgoài ra, bạn có thể đánh dấu không backup các resource của system bằng lệnh sau:\\n\\nbash ./velero_helper.sh mark_exclude -c\\n\\nThực hiện apply file bên dưới để tạo default VolumeSnapshotClass:\\n\\n```yaml apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: vngcloud-vsclass labels: velero.io/csi-volumesnapshot-class: \"true\" driver: bs.csi.vngcloud.vn deletionPolicy: Delete\\n\\nuser can choose the VolumeSnapshotClass by setting annotation velero.io/csi-volumesnapshot-class_disk.csi.cloud.com: \"test-snapclass\" on backup resource.\\n\\nuser can choose the VolumeSnapshotClass by setting annotation velero.io/csi-volumesnapshot-class: \"test-snapclass\" on PersistentVolumeClaim resource.\\n\\n```\\n\\nThực hiện backup theo cú pháp:\\n\\nbash velero backup create vks-full-backup \\\\ --exclude-namespaces velero \\\\ --include-cluster-resources=true \\\\ --wait\\n\\nbash velero backup describe vks-full-backup --details\\n\\nTại Cluster đích\\n\\nThực hiện restore theo lệnh:\\n\\nbash velero restore create --from-backup vks-full-backup \\\\ --exclude-resources=\"MutatingWebhookConfiguration,ValidatingWebhookConfiguration\"\\n\\nbash velero restore create --from-backup vks-full-backup'),\n",
       " Document(metadata={'source': './../../data/vks/migration/README.md'}, page_content='Migration\\n\\nTổng quan\\n\\nMigration từ một cluster sang một cluster là quá trình chuyển dữ liệu, ứng dụng, và các dịch vụ từ một nhóm máy chủ này sang một nhóm máy chủ khác. Mục đích của việc này thường là để nâng cấp hệ thống, tăng cường tính sẵn sàng và khả năng chịu lỗi, hoặc để mở rộng quy mô hệ thống.\\n\\nMô hình tổng quan\\n\\nCác bước thực hiện\\n\\nCụ thể:\\n\\nBước 1: Chuẩn bị cluster đích (Prepard target resource): trên hệ thống VKS, bạn cần thực hiện khởi tạo một Cluster theo hướng dẫn tại đây. Đảm bảo rằng cấu hình của cluster đích giống với cấu hình của cluster nguồn.\\n\\nBước 2 [Optional]: Nếu cluster của bạn có các tài nguyên riêng tư như image, database, storage...Lúc này, trước khi bắt đầu migrate, bạn cần chủ động tự thực hiện việc migrate các tài nguyên này.\\n\\nBước 3: Cài đặt Velero trên cả 2 cluster nguồn và cluster đích(Install Velero tool): sau khi bạn đã thực hiện migrate các tài nguyên private ngoài cluster, bạn có thể sử dụng công cụ migration để sao lưu (backup) và khôi phục (restore) application trên cluster nguồn và cluster đích.\\n\\nBước 4: Sao lưu (Backup): Để sao lưu tài nguyên, hãy sử dụng công cụ Velero để tạo đối tượng sao lưu trong cluster nguồn. Velero sẽ thực hiện truy vấn, đóng gói dữ liệu và tải chúng lên một S3 Compatible Object Storage.\\n\\nBước 5: Khôi phục (Restore): Trong quá trình khôi phục tại cluster đích, Velero sẽ thực hiện tải dữ liệu sao lưu xuống cụm mới và triển khai lại tài nguyên dựa trên tệp JSON.\\n\\nBước 6 [Optional]: Update resource config: Sau khi tài nguyên của cluster đích được triển khai đúng cách, bạn có thể thực hiện switch traffic cho dịch vụ của bạn. Sau khi xác nhận rằng tất cả các dịch vụ đều chạy bình thường, bạn có thể thực hiện xóa cluster nguồn.\\n\\nBên dưới là hướng dẫn chi tiết các trường hợp phổ biến khi bạn thực hiện migrate workload từ một Cluster sang một Cluster khác, bạn có thể tham khảo và làm theo hướng dẫn tại:\\n\\nMigrate Cluster from VKS to VKS\\n\\nMigrate Cluster from vContainer to VKS\\n\\nMigrate Cluster from another platform to VKS'),\n",
       " Document(metadata={'source': './../../data/vks/migration/migrate-cluster-from-other-to-vks.md'}, page_content='Migrate Cluster from another platform to VKS\\n\\nĐể migrate một Cluster từ hệ thống Cloud Provider hoặc On-premise tới hệ thống VKS, hãy thực hiện theo các bước theo tài liệu này.\\n\\nĐiều kiện cần\\n\\nThực hiện tải xuống helper bash script và grand execute permission cho file này (velero_helper.sh)\\n\\n(Optional) Triển khai một vài service để kiểm tra tính đúng đắn của việc migrate. Giả sử, tại Cluster nguồn, tôi đã triển khai một service nginx như sau:\\n\\nFile triển khai:\\n\\n```yaml apiVersion: v1 kind: Service metadata: name: nginx namespace: mynamespace labels: app: nginx spec: ports: - port: 80 name: web selector: app: nginx type: NodePort\\n\\napiVersion: apps/v1 kind: StatefulSet metadata: name: web namespace: mynamespace spec: selector: matchLabels: app: nginx serviceName: \"nginx\" replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: web volumeMounts: - name: disk-ssd mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: disk-ssd namespace: mynamespace spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: standard-rwo resources: requests: storage: 40Gi ```\\n\\nbash kubectl exec -n mynamespace -it web-0 bash cd /usr/share/nginx/html echo -e \"<html>\\\\n<head>\\\\n <title>MyVNGCloud</title>\\\\n</head>\\\\n<body>\\\\n <h1>Hello, MyVNGCloud</h1>\\\\n</body>\\\\n</html>\" > index.html * Lúc này, khi bạn truy cập vào Public IP của Node, bạn sẽ thấy \"Hello, MyVNGCloud\".\\n\\nChuẩn bị cluster đích (Prepare target resource)\\n\\nTrên hệ thống VKS, bạn cần thực hiện khởi tạo một Cluster theo hướng dẫn tại đây. Đảm bảo rằng cấu hình của cluster đích giống với cấu hình của cluster nguồn.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nĐể việc migrate thành công, trên Cluster đích, bạn cần đảm bảo các yêu cầu sau:\\n\\nLượng resource cần thiết như số lượng node, cấu hình instance của node,...\\n\\nNode labels và node taints giống cluster cũ.\\n\\nStorage Class tương ứng hoặc thay thế. {% endhint %}\\n\\n[Optional] Migrate resources private outside cluster\\n\\nMigrating resources private outside cluster (di chuyển tài nguyên riêng tư bên ngoài cụm) là quá trình di chuyển tài nguyên riêng tư nằm ngoài Cluster nguồn sang một nơi mà Cluster đích có thể sử dụng. Ví dụ, bạn có thể có những tài nguyên riêng tư như image, database,... Lúc này, trước khi bắt đầu migrate, bạn cần tự thực hiện việc migrate các tài nguyên này. Ví dụ, nếu bạn cần:\\n\\nMigrate Container Images: bạn có thể migrate image tới VNGCloud Container Registry thông qua hướng dẫn tại đây.\\n\\nMigrate Databases: bạn có thể sử dụng Relational Database Service (RDS) và Object Storage Service (OBS) tùy theo nhu cầu sử dụng của bạn. Sau khi việc migration hoàn tất, hãy nhớ config lại database cho applications của bạn trên VKS Cluster.\\n\\nMigrate Storage: bạn có thể sử dụng NFS Server của vServer.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nSau khi bạn thực hiện migrate các resource ngoài Cluster, bạn cần đảm bảo Cluster đích kết nối được tới các resource đã migrate này. {% endhint %}\\n\\nCài đặt Velero trên cả 2 cluster nguồn và cluster đích (Install Velero tool)\\n\\nSau khi bạn đã thực hiện migrate các tài nguyên private ngoài cluster, bạn có thể sử dụng công cụ migration để sao lưu (backup) và khôi phục (restore) application trên cluster nguồn và cluster đích.\\n\\nTạo một vStorage Project, Container làm nơi nhận dữ liệu backup của cụm theo hướng dẫn tại đây.\\n\\nKhởi tạo S3 key tương ứng với vStorage Project này theo hướng dẫn tại đây.\\n\\nVí dụ, tôi đã khởi tạo một vStorage Project, Container có thông tin sau: Region: HCM03, Container: mycontainer, Endpoint: https://hcm03.vstorage.vngcloud.vn.\\n\\nTrên cả 2 Cluster (source and target)\\n\\nTạo file credentials-velero với nội dung sau:\\n\\nyaml [default] aws_access_key_id=________________________ aws_secret_access_key=________________________ * Cài đặt Velero CLI:\\n\\nbash curl -OL https://github.com/vmware-tanzu/velero/releases/download/v1.13.2/velero-v1.13.2-linux-amd64.tar.gz tar -xvf velero-v1.13.2-linux-amd64.tar.gz cp velero-v1.13.2-linux-amd64/velero /usr/local/bin * Cài đặt Velero trên 2 cụm của bạn theo lệnh:\\n\\nbash velero install \\\\ --provider aws \\\\ --plugins velero/velero-plugin-for-aws:v1.9.0 \\\\ --use-node-agent \\\\ --use-volume-snapshots=false \\\\ --secret-file ./credentials-velero \\\\ --bucket ______________________________ \\\\ --backup-location-config region=hcm03,s3ForcePathStyle=\"true\",s3Url=https://hcm03.vstorage.vngcloud.vn\\n\\nĐối với Cluster trên Amazon Elastic Kubernetes Service (EKS)\\n\\nTại Cluster nguồn\\n\\nAnnotate các Persistent Volume cần backup. Mặc định velero sẽ không backup volume. Bạn có thể chạy lệnh dưới để annotate backup tất cả volume.\\n\\nyaml ./velero_helper.sh mark_volume -c * Ngoài ra, bạn có thể đánh dấu không backup các resource của system bằng lệnh sau:\\n\\nyaml ./velero_helper.sh mark_exclude -c * Thực hiện backup theo cú pháp:\\n\\nbash velero backup create eks-cluster --include-namespaces \"\" \\\\ --include-cluster-resources=true \\\\ --wait\\n\\nbash velero backup create eks-namespace --exclude-namespaces velero \\\\ --wait\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nBạn phải tạo 2 phiên bản backup cho Cluster Resource và Namespace Resource. {% endhint %}\\n\\nTại Cluster đích\\n\\nTạo file mapping Storage Class giữa Cluster nguồn và đích:\\n\\nyaml apiVersion: v1 kind: ConfigMap metadata: name: change-storage-class-config namespace: velero labels: velero.io/plugin-config: \"\" velero.io/change-storage-class: RestoreItemAction data: _______old_storage_class_______: _______new_storage_class_______ # <= Adjust here _______old_storage_class_______: _______new_storage_class_______ # <= Adjust here * Thực hiện restore theo lệnh:\\n\\nbash velero restore create --item-operation-timeout 1m --from-backup eks-cluster \\\\ --exclude-resources=\"MutatingWebhookConfiguration,ValidatingWebhookConfiguration\"\\n\\nbash velero restore create --item-operation-timeout 1m --from-backup eks-namespace\\n\\nbash velero restore create --item-operation-timeout 1m --from-backup eks-cluster\\n\\nĐối với Cluster trên Google Kubernetes Engine (GKE)\\n\\nTại Cluster nguồn\\n\\nAnnotate các Persistent Volume và lable resource cần loại trừ khỏi bản backup\\n\\nyaml ./velero_helper.sh mark_volume -c * Ngoài ra, bạn có thể đánh dấu không backup các resource của system bằng lệnh sau:\\n\\nyaml ./velero_helper.sh mark_exclude -c * Thực hiện backup theo cú pháp:\\n\\nbash velero backup create gke-cluster --include-namespaces \"\" \\\\ --include-cluster-resources=true \\\\ --wait\\n\\nbash velero backup create gke-namespace --exclude-namespaces velero \\\\ --wait\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nBạn phải tạo 2 phiên bản backup cho Cluster Resource và Namespace Resource. {% endhint %}\\n\\nTại Cluster đích\\n\\nTạo file mapping Storage Class giữa Cluster nguồn và đích:\\n\\nyaml apiVersion: v1 kind: ConfigMap metadata: name: change-storage-class-config namespace: velero labels: velero.io/plugin-config: \"\" velero.io/change-storage-class: RestoreItemAction data: _______old_storage_class_______: _______new_storage_class_______ # <= Adjust here _______old_storage_class_______: _______new_storage_class_______ # <= Adjust here * Thực hiện restore theo lệnh:\\n\\nbash velero restore create --item-operation-timeout 1m --from-backup gke-cluster \\\\ --exclude-resources=\"MutatingWebhookConfiguration,ValidatingWebhookConfiguration\"\\n\\nbash velero restore create --item-operation-timeout 1m --from-backup gke-namespace\\n\\nbash velero restore create --item-operation-timeout 1m --from-backup gke-cluster\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nGoogle Kubernetes Engine (GKE) không cho phép triển khai daemonset trên tất cả các node. Tuy nhiên, Velero chỉ cần triển khai daemonset trên node có mount PV. Giải pháp cho vấn đề này là bạn có thể điều chỉnh taint và toleration của daemonset để chỉ triển khai nó trên node có mount PV.\\n\\nBạn có thể thay đổi yêu cầu tài nguyên mặc định cpu:500m và mem:512M trong bước cài đặt hoặc điều chỉnh khi triển khai yaml. {% endhint %}'),\n",
       " Document(metadata={'source': './../../data/vks/migration/gioi-han-va-han-che.md'}, page_content='Giới hạn và hạn chế\\n\\nKhi sử dụng Velero để migrate Cluster to Cluster, bạn có thể thêm các tùy chọn sau.\\n\\nĐánh dấu các Volume bạn muốn backup và các resource không cần thiết\\n\\nĐể đánh dấu các Volume bạn muốn backup và các resource không cần thiết, đầu tiên bạn cần tài xuống đoạn helper bash script được chung tôi cung cấp sẵn và thực hiện grand execute permission. Chi tiết file mấu bạn có thể xem tại: velero_helper.sh\\n\\n1. Chuyển hostPath Volume thành Persistent Volume để có thể thực hiện backup\\n\\nDo Velero không hỗ trợ sao lưu hostPath Volume, bạn cần phải chuyển hostPath Volume thành Persistent Volume theo hướng dẫn sau đây:\\n\\nĐể list các hostPath Volume đang sử dụng:\\n\\nbash ./helper.sh check_hostPath\\n\\n2. Mark Persistent Volume to include in backup\\n\\nTất cả data Persistent Volumes được lưu trữ trên vStorage. Cần thêm annotation cho tất cả pod dùng PV với volume name: backup.velero.io/backup-volumes=volume1,volume2\\n\\nHoặc có thể tự động tìm các volume bằng cách:\\n\\nbash ./helper.sh mark_volume\\n\\n3. Mark resource in exclude in backup\\n\\nDo VKS hoạt động theo cơ chế Fully Managed Control Plane, nên bạn không cần backup các resource như: calico, kube-dns, kube-scheduler, kube-apiserver,... Ngoài ra, các resource của vContainer như là: magnum-auto-healer, cluster-autoscaler, csi-cinder,... cũng sẽ được bỏ qua.\\n\\nĐánh dầu resource không cần backup thông qua lệnh:\\n\\nbash ./helper.sh mark_exclude\\n\\n4. Check label and taint of node\\n\\nKhi thực hiện migrate, có thể tài nguyên trong Cluster nguồn đang sử dụng label và taint. Bạn cần đảm bảo các label và taint quan trọng này tồn tại trong Cluster đích.\\n\\nKiểm tra lable và taint thông qua lệnh:\\n\\nbash ./helper.sh check_node_label ./helper.sh check_node_taint\\n\\n5. Mapping Storage Class\\n\\nNếu Storage Class của bạn khác nhau giữa Cluster nguồn và Cluster đích, bạn cần chuyển Storage Class giữa 2 cụm. Ví dụ:\\n\\nTại Cluster nguồn, bạn đang có 2 Storage Class sau:\\n\\nbash @ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE sc-iops-200-retain (default) csi.vngcloud.vn Retain Immediate true 81s sc-ssd-10000-delete (default) csi.vngcloud.vn Delete Immediate true 14d\\n\\nBạn có thể tạo file mapping chưa nội dung như ví dụ bên dưới để thực hiện chuyển đổi 2 storage class từ Cluster nguồn thành 2 storage class tại Cluster đích. File này phải được apply tại Cluster đích trước khi bạn chạy lệnh backup:\\n\\nyaml apiVersion: v1 kind: ConfigMap metadata: name: change-storage-class-config namespace: velero labels: velero.io/plugin-config: \"\" velero.io/change-storage-class: RestoreItemAction data: sc-iops-200-retain: ssd-200 sc-ssd-10000-delete: ssd-10000'),\n",
       " Document(metadata={'source': './../../data/vks/migration/migration-cluster-from-vcontainer-to-vks.md'}, page_content='Migration Cluster từ vContainer tới VKS\\n\\nĐể migrate một Cluster từ hệ thống vContainer tới hệ thống VKS, hãy thực hiện theo các bước theo tài liệu này.\\n\\nĐiều kiện cần\\n\\nThực hiện tải xuống helper bash script và grand execute permission cho file này (velero_helper.sh)\\n\\nQuy trình thực hiện\\n\\nQuy trình thực hiện migrate từ vContainer sang vKS (Khách hàng + VNG Cloud)\\n\\nStep 1: Đánh giá version hiện tại của vContainer cluster và tương ứng với vKS cluster sẽ migrate. Lúc này sẽ xảy ra 2 trường hợp (step 2 và step 3)\\n\\nStep 2: Nếu version vContainer thấp hơn so với các version đang được hỗ trợ bởi vKS, thì các Custom Resouces (CRD) cần được xem xét mức độ tương tích với các version kubernetes mới.\\n\\nNếu tương thích với version vKS mới, tiếp tục thực hiện step 3\\n\\nNếu không tương thích, cần manual update và cấu hình lại CRD cũng như các Applications liên quan\\n\\nStep 3: Nếu version vContainer được hỗ trợ bởi vKS (1.27, 1.28 và 1.29), thì cần kiểm tra các resouces trên vContainer trước khi backup. Cần lưu ý với các loại resources sau:\\n\\nPV/PVC: Velero không hỗ trợ backup với type hostPath, chỉ hỗ trợ type Local.\\n\\nIngress resources: Ingress resources được quản lý bởi container-ingress-nginx-controller sau khi migrate sang sẽ không hoạt động được.\\n\\nLabel và Taint node: Velero không thực hiện gắn lại các Label và Taint cho các nodes ở vKS\\n\\nStep 4: Thực hiện backup các resouces trên vContainer cluster\\n\\nStep 5: Thực hiện restore các resources trên vKS cluster\\n\\nStep 6: Thực hiện kiểm tra và những điều chỉnh\\n\\nLưu ý quan trọng:\\n\\n1. Mapping StorageClass trên vKS cluster\\n\\nKiểm tra các StorageClass hiện có trên vContainer và tạo các StorageClass tương ứng trên vKS. Sau đó thực hiện mapping 1:1 StorageClass giữa vContainer và vKS cluster.\\n\\nVí dụ file ConfigMap ở Bước 3.\\n\\n2. Sử dụng PersistentVolume dạng hostPath\\n\\nVelero không hỗ trợ backup volume dạng hostPath, chỉ hỗ trợ dạng local.\\n\\nĐối với các cluster đang sử dụng PV type hostPath, cần thực hiện chuyển sang dạng local như sau:\\n\\nLưu ý: Cần thực hiện xóa và deploy lại Application đang sử dụng PV type hostPath\\n\\nCase 1: Type Local hỗ trợ đường dẫn hostPath\\n\\nVí dụ PV hostPath đang cấu hình mount tại folder /opt/data, thực hiện convert sang type Local và mount vào lại Pods, theo mẫu sau:\\n\\n```bash apiVersion: storage.k8s.io/v1\\n\\nkind: StorageClass\\n\\nmetadata:\\n\\nname: manual\\n\\nprovisioner: kubernetes.io/no-provisioner\\n\\nvolumeBindingMode: WaitForFirstConsumer\\n\\napiVersion: v1\\n\\nkind: PersistentVolume\\n\\nmetadata:\\n\\nname: pv-volume\\n\\nlabels:\\n\\ntype: local\\n\\nspec:\\n\\nstorageClassName: manual\\n\\ncapacity:\\n\\nstorage: 1Gi\\n\\naccessModes:\\n\\nReadWriteOnce\\n\\nlocal:\\n\\npath: \"/opt/data\"\\n\\nnodeAffinity:\\n\\nrequired:\\n\\n  nodeSelectorTerms:\\n\\n  - matchExpressions:\\n\\n    - key: kubernetes.io/hostname\\n\\n      operator: Exists\\n\\napiVersion: v1\\n\\nkind: PersistentVolumeClaim\\n\\nmetadata:\\n\\nname: pv-claim\\n\\nspec:\\n\\nstorageClassName: manual\\n\\naccessModes:\\n\\nReadWriteOnce\\n\\nresources:\\n\\nrequests:\\n\\n  storage: 1Gi\\n\\n```\\n\\nCase 2: Type Local không hỗ trợ đường dẫn hostPath gốc\\n\\nKhi thực hiện tạo các PV/PVC theo Case 1, Pod không mount được PVC và xuất hiện lỗi\\n\\nMountVolume.NewMounter initialization failed for volume \"pvc\" : path \"/mnt/data\" does not exist\\n\\nThực hiện copy data sang một folder mới (ví dụ /var, /opt, /tmp, ...) và thực hiện lại theo Case 1, sau đó mount PVC vào Pod như thường:\\n\\nbash cp -R /mnt/data /var\\n\\nCác bước thực hiện chi tiết\\n\\nBước 1: Cài đặt Velero trên cả 2 cluster (vContainer và vKS)\\n\\nTạo một project vStorage, Container và S3 key tương ứng để làm nơi lưu trữ dữ liệu backup\\n\\nTrên cả 2 cluster:\\n\\nTạo file credentials-velero với nội dung sau:\\n\\nbash [default] aws_access_key_id=________________________ # <= Adjust here aws_secret_access_key=________________________ # <= Adjust here\\n\\nCài đặt Velero CLI\\n\\n```bash curl -OL https://github.com/vmware-tanzu/velero/releases/download/v1.14.1/velero-v1.14.1-linux-amd64.tar.gz\\n\\ntar -xvf velero-v1.14.1-linux-amd64.tar.gz cp velero-v1.14.1 -linux-amd64/velero /usr/local/bin ```\\n\\nCài đặt Velero trên 2 cluster kubernetes\\n\\nbash velero install \\\\ --provider aws \\\\ --plugins velero/velero-plugin-for-aws:v1.9.0 \\\\ --use-node-agent \\\\ --use-volume-snapshots=false \\\\ --secret-file ./credentials-velero \\\\ --bucket __________\\\\ # <= Adjust here --backup-location-config region=hcm03,s3ForcePathStyle=\"true\",s3Url=https://hcm03.vstorage.vngcloud.vn\\n\\nBước 2: Thực hiện backup trên Cluster vContainer\\n\\nĐối với\\n\\nAnnotate các Persistent Volume cần backup\\n\\nbash ./velero_helper.sh mark_volume --confirm\\n\\nAnnotate các resource không backup của kube-system\\n\\nbash ./velero_helper.sh mark_exclude --confirm\\n\\nAnnotate các resource khác (không được mark trong file velero_helper.sh), như CSI, Ingress Controller, hoặc những resources khác không muốn migrate (lưu ý cần mark label hết toàn bộ resources của objects không cần backup).\\n\\nVí dụ như một application bao gồm DaemonSet, Deployment, Pod, ... thì cần mark label cho toàn bộ resources đó\\n\\n```bash\\n\\nThêm label velero.io/exclude-from-backup=true cho từng resources\\n\\nVới Cinder CSI\\n\\nkubectl -n kube-system label StatefulSet/csi-cinder-controllerplugin velero.io/exclude-from-backup=true\\n\\nkubec kubectl -n kube-system label DaemonSet/csi-cinder-nodeplugin velero.io/exclude-from-backup=true\\n\\nVới vcontainer-ingress-nginx-controller\\n\\nkubectl -n kube-system label Deployment/ vcontainer-ingress-nginx-controller velero.io/exclude-from-backup=true\\n\\nkubectl -n kube-system label Deployment/ vcontainer-ingress-nginx-default-backend velero.io/exclude-from-backup=true ```\\n\\nThực hiện kiểm tra và gắn các Labels and Taint trên nodes vContainer vào các nodes vKS trước khi restore\\n\\n```bash\\n\\nKiểm tra các Label nodes\\n\\n./velero_helper.sh check_node_label\\n\\nKiểm tra các Taint nodes\\n\\n./velero_helper.sh check_node_taint ```\\n\\nTạo 2 bản backup cho Cluster resources và Namespace resource theo cú pháp\\n\\n```bash\\n\\nTạo cluster resource backup\\n\\nvelero backup create vcontainer-cluster --include-namespaces \"\" --include-cluster-resources=true --wait\\n\\nTạo cluster namespace backup\\n\\nvelero backup create vcontainer-namespace --exclude-namespaces velero --wait\\n\\nXóa các bản backup (nếu cần)\\n\\nvelero backup delete vcontainer-namespace vcontainer-cluster --confirm ```\\n\\nXem các bản backup đã được tạo và details các resources được backup (chú ý STATUS của backup)\\n\\n```bash\\n\\nList các bản backup được tạo\\n\\nvelero get backup\\n\\nChi tiết của một bản backup\\n\\nvelero backup describe\\n\\nXem logs quá trình backup\\n\\nvelero backup logs\\n\\nBước 3: Thực hiện Restore trên Cluster VKS\\n\\nNếu ở cluster vContainer sử dụng CSI là cinder.csi.openstack.org, cần thực hiện mapping StorageClass giữa 2 cluster vContainer và vKS\\n\\nMapping csi-sc-cinderplugin-nvme-5000 (vContainer) và vngcloud-nvme-5000-delete (vKS), tương tự đối với các StorageClass khác\\n\\nỞ vKS cần tạo các StorageClass tương ứng\\n\\nTạo file sc-mapping.yaml và apply trên cluster VKS\\n\\n```bash apiVersion: v1\\n\\nkind: ConfigMap\\n\\nmetadata:\\n\\nname: change-storage-class-config\\n\\nnamespace: velero\\n\\nlabels:\\n\\nvelero.io/plugin-config: \"\"\\n\\nvelero.io/change-storage-class: RestoreItemAction\\n\\ndata:\\n\\ncsi-sc-cinderplugin-nvme-5000: vngcloud-nvme-5000-delete\\n\\n#_old_storage_class_: _new_storage_class_ # <= Add here ```\\n\\nThực hiện thêm permissions cho Velero được quyền restore data PersistentVolume\\n\\nTạo file add-permission.yaml và apply trên cluster VKS\\n\\n```bash apiVersion: v1\\n\\nkind: ConfigMap\\n\\nmetadata:\\n\\nname: fs-restore-action-config\\n\\nnamespace: velero\\n\\nlabels:\\n\\nvelero.io/plugin-config: \"\"\\n\\nvelero.io/pod-volume-restore: RestoreItemAction\\n\\ndata:\\n\\nsecCtx: |\\n\\ncapabilities:\\n\\n  drop: []\\n\\n  add: []\\n\\nallowPrivilegeEscalation: false\\n\\nreadOnlyRootFilesystem: true\\n\\nrunAsUser: 0\\n\\nrunAsGroup: 0\\n\\n```\\n\\nThực hiện restore theo thứ tự\\n\\nLưu ý, với mỗi lần thực hiện restore, cần kiểm tra quá trình restore đã thành công hay chưa rồi mới tiếp tục thực hiện các command khác\\n\\n```bash\\n\\nKiểm tra restore\\n\\nvelero get restore\\n\\nvelero describe restore\\n\\nbash velero restore create --item-operation-timeout 1m --from-backup vcontainer-cluster --exclude-resources=\"MutatingWebhookConfiguration,ValidatingWebhookConfiguration\"\\n\\nbash velero restore create --item-operation-timeout 1m --from-backup vcontainer-namespace\\n\\nbash velero restore create --item-operation-timeout 1m --from-backup vcontainer-cluster\\n\\nTrường hợp migrate qua VKS vẫn sử dụng vcontainer-nginx-ingress-controller, thì cần thực hiện đổi type Service thành LoadBalancer\\n\\nbash kubectl patch service -n kube-system vcontainer-ingress-nginx-controller -p \\'{\"spec\": {\"type\": \"LoadBalancer\"}}\\''),\n",
       " Document(metadata={'source': './../../data/vks/storage/README.md'}, page_content='Storage'),\n",
       " Document(metadata={'source': './../../data/vks/storage/lam-viec-voi-container-storage-interface-csi/integrate-with-container-storage-interface-csi.md'}, page_content='Integrate with Container Storage Interface (CSI)\\n\\nĐể integrate CSI với Kubernetes cluser, hãy làm theo các bước sau đây:\\n\\nChuẩn bị\\n\\nTạo một Kubernetes cluster trên VNGCloud, hoặc sử dụng một cluster đã có. Lưu ý: đảm bảo bạn đã tải xuống cluster configuration file sau khi cluster được khởi tạo thành công và truy cập vào cluster của bạn.\\n\\nKhởi tạo Service Account và cài đặt VNGCloud BlockStorage CSI Driver\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi bạn thực hiện khởi tạo Cluster theo hướng dẫn bên trên, nếu bạn chưa bật option Enable BlockStore Persistent Disk CSI Driver, mặc định chúng tôi sẽ không cài sẵn plugin này vào Cluster của bạn. Bạn cần tự thực hiện Khởi tạo Service Account và cài đặt VNGCloud BlockStorage CSI Driver theo hướng dẫn bên dưới. Nếu bạn đã bật option Enable BlockStore Persistent Disk CSI Driver, thì chúng tôi đã cài sẵn plugin này vào Cluster của bạn, hãy bỏ qua bước Khởi tạo Service Account, cài đặt VNGCloud BlockStorage CSI Driver và tiếp tục thực hiện theo hướng dẫn kể từ Deploy một Workload.\\n\\nVNGCloud BlockStorage CSI Driver chỉ hỗ trợ attach volume với một node (VM) duy nhất trong suốt vòng đời của volume đó. Nếu bạn có nhu cầu ReadWriteMany, bạn có thể cân nhắc sử dụng NFS CSI Driver, vì nó cho phép nhiều nodes có thể Read và Write trên cùng một volume cùng một lúc. Điều này rất hữu ích cho các ứng dụng cần chia sẻ dữ liệu giữa nhiều pods hoặc services trong Kubernetes. {% endhint %}\\n\\nDeploy một Workload\\n\\nSau đây là hướng dẫn để bạn deploy service nginx trên Kubernetes.\\n\\nBước 1: Tạo Deployment cho Nginx app.\\n\\nTạo file nginx-service.yaml với nội dung sau:\\n\\n```yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-app spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.19.1 ports: - containerPort: 80\\n\\napiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 ``` * Deploy Deployment này bằng lệch:\\n\\nbash kubectl apply -f nginx-service.yaml\\n\\nBước 2: Kiểm tra thông tin Deployment, Service vừa deploy\\n\\nChạy câu lệnh sau đây để kiểm tra Deployment\\n\\nbash kubectl get svc,deploy,pod -owide * Nếu kết quả trả về như bên dưới tức là bạn đã deploy Deployment thành công.\\n\\n```bash NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1\\n\\nNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-app 1/1 1 1 16s nginx nginx:1.19.1 app=nginx\\n\\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-app-7f45b65946-t7d7k 1/1 Running 0 16s 172.16.24.202 ng-3f06013a-f6a5-47ba-a51f-bc5e9c2b10a7-ecea1\\n\\nTạo Persistent Volume\\n\\nTạo file persistent-volume.yaml với nội dung sau:\\n\\n```yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: my-expansion-storage-class # [1] The StorageClass name, CAN be changed provisioner: bs.csi.vngcloud.vn # The VNG-CLOUD CSI driver name parameters: type: vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018 # The volume type UUID allowVolumeExpansion: true # MUST set this value to turn on volume expansion feature\\n\\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-expansion-pvc # [2] The PVC name, CAN be changed spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi # [3] The PVC size, CAN be changed, this value MUST be in the valid range of the proper volume type storageClassName: my-expansion-storage-class # [4] The StorageClass name, MUST be the same as [1]\\n\\napiVersion: v1 kind: Pod metadata: name: nginx # [5] The Pod name, CAN be changed spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP volumeMounts: - mountPath: /var/lib/www/html name: my-volume-name # MUST be the same as [6] volumes: - name: my-volume-name # [6] The volume name, CAN be changed persistentVolumeClaim: claimName: my-expansion-pvc # MUST be the same as [2] readOnly: false ``` * Chạy câu lệnh sau đây để triển khai 1 Pod sử dụng Persistent Volume\\n\\nbash kubectl apply -f persistent-volume.yaml\\n\\nLúc này, hệ thống vServer sẽ tự động tạo một Volume tương ứng với file yaml bên trên, ví dụ:\\n\\nTạo Snapshot\\n\\nSnapshot là phương pháp sao lưu giữ liệu với chi phí thấp, thuận tiện và hiệu quả và có thể được sử dụng để tạo image, phục hồi dữ liệu và phân phối các bản sao dữ liệu. Nếu bạn là người dùng mới chưa từng sử dụng dịch vụ Snapshot, bạn cần thực hiện Activate Snapshot Service (Kích hoạt dịch vụ Snapshot) trước khi có thể tạo Snapshot cho Persistent Volume của bạn.\\n\\nActivate Snapshot Service\\n\\nĐể có thể tạo Snapshot, bạn cần thực hiện Activate Snapshot Service. Bạn sẽ không bị tính phí khi kích hoạt dịch vụ snapshot. Sau khi bạn tạo snapshot, chi phí sẽ được tính dựa trên dung lượng lưu trữ và thời gian lưu trữ của các bản snapshot này. Thực hiện theo các bước sau đây để kích hoạt dịch vụ Snapshot:\\n\\nBước 1: Truy cập vào https://hcm-3.console.vngcloud.vn/vserver/block-store/snapshot/overview\\n\\nBước 2: Chọn Activate Snapshot Service.\\n\\nVí dụ:\\n\\nCài đặt VNGCloud Snapshot Controller\\n\\nCài đặt Helm phiên bản từ 3.0 trở lên. Tham khảo tại https://helm.sh/docs/intro/install/ để biết cách cài đặt.\\n\\nThêm repo này vào cluster của bạn qua lệnh:\\n\\nbash helm repo add vks-helm-charts https://vngcloud.github.io/vks-helm-charts helm repo update * Tiếp tục chạy:\\n\\nbash helm install vngcloud-snapshot-controller vks-helm-charts/vngcloud-snapshot-controller \\\\ --replace --namespace kube-system * Sau khi việc cài đặt hoàn tất, thực hiện kiểm tra trạng thái của vngcloud-blockstorage-csi-driver pods:\\n\\nbash kubectl get pods -n kube-system | grep snapshot-controller\\n\\nVí dụ như ảnh bên dưới là bạn đã cài đặt thành công vngcloud-snapshot-controller:\\n\\nbash NAME READY STATUS RESTARTS AGE snapshot-controller-7fdd984f89-745tg 0/1 ContainerCreating 0 3s snapshot-controller-7fdd984f89-k94wq 0/1 ContainerCreating 0 3s\\n\\nTạo file snapshot.yaml với nội dung sau\\n\\n```yaml apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: my-snapshot-storage-class # [2] The name of the volume snapshot class, CAN be changed driver: bs.csi.vngcloud.vn deletionPolicy: Delete parameters: force-create: \"false\"\\n\\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: my-snapshot-pvc # [4] The name of the snapshot, CAN be changed spec: volumeSnapshotClassName: my-snapshot-storage-class # MUST match with [2] source: persistentVolumeClaimName: my-expansion-pvc # MUST match with [3] ```\\n\\nChạy câu lệnh sau đây để triển khai Volume Snapshot\\n\\nbash kubectl apply -f snapshot.yaml\\n\\nKiểm tra PVC và Snapshot vừa tạo\\n\\nSau khi apply tập tin thành công, bạn có thể kiểm tra danh sách service, pvc thông qua:\\n\\nbash kubectl get sc,pvc,pod -owide\\n\\n```bash NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE storageclass.storage.k8s.io/my-expansion-storage-class bs.csi.vngcloud.vn Delete Immediate true 10m storageclass.storage.k8s.io/sc-iops-200-retain (default) bs.csi.vngcloud.vn Retain Immediate false 2d4h\\n\\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE persistentvolumeclaim/my-expansion-pvc Bound pvc-14456f4a-ee9e-435d-a94f-5a2e820954e9 20Gi RWO my-expansion-storage-class 10m Filesystem\\n\\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx 1/1 Running 0 10m 172.16.24.203 ng-3f06013a-f6a5-47ba-a51f-bc5e9c2b10a7-ecea1\\n\\nThay đổi thông số IOPS của Persistent Volume vừa tạo\\n\\nĐể thay đổi thông số IOPS của Persistent Volume vừa tạo, hãy thực hiện theo các bước sau đây:\\n\\nBước 1: Chạy lệnh bên dưới để liệt kê các PVC trong Cluster của bạn\\n\\nbash kubectl get persistentvolumes\\n\\nBước 2: Chỉnh sửa tệp tin YAML của PVC theo lệnh\\n\\nbash kubectl edit pvc my-expansion-pvc\\n\\nNếu bạn chưa chỉnh sửa IOPS của Persistent Volume lần nào trước đó, khi bạn chạy lệnh trên, bạn hãy thêm 1 annotation bs.csi.vngcloud.vn/volume-type: \"volume-type-id\" . Ví dụ: bên dưới tôi đang thay đổi IOPS của Persistent Volume từ 200 (Volume type id = vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018) lên 1000 (Volume type id = vtype-85b39362-a360-4bbb-9afa-a36a40cea748)\\n\\nyaml apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: bs.csi.vngcloud.vn/volume-type: \"vtype-85b39362-a360-4bbb-9afa-a36a40cea748\" kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"annotations\":{},\"name\":\"my-expansion-pvc\",\"namespace\":\"default\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"20Gi\"}},\"storageClassName\":\"my-expansion-storage-class\"}} pv.kubernetes.io/bind-completed: \"yes\" pv.kubernetes.io/bound-by-controller: \"yes\" volume.beta.kubernetes.io/storage-provisioner: bs.csi.vngcloud.vn volume.kubernetes.io/storage-provisioner: bs.csi.vngcloud.vn creationTimestamp: \"2024-04-21T14:16:53Z\" finalizers: - kubernetes.io/pvc-protection name: my-expansion-pvc namespace: default resourceVersion: \"11041591\" uid: 14456f4a-ee9e-435d-a94f-5a2e820954e9 spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi storageClassName: my-expansion-storage-class volumeMode: Filesystem volumeName: pvc-14456f4a-ee9e-435d-a94f-5a2e820954e9 status: accessModes: - ReadWriteOnce capacity: storage: 20Gi phase: Bound * Nếu bạn đã chỉnh sửa IOPS của Persistent Volume lần nào trước đó, khi bạn chạy lệnh trên, tệp tin yaml của bạn đã có sẵn annotation bs.csi.vngcloud.vn/volume-type: \"volume-type-id\" . Lúc này, hãy chỉnh sửa annotation này về Volume type id có IOPS mà bạn mong muốn.\\n\\nThay đổi Disk Volume của Persistent Volume vừa tạo\\n\\n\\\\ Để thay đổi Disk Volume của Persistent Volume vừa tạo, hãy thực hiện chạy lệnh sau:\\n\\nVí dụ: ban đầu PVC được tạo có kích cỡ 20 Gi, hiện tại tôi sẽ tăng nó lên 30Gi\\n\\nbash kubectl patch pvc my-expansion-pvc -p \\'{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"30Gi\"}}}}\\'\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nBạn chỉ có thể thực hiện tăng Disk Volume mà không thể thực hiện giảm kích thước Disk Volume này. {% endhint %}\\n\\nRestore Persistent Volume từ Snapshot\\n\\nĐể khôi phục Persistent Volume từ Snapshot, bạn hãy thực hiện theo các bước sau:\\n\\nTạo file restore-volume.yaml với nội dung sau:\\n\\nyaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-restore-pvc # The name of the PVC, CAN be changed spec: storageClassName: my-expansion-storage-class dataSource: name: my-snapshot-pvc # MUST match with [4] from the section 5.2 kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 20Gi'),\n",
       " Document(metadata={'source': './../../data/vks/storage/lam-viec-voi-container-storage-interface-csi/README.md'}, page_content='Làm việc với Container Storage Interface (CSI)\\n\\nCSI là gì?\\n\\nContainer Storage Interface (CSI) là một giao diện tiêu chuẩn cho phép các container tương tác với các hệ thống lưu trữ khác nhau. CSI cung cấp một tập hợp các API chung mà các container có thể sử dụng để truy cập và quản lý dữ liệu, bất kể hệ thống lưu trữ cơ bản là gì.'),\n",
       " Document(metadata={'source': './../../data/vks/storage/lam-viec-voi-container-storage-interface-csi/gioi-han-va-han-che-csi.md'}, page_content='Giới hạn và hạn chế CSI'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/integrate-with-container-storage-interface-csi.md'}, page_content='Integrate with Container Storage Interface (CSI)\\n\\nĐiều kiện cần\\n\\nĐể có thể khởi tạo một Cluster và Deploy một Workload, bạn cần:\\n\\nCó ít nhất 1 VPC và 1 Subnet đang ở trạng thái ACTIVE. Nếu bạn chưa có VPC, Subnet nào, vui lòng khởi tạo VPC, Subnet theo hướng dẫn tại đây.\\n\\nCó ít nhất 1 SSH key đang ở trạng thái ACTIVE. Nếu bạn chưa có SSH key nào, vui lòng khởi tạo SSH key theo hướng dẫn tại đây.\\n\\nĐã cài đặt và cấu hình kubectl trên thiết bị của bạn. vui lòng tham khảo tại đây nếu bạn chưa rõ cách cài đặt và sử dụng kuberctl. Ngoài ra, bạn không nên sử dụng phiên bản kubectl quá cũ, chúng tôi khuyến cáo bạn nên sử dụng phiên bản kubectl sai lệch không quá một phiên bản với version của cluster.\\n\\nKhởi tạo Cluster\\n\\nCluster trong Kubernetes là một tập hợp gồm một hoặc nhiều máy ảo (VM) được kết nối lại với nhau để chạy các ứng dụng được đóng gói dạng container. Cluster cung cấp một môi trường thống nhất để triển khai, quản lý và vận hành các container trên quy mô lớn.\\n\\nĐể khởi tạo một Cluster, hãy làm theo các bước bên dưới:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn Activate.\\n\\nBước 3: Chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn. Sau khi Activate thành công, bạn hãy chọn Create a Cluster\\n\\nBước 4: Tại màn hình khởi tạo Cluster, chúng tôi đã thiết lập thông tin cho Cluster và một Default Node Group cho bạn, bạn có thể giữ các giá trị mặc định này hoặc điều chỉnh các thông số mong muốn cho Cluster và Node Group của bạn tại Cluster Configuration, Default Node Group Configuration, Plugin. Khi bạn chọn bật option Enable vLB Native Integration Driver, mặc định chúng tôi sẽ cài sẵn plugin này vào Cluster của bạn.\\n\\nBước 5: Chọn Create Kubernetes cluster. Hãy chờ vài phút để chúng tôi khởi tạo Cluster của bạn, trạng thái của Cluster lúc này là Creating.\\n\\nBước 6: Khi trạng thái Cluster là Active, bạn có thể xem thông tin Cluster, thông tin Node Group bằng cách chọn vào Cluster Name tại cột Name.\\n\\nKết nối và kiểm tra thông tin Cluster vừa tạo\\n\\nSau khi Cluster được khởi tạo thành công, bạn có thể thực hiện kết nối và kiểm tra thông tin Cluster vừa tạo theo các bước:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/k8s-cluster\\n\\nBước 2: Danh sách Cluster được hiển thị, chọn Download Config File để thực hiện tải xuống file kubeconfig. File này sẽ giúp bạn có toàn quyền truy cập vào Cluster của bạn.\\n\\nBước 3: Đổi tên file này thành config và lưu nó vào thư mục \\\\~/.kube/config\\n\\nBước 4: Thực hiện kiểm tra Cluster thông qua lệnh:\\n\\nChạy câu lệnh sau đây để kiểm tra node\\n\\nkubectl get nodes\\n\\nNếu kết quả trả về như bên dưới tức là bạn Cluster của bạn được khởi tạo thành công với 3 node như bên dưới.\\n\\nNAME STATUS ROLES AGE VERSION ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-834b7 Ready <none> 50m v1.28.8 ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-cf652 Ready <none> 23m v1.28.8 ng-0f4ed631-1252-49f7-8dfc-386fa0b2d29b-a8ef0 Ready <none> 28m v1.28.8\\n\\nKhởi tạo Service Account và cài đặt VNGCloud BlockStorage CSI Driver\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi bạn thực hiện khởi tạo Cluster theo hướng dẫn bên trên, nếu bạn chưa bật option Enable BlockStore Persistent Disk CSI Driver, mặc định chúng tôi sẽ không cài sẵn plugin này vào Cluster của bạn. Bạn cần tự thực hiện Khởi tạo Service Account và cài đặt VNGCloud BlockStorage CSI Driver theo hướng dẫn bên dưới. Nếu bạn đã bật option Enable BlockStore Persistent Disk CSI Driver, thì chúng tôi đã cài sẵn plugin này vào Cluster của bạn, hãy bỏ qua bước Khởi tạo Service Account, cài đặt VNGCloud BlockStorage CSI Driver và tiếp tục thực hiện theo hướng dẫn kể từ Deploy một Workload.\\n\\nVNGCloud BlockStorage CSI Driver chỉ hỗ trợ attach volume với một node (VM) duy nhất trong suốt vòng đời của volume đó. Nếu bạn có nhu cầu ReadWriteMany, bạn có thể cân nhắc sử dụng NFS CSI Driver, vì nó cho phép nhiều nodes có thể Read và Write trên cùng một volume cùng một lúc. Điều này rất hữu ích cho các ứng dụng cần chia sẻ dữ liệu giữa nhiều pods hoặc services trong Kubernetes. {% endhint %}\\n\\nDeploy một Workload\\n\\nSau đây là hướng dẫn để bạn deploy service nginx trên Kubernetes.\\n\\nBước 1: Tạo Deployment cho Nginx app.\\n\\nTạo file nginx-service.yaml với nội dung sau:\\n\\n``` apiVersion: apps/v1 kind: Deployment metadata: name: nginx-app spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.19.1 ports: - containerPort: 80\\n\\napiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 ```\\n\\nDeploy Deployment này bằng lệch:\\n\\nkubectl apply -f nginx-service.yaml\\n\\nBước 2: Kiểm tra thông tin Deployment, Service vừa deploy\\n\\nChạy câu lệnh sau đây để kiểm tra Deployment\\n\\nkubectl get svc,deploy,pod -owide\\n\\nNếu kết quả trả về như bên dưới tức là bạn đã deploy Deployment thành công.\\n\\n``` NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1\\n\\nNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-app 1/1 1 1 16s nginx nginx:1.19.1 app=nginx\\n\\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-app-7f45b65946-t7d7k 1/1 Running 0 16s 172.16.24.202 ng-3f06013a-f6a5-47ba-a51f-bc5e9c2b10a7-ecea1\\n\\nTạo Persistent Volume\\n\\nTạo file persistent-volume.yaml với nội dung sau:\\n\\n``` apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: my-expansion-storage-class # [1] The StorageClass name, CAN be changed provisioner: bs.csi.vngcloud.vn # The VNG-CLOUD CSI driver name parameters: type: vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018 # The volume type UUID allowVolumeExpansion: true # MUST set this value to turn on volume expansion feature\\n\\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-expansion-pvc # [2] The PVC name, CAN be changed spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi # [3] The PVC size, CAN be changed, this value MUST be in the valid range of the proper volume type storageClassName: my-expansion-storage-class # [4] The StorageClass name, MUST be the same as [1]\\n\\napiVersion: v1 kind: Pod metadata: name: nginx # [5] The Pod name, CAN be changed spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP volumeMounts: - mountPath: /var/lib/www/html name: my-volume-name # MUST be the same as [6] volumes: - name: my-volume-name # [6] The volume name, CAN be changed persistentVolumeClaim: claimName: my-expansion-pvc # MUST be the same as [2] readOnly: false ```\\n\\nChạy câu lệnh sau đây để triển khai Ingress\\n\\nkubectl apply -f persistent-volume.yaml\\n\\nLúc này, hệ thống vServer sẽ tự động tạo một Volume tương ứng với file yaml bên trên, ví dụ:\\n\\nTạo Snapshot\\n\\nSnapshot là phương pháp sao lưu giữ liệu với chi phí thấp, thuận tiện và hiệu quả và có thể được sử dụng để tạo image, phục hồi dữ liệu và phân phối các bản sao dữ liệu. Nếu bạn là người dùng mới chưa từng sử dụng dịch vụ Snapshot, bạn cần thực hiện Activate Snapshot Service (Kích hoạt dịch vụ Snapshot) trước khi có thể tạo Snapshot cho Persistent Volume của bạn.\\n\\nActivate Snapshot Service\\n\\nĐể có thể tạo Snapshot, bạn cần thực hiện Activate Snapshot Service. Bạn sẽ không bị tính phí khi kích hoạt dịch vụ snapshot. Sau khi bạn tạo snapshot, chi phí sẽ được tính dựa trên dung lượng lưu trữ và thời gian lưu trữ của các bản snapshot này. Thực hiện theo các bước sau đây để kích hoạt dịch vụ Snapshot:\\n\\nBước 1: Truy cập vào https://hcm-3.console.vngcloud.vn/vserver/block-store/snapshot/overview\\n\\nBước 2: Chọn Activate Snapshot Service.\\n\\nVí dụ:\\n\\nCài đặt VNGCloud Snapshot Controller\\n\\nCài đặt Helm phiên bản từ 3.0 trở lên. Tham khảo tại https://helm.sh/docs/intro/install/ để biết cách cài đặt.\\n\\nThêm repo này vào cluster của bạn qua lệnh:\\n\\nhelm repo add vks-helm-charts https://vngcloud.github.io/vks-helm-charts helm repo update\\n\\nTiếp tục chạy:\\n\\nhelm install vngcloud-snapshot-controller vks-helm-charts/vngcloud-snapshot-controller \\\\ --replace --namespace kube-system\\n\\nSau khi việc cài đặt hoàn tất, thực hiện kiểm tra trạng thái của vngcloud-blockstorage-csi-driver pods:\\n\\nkubectl get pods -n kube-system\\n\\nVí dụ như ảnh bên dưới là bạn đã cài đặt thành công vngcloud-controller-manager:\\n\\n``` NAME READY STATUS RESTARTS AGE\\n\\nsnapshot-controller-7fdd984f89-745tg 0/1 ContainerCreating 0 3s snapshot-controller-7fdd984f89-k94wq 0/1 ContainerCreating 0 3s ```\\n\\nTạo file snapshot.yaml với nội dung sau:\\n\\n``` apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: my-snapshot-storage-class # [2] The name of the volume snapshot class, CAN be changed driver: bs.csi.vngcloud.vn deletionPolicy: Delete parameters: force-create: \"false\"\\n\\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: my-snapshot-pvc # [4] The name of the snapshot, CAN be changed spec: volumeSnapshotClassName: my-snapshot-storage-class # MUST match with [2] source: persistentVolumeClaimName: my-expansion-pvc # MUST match with [3] ```\\n\\nChạy câu lệnh sau đây để triển khai Ingress\\n\\nkubectl apply -f snapshot.yaml\\n\\nKiểm tra PVC và Snapshot vừa tạo\\n\\nSau khi apply tập tin thành công, bạn có thể kiểm tra danh sách service, pvc thông qua:\\n\\nkubectl get sc,pvc,pod -owide\\n\\n``` NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE storageclass.storage.k8s.io/my-expansion-storage-class bs.csi.vngcloud.vn Delete Immediate true 10m storageclass.storage.k8s.io/sc-iops-200-retain (default) bs.csi.vngcloud.vn Retain Immediate false 2d4h\\n\\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE persistentvolumeclaim/my-expansion-pvc Bound pvc-14456f4a-ee9e-435d-a94f-5a2e820954e9 20Gi RWO my-expansion-storage-class 10m Filesystem\\n\\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx 1/1 Running 0 10m 172.16.24.203 ng-3f06013a-f6a5-47ba-a51f-bc5e9c2b10a7-ecea1\\n\\nThay đổi thông số IOPS của Persistent Volume vừa tạo\\n\\n\\\\ Để thay đổi thông số IOPS của Persistent Volume vừa tạo, hãy thực hiện theo các bước sau đây:\\n\\nBước 1: Chạy lệnh bên dưới để liệt kê các PVC trong Cluster của bạn\\n\\nkubectl get persistentvolumes\\n\\nBước 2: Chỉnh sửa tệp tin YAML của PVC theo lệnh\\n\\nkubectl edit pvc my-expansion-pvc\\n\\nNếu bạn chưa chỉnh sửa IOPS của Persistent Volume lần nào trước đó, khi bạn chạy lệnh trên, bạn hãy thêm 1 annotation bs.csi.vngcloud.vn/volume-type: \"volume-type-id\" . Ví dụ: bên dưới tôi đang thay đổi IOPS của Persistent Volume từ 200 (Volume type id = vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018) lên 1000 (Volume type id = vtype-85b39362-a360-4bbb-9afa-a36a40cea748)\\n\\napiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: bs.csi.vngcloud.vn/volume-type: \"vtype-85b39362-a360-4bbb-9afa-a36a40cea748\" kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"annotations\":{},\"name\":\"my-expansion-pvc\",\"namespace\":\"default\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"20Gi\"}},\"storageClassName\":\"my-expansion-storage-class\"}} pv.kubernetes.io/bind-completed: \"yes\" pv.kubernetes.io/bound-by-controller: \"yes\" volume.beta.kubernetes.io/storage-provisioner: bs.csi.vngcloud.vn volume.kubernetes.io/storage-provisioner: bs.csi.vngcloud.vn creationTimestamp: \"2024-04-21T14:16:53Z\" finalizers: - kubernetes.io/pvc-protection name: my-expansion-pvc namespace: default resourceVersion: \"11041591\" uid: 14456f4a-ee9e-435d-a94f-5a2e820954e9 spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi storageClassName: my-expansion-storage-class volumeMode: Filesystem volumeName: pvc-14456f4a-ee9e-435d-a94f-5a2e820954e9 status: accessModes: - ReadWriteOnce capacity: storage: 20Gi phase: Bound\\n\\nNếu bạn đã chỉnh sửa IOPS của Persistent Volume lần nào trước đó, khi bạn chạy lệnh trên, tệp tin yaml của bạn đã có sẵn annotation bs.csi.vngcloud.vn/volume-type: \"volume-type-id\" . Lúc này, hãy chỉnh sửa annotation này về Volume type id có IOPS mà bạn mong muốn.\\n\\nThay đổi Disk Volume của Persistent Volume vừa tạo\\n\\n\\\\ Để thay đổi Disk Volume của Persistent Volume vừa tạo, hãy thực hiện chạy lệnh sau:\\n\\nVí dụ: ban đầu PVC được tạo có kích cỡ 20 Gi, hiện tại tôi sẽ tăng nó lên 30Gi\\n\\nkubectl patch pvc my-expansion-pvc -p \\'{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"30Gi\"}}}}\\'\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nBạn chỉ có thể thực hiện tăng Disk Volume mà không thể thực hiện giảm kích thước Disk Volume này. {% endhint %}\\n\\nRestore Persistent Volume từ Snapshot\\n\\nĐể khôi phục Persistent Volume từ Snapshot, bạn hãy thực hiện theo các bước sau:\\n\\nTạo file restore-volume.yaml với nội dung sau:\\n\\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-restore-pvc # The name of the PVC, CAN be changed spec: storageClassName: my-expansion-storage-class dataSource: name: my-snapshot-pvc # MUST match with [4] from the section 5.2 kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 20Gi'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-cluster-thong-qua-vi-poc.md'}, page_content='Khởi tạo một Cluster thông qua ví POC\\n\\nTài nguyên POC sinh ra nhằm mục đích hỗ trợ người dùng có thể trải nghiệm dịch vụ tại VNG Cloud một cách tốt nhất.\\n\\nĐiều kiện sử dụng tài nguyên POC:\\n\\nĐối tượng: Người dùng trả trước được cấp ví POC\\n\\nNguồn tiền: Ví POC\\n\\nTài nguyên: Tất cả các tài nguyên được áp dụng POC\\n\\nThời gian sử dụng: Tùy thuộc vào thời hạn ví POC được cấp.\\n\\nĐiều kiện cần\\n\\nĐể có thể khởi tạo một Cluster và Deploy một Workload, bạn cần:\\n\\nCó ít nhất 1 VPC và 1 Subnet đang ở trạng thái ACTIVE. Nếu bạn chưa có VPC, Subnet nào, vui lòng khởi tạo VPC, Subnet theo hướng dẫn tại đây.\\n\\nCó ít nhất 1 SSH key đang ở trạng thái ACTIVE. Nếu bạn chưa có SSH key nào, vui lòng khởi tạo SSH key theo hướng dẫn tại đây.\\n\\nĐã cài đặt và cấu hình kubectl trên thiết bị của bạn. vui lòng tham khảo tại đây nếu bạn chưa rõ cách cài đặt và sử dụng kuberctl. Ngoài ra, bạn không nên sử dụng phiên bản kubectl quá cũ, chúng tôi khuyến cáo bạn nên sử dụng phiên bản kubectl sai lệch không quá một phiên bản với version của cluster.\\n\\nKhởi tạo Cluster\\n\\nCluster trong Kubernetes là một tập hợp gồm một hoặc nhiều máy ảo (VM) được kết nối lại với nhau để chạy các ứng dụng được đóng gói dạng container. Cluster cung cấp một môi trường thống nhất để triển khai, quản lý và vận hành các container trên quy mô lớn.\\n\\nĐể khởi tạo một Cluster, hãy làm theo các bước bên dưới:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn Activate.\\n\\nBước 3: Chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn. Sau khi Activate thành công, bạn hãy chọn Create a Cluster\\n\\nBước 4: Tại màn hình khởi tạo Cluster, chúng tôi đã thiết lập thông tin cho Cluster và một Default Node Group cho bạn, bạn có thể giữ các giá trị mặc định này hoặc điều chỉnh các thông số mong muốn cho Cluster và Node Group của bạn tại Cluster Configuration, Default Node Group Configuration, Plugin. Mặc định chúng tôi sẽ khởi tạo cho bạn một Public Cluster với Public Node Group.\\n\\nBước 5: Chọn POC và chọn tiếp Create Kubernetes cluster. Hãy chờ vài phút để chúng tôi khởi tạo Cluster của bạn, trạng thái của Cluster lúc này là Creating.\\n\\nBước 6: Khi trạng thái Cluster là Active, bạn có thể xem thông tin Cluster, thông tin Node Group bằng cách chọn vào Cluster Name tại cột Name.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi bạn khởi tạo Cluster và chọn sử dụng ví POC, chúng tôi đã tự động tạo Control Plane, Node, Volume và Private Service Endpoint (nếu bạn chọn sử dụng) thông qua ví POC. Đối với các tài nguyên khác như\\n\\nPVC: khi thực hiện khởi tạo qua yaml, bạn vui lòng thêm tham số isPOC: \"true\" vào file yaml này. Tham khảo ví dụ bên dưới.\\n\\nLoadBalancer: khi thực hiện khởi tạo qua yaml, bạn vui lòng thêm annotation vks.vngcloud.vn/is-poc: \"true\" vào file yaml này. Tham khảo ví dụ bên dưới.\\n\\nDo các resource Load Balancer và PVC được quản lý thông qua YAML, sau khi Stop POC, nếu trong file YAML của bạn vẫn có tham số isPOC : true hoặc is-poc : true, trong trường hợp bạn xóa Load Balancer từ Portal vLB và xóa tham sốload-balancer-id trong yaml, lúc này hệ thống sẽ tự động tạo lại các resource này thông qua ví POC. Để tạo Load Balancer và PVC khác bằng tiền thật, vui lòng thay đổi tham số isPOC thành false. (isPOC : false hoặc is-poc : false). Chúng tôi khuyến cáo bạn nên thực hiện điều chỉnh tham số này trước khi thực hiện Stop POC cho Cluster của bạn. {% endhint %}\\n\\nKết nối và kiểm tra thông tin Cluster vừa tạo\\n\\nSau khi Cluster được khởi tạo thành công, bạn có thể thực hiện kết nối và kiểm tra thông tin Cluster vừa tạo theo các bước:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/k8s-cluster\\n\\nBước 2: Danh sách Cluster được hiển thị, chọn biểu tượng Download và chọn Download Config File để thực hiện tải xuống file kubeconfig. File này sẽ giúp bạn có toàn quyền truy cập vào Cluster của bạn.\\n\\nBước 3: Đổi tên file này thành config và lưu nó vào thư mục \\\\~/.kube/config\\n\\nBước 4: Thực hiện kiểm tra Cluster thông qua lệnh:\\n\\nChạy câu lệnh sau đây để kiểm tra node\\n\\nbash kubectl get nodes\\n\\nNếu kết quả trả về như bên dưới tức là bạn Cluster của bạn được khởi tạo thành công với 3 node như bên dưới.\\n\\nbash NAME STATUS ROLES AGE VERSION ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-834b7 Ready <none> 50m v1.28.8 ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-cf652 Ready <none> 23m v1.28.8 ng-0f4ed631-1252-49f7-8dfc-386fa0b2d29b-a8ef0 Ready <none> 28m v1.28.8\\n\\nDeploy Workload và expose service thông qua vLB Layer 4 hoặc vLB Layer 7\\n\\nSau đây là hướng dẫn để bạn deploy 2 workload và expose chúng qua Load Balancer Layer 4 và Load Balancer Layer 7 trên Kubernetes.\\n\\nBước 1: Tạo Deployment, Service cho Nginx app.\\n\\nTạo file nginx-service.yaml với nội dung sau:\\n\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: nginx-deployment\\n  namespace: default\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      unique-label: app1\\n  template:\\n    metadata:\\n      labels:\\n        unique-label: app1\\n        same-label: vngcloud\\n    spec:\\n      containers:\\n        - name: nginx-deployment\\n          image: nginx\\n          ports:\\n            - containerPort: 80\\n              name: http\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: python-http-server\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: python-http-server\\n  template:\\n    metadata:\\n      labels:\\n        app: python-http-server\\n        same-label: vngcloud\\n    spec:\\n      containers:\\n      - name: python-http-server\\n        image: python:3.9-slim\\n        command: [\"python\", \"-m\", \"http.server\", \"8080\"]\\n        ports:\\n        - containerPort: 8080\\n          name: http\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: nginx-service\\n  namespace: default\\n  annotations:\\n    vks.vngcloud.vn/is-poc: \"true\"      # Tham số chỉ định Load Balancer được tạo sẽ bằng ví POC\\nspec:\\n  ports:\\n    - port: 80\\n      targetPort: http\\n      protocol: TCP\\n      name: http-server\\n  selector:\\n    same-label: vngcloud\\n  type: LoadBalancer\\n\\n---\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: webapp-02\\n  namespace: default\\n  annotations:\\n    vks.vngcloud.vn/is-poc: \"true\"      # Tham số chỉ định Load Balancer được tạo sẽ bằng ví POC\\nspec:\\n  ingressClassName: vngcloud\\n  defaultBackend:\\n    service:\\n      name: nginx-service\\n      port:\\n        name: http-server\\n\\nDeploy Service này bằng lệch:\\n\\nbash kubectl apply -f nginx-service.yaml\\n\\nTiếp theo, bạn có thể thực hiện kiểm tra Deployment qua lệnh:\\n\\nbash kubectl get svc,deploy,pod -owide\\n\\nTạo Persistent Volume\\n\\nTạo file persistent-volume.yaml với nội dung sau:\\n\\n```bash apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: my-expansion-storage-class # [1] The StorageClass name, CAN be changed provisioner: bs.csi.vngcloud.vn # The VNG-CLOUD CSI driver name parameters: type: vtype-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx # The volume type UUID isPOC: \"true\" # Tham số chỉ định Volume được tạo sẽ bằng ví POC allowVolumeExpansion: true # MUST set this value to turn on volume expansion feature\\n\\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-expansion-pvc # [2] The PVC name, CAN be changed spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi # [3] The PVC size, CAN be changed, this value MUST be in the valid range of the proper volume type storageClassName: my-expansion-storage-class # [4] The StorageClass name, MUST be the same as [1]\\n\\napiVersion: v1 kind: Pod metadata: name: nginx # [5] The Pod name, CAN be changed spec: containers: - image: nginx imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP volumeMounts: - mountPath: /var/lib/www/html name: my-volume-name # MUST be the same as [6] volumes: - name: my-volume-name # [6] The volume name, CAN be changed persistentVolumeClaim: claimName: my-expansion-pvc # MUST be the same as [2] readOnly: false ```\\n\\nChạy câu lệnh sau đây để triển khai Ingress\\n\\nbash kubectl apply -f persistent-volume.yaml\\n\\nTạo Snapshot\\n\\nĐối với loại resource Snapshot, bạn không thể chỉ định snapshot sử dụng ví POC từ VKS. Để thực hiện tạo Snapshot qua ví POC, tại vServer Portal, vui lòng chọn Activate Snapshot, sau đó tại màn hình Checkout, vui lòng chọn sử dụng ví POC. Lúc này tất cả các resource snapshot của bạn sẽ được tạo qua ví POC. Do đó, việc stop POC cần được bạn thực hiện thông qua vConsole hoặc vServer Portal. Tham khảo thêm hình bên dưới.\\n\\nCài đặt VNGCloud Snapshot Controller\\n\\nCài đặt Helm phiên bản từ 3.0 trở lên. Tham khảo tại https://helm.sh/docs/intro/install/ để biết cách cài đặt.\\n\\nThêm repo này vào cluster của bạn qua lệnh:\\n\\nbash helm repo add vks-helm-charts https://vngcloud.github.io/vks-helm-charts helm repo update\\n\\nTiếp tục chạy:\\n\\nbash helm install vngcloud-snapshot-controller vks-helm-charts/vngcloud-snapshot-controller \\\\ --replace --namespace kube-system\\n\\nSau khi việc cài đặt hoàn tất, thực hiện kiểm tra trạng thái của vngcloud-blockstorage-csi-driver pods:\\n\\nbash kubectl get pods -n kube-system\\n\\nVí dụ như ảnh bên dưới là bạn đã cài đặt thành công vngcloud-controller-manager:\\n\\n```bash NAME READY STATUS RESTARTS AGE\\n\\nsnapshot-controller-7fdd984f89-745tg 0/1 ContainerCreating 0 3s snapshot-controller-7fdd984f89-k94wq 0/1 ContainerCreating 0 3s ```\\n\\nTạo file snapshot.yaml với nội dung sau:\\n\\n```bash apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: my-snapshot-storage-class # [2] The name of the volume snapshot class, CAN be changed driver: bs.csi.vngcloud.vn deletionPolicy: Delete parameters: force-create: \"false\"\\n\\napiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: my-snapshot-pvc # [4] The name of the snapshot, CAN be changed spec: volumeSnapshotClassName: my-snapshot-storage-class # MUST match with [2] source: persistentVolumeClaimName: my-expansion-pvc # MUST match with [3] ```\\n\\nChạy câu lệnh sau đây để triển khai Ingress\\n\\nbash kubectl apply -f snapshot.yaml\\n\\nKiểm tra PVC và Snapshot vừa tạo\\n\\nSau khi apply tập tin thành công, bạn có thể kiểm tra danh sách service, pvc thông qua:\\n\\nbash kubectl get sc,pvc,pod -owide\\n\\n```bash NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE storageclass.storage.k8s.io/my-expansion-storage-class bs.csi.vngcloud.vn Delete Immediate true 10m storageclass.storage.k8s.io/sc-iops-200-retain (default) bs.csi.vngcloud.vn Retain Immediate false 2d4h\\n\\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE persistentvolumeclaim/my-expansion-pvc Bound pvc-14456f4a-ee9e-435d-a94f-5a2e820954e9 20Gi RWO my-expansion-storage-class 10m Filesystem\\n\\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx 1/1 Running 0 10m 172.16.24.203 ng-3f06013a-f6a5-47ba-a51f-bc5e9c2b10a7-ecea1\\n\\nThay đổi thông số IOPS của Persistent Volume vừa tạo\\n\\n\\\\ Để thay đổi thông số IOPS của Persistent Volume vừa tạo, hãy thực hiện theo các bước sau đây:\\n\\nBước 1: Chạy lệnh bên dưới để liệt kê các PVC trong Cluster của bạn\\n\\nbash kubectl get persistentvolumes\\n\\nBước 2: Chỉnh sửa tệp tin YAML của PVC theo lệnh\\n\\nbash kubectl edit pvc my-expansion-pvc\\n\\nNếu bạn chưa chỉnh sửa IOPS của Persistent Volume lần nào trước đó, khi bạn chạy lệnh trên, bạn hãy thêm 1 annotation bs.csi.vngcloud.vn/volume-type: \"volume-type-id\" . Ví dụ: bên dưới tôi đang thay đổi IOPS của Persistent Volume từ 200 (Volume type id = vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018) lên 1000 (Volume type id = vtype-85b39362-a360-4bbb-9afa-a36a40cea748)\\n\\nbash apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: bs.csi.vngcloud.vn/volume-type: \"vtype-85b39362-a360-4bbb-9afa-a36a40cea748\" kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"annotations\":{},\"name\":\"my-expansion-pvc\",\"namespace\":\"default\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"20Gi\"}},\"storageClassName\":\"my-expansion-storage-class\"}} pv.kubernetes.io/bind-completed: \"yes\" pv.kubernetes.io/bound-by-controller: \"yes\" volume.beta.kubernetes.io/storage-provisioner: bs.csi.vngcloud.vn volume.kubernetes.io/storage-provisioner: bs.csi.vngcloud.vn creationTimestamp: \"2024-04-21T14:16:53Z\" finalizers: - kubernetes.io/pvc-protection name: my-expansion-pvc namespace: default resourceVersion: \"11041591\" uid: 14456f4a-ee9e-435d-a94f-5a2e820954e9 spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi storageClassName: my-expansion-storage-class volumeMode: Filesystem volumeName: pvc-14456f4a-ee9e-435d-a94f-5a2e820954e9 status: accessModes: - ReadWriteOnce capacity: storage: 20Gi phase: Bound\\n\\nNếu bạn đã chỉnh sửa IOPS của Persistent Volume lần nào trước đó, khi bạn chạy lệnh trên, tệp tin yaml của bạn đã có sẵn annotation bs.csi.vngcloud.vn/volume-type: \"volume-type-id\" . Lúc này, hãy chỉnh sửa annotation này về Volume type id có IOPS mà bạn mong muốn.\\n\\nThay đổi Disk Volume của Persistent Volume vừa tạo\\n\\n\\\\ Để thay đổi Disk Volume của Persistent Volume vừa tạo, hãy thực hiện chạy lệnh sau:\\n\\nVí dụ: ban đầu PVC được tạo có kích cỡ 20 Gi, hiện tại tôi sẽ tăng nó lên 30Gi\\n\\nbash kubectl patch pvc my-expansion-pvc -p \\'{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"30Gi\"}}}}\\'\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nBạn chỉ có thể thực hiện tăng Disk Volume mà không thể thực hiện giảm kích thước Disk Volume này. {% endhint %}\\n\\nRestore Persistent Volume từ Snapshot\\n\\nĐể khôi phục Persistent Volume từ Snapshot, bạn hãy thực hiện theo các bước sau:\\n\\nTạo file restore-volume.yaml với nội dung sau:\\n\\nbash apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-restore-pvc # The name of the PVC, CAN be changed spec: storageClassName: my-expansion-storage-class dataSource: name: my-snapshot-pvc # MUST match with [4] from the section 5.2 kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 20Gi\\n\\nSau một thời gian trải nghiệm VKS, nếu bạn muốn chuyển các tài nguyên POC này thành tài nguyên thật, vui lòng thực hiện theo các bước theo hướng dẫn tại đây.'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/expose-mot-service-thong-qua-vlb-layer7.md'}, page_content='Expose một service thông qua vLB Layer7\\n\\nĐiều kiện cần\\n\\nĐể có thể khởi tạo một Cluster và Deploy một Workload, bạn cần:\\n\\nCó ít nhất 1 VPC và 1 Subnet đang ở trạng thái ACTIVE. Nếu bạn chưa có VPC, Subnet nào, vui lòng khởi tạo VPC, Subnet theo hướng dẫn tại đây.\\n\\nCó ít nhất 1 SSH key đang ở trạng thái ACTIVE. Nếu bạn chưa có SSH key nào, vui lòng khởi tạo SSH key theo hướng dẫn tại đây.\\n\\nĐã cài đặt và cấu hình kubectl trên thiết bị của bạn. vui lòng tham khảo tại đây nếu bạn chưa rõ cách cài đặt và sử dụng kuberctl. Ngoài ra, bạn không nên sử dụng phiên bản kubectl quá cũ, chúng tôi khuyến cáo bạn nên sử dụng phiên bản kubectl sai lệch không quá một phiên bản với version của cluster.\\n\\nKhởi tạo Cluster\\n\\nCluster trong Kubernetes là một tập hợp gồm một hoặc nhiều máy ảo (VM) được kết nối lại với nhau để chạy các ứng dụng được đóng gói dạng container. Cluster cung cấp một môi trường thống nhất để triển khai, quản lý và vận hành các container trên quy mô lớn.\\n\\nĐể khởi tạo một Cluster, hãy làm theo các bước bên dưới:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn Activate.\\n\\nBước 3: Chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn. Sau khi Activate thành công, bạn hãy chọn Create a Cluster\\n\\nBước 4: Tại màn hình khởi tạo Cluster, chúng tôi đã thiết lập thông tin cho Cluster và một Default Node Group cho bạn, bạn có thể giữ các giá trị mặc định này hoặc điều chỉnh các thông số mong muốn cho Cluster và Node Group của bạn tại Cluster Configuration, Default Node Group Configuration, Plugin. Khi bạn chọn bật option , mặc định chúng tôi sẽ cài sẵn plugin này vào Cluster của bạn.\\n\\nBước 5: Chọn Create Kubernetes cluster. Hãy chờ vài phút để chúng tôi khởi tạo Cluster của bạn, trạng thái của Cluster lúc này là Creating.\\n\\nBước 6: Khi trạng thái Cluster là Active, bạn có thể xem thông tin Cluster, thông tin Node Group bằng cách chọn vào Cluster Name tại cột Name.\\n\\nKết nối và kiểm tra thông tin Cluster vừa tạo\\n\\nSau khi Cluster được khởi tạo thành công, bạn có thể thực hiện kết nối và kiểm tra thông tin Cluster vừa tạo theo các bước:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/k8s-cluster\\n\\nBước 2: Danh sách Cluster được hiển thị, chọn biểu tượng và chọn Download Config File để thực hiện tải xuống file kubeconfig. File này sẽ giúp bạn có toàn quyền truy cập vào Cluster của bạn.\\n\\nBước 3: Đổi tên file này thành config và lưu nó vào thư mục \\\\~/.kube/config\\n\\nBước 4: Thực hiện kiểm tra Cluster thông qua lệnh:\\n\\nChạy câu lệnh sau đây để kiểm tra node\\n\\nkubectl get nodes\\n\\nNếu kết quả trả về như bên dưới tức là bạn Cluster của bạn được khởi tạo thành công với 3 node như bên dưới.\\n\\nNAME STATUS ROLES AGE VERSION ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-834b7 Ready <none> 50m v1.28.8 ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-cf652 Ready <none> 23m v1.28.8 ng-0f4ed631-1252-49f7-8dfc-386fa0b2d29b-a8ef0 Ready <none> 28m v1.28.8\\n\\nKhởi tạo Service Account và cài đặt VNGCloud LoadBalancer Controller\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi bạn thực hiện khởi tạo Cluster theo hướng dẫn bên trên, nếu bạn chưa bật option Enable vLB Native Integration Driver, mặc định chúng tôi sẽ không cài sẵn plugin này vào Cluster của bạn. Bạn cần tự thực hiện Khởi tạo Service Account và cài đặt VNGCloud LoadBalancer Controller theo hướng dẫn bên dưới. Nếu bạn đã bật option Enable vLB Native Integration Driver, thì chúng tôi đã cài sẵn plugin này vào Cluster của bạn, hãy bỏ qua bước Khởi tạo Service Account, cài đặt VNGCloud LoadBalancer Controller và tiếp tục thực hiện theo hướng dẫn kể từ Deploy một Workload. {% endhint %}\\n\\nDeploy một Workload\\n\\nSau đây là hướng dẫn để bạn deploy service nginx trên Kubernetes.\\n\\nBước 1: Tạo Deployment cho Nginx app.\\n\\nTạo file nginx-service-lb7.yaml với nội dung sau:\\n\\n``` apiVersion: apps/v1 kind: Deployment metadata: name: nginx-app spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.19.1 ports: - containerPort: 80\\n\\napiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app: nginx type: NodePort ports: - protocol: TCP port: 80 targetPort: 80 ```\\n\\nDeploy Deployment này bằng lệch:\\n\\nkubectl apply -f nginx-service-lb7.yaml\\n\\nBước 2: Kiểm tra thông tin Deployment, Service vừa deploy\\n\\nChạy câu lệnh sau đây để kiểm tra Deployment\\n\\nkubectl get svc,deploy,pod -owide\\n\\nNếu kết quả trả về như bên dưới tức là bạn đã deploy Deployment thành công.\\n\\n``` NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1\\n\\nNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-app 1/1 1 1 2m50s nginx nginx:1.19.1 app=nginx\\n\\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-app-7f45b65946-6wlgw 1/1 Running 0 2m49s 172.16.54.3 ng-e0fc7245-0c6e-4336-abcc-31a70eeed71d-972a9\\n\\nTạo Ingress Resource\\n\\nTạo file nginx-ingress.yaml với nội dung sau:\\n\\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-ingress spec: ingressClassName: \"vngcloud\" defaultBackend: service: name: nginx-service port: number: 80 rules: - http: paths: - path: /path1 pathType: Exact backend: service: name: nginx-service port: number: 80\\n\\nChạy câu lệnh sau đây để triển khai Ingress\\n\\nkubectl apply -f nginx-ingress.yaml\\n\\nLúc này, hệ thống vLB sẽ tự động tạo một LB tương ứng với Ingress resource bên trên, ví dụ:\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nHiện tại Ingress chỉ hỗ trợ duy nhất TLS port 443 và là điểm kết thúc cho TLS (TLS termination). TLS Secret phải chứa các trường với tên key là tls.crt và tls.key, đây chính là certificate và private key để sử dụng cho TLS. Nếu bạn muốn sử dụng Certificate cho một host, hãy thực hiện tải lên Certificate theo hướng dẫn tại [Upload a certificate] và sử dụng chúng như một annotation. Ví dụ:\\n\\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress annotations: # kubernetes.io/ingress.class: \"vngcloud\" # this annotation is deprecated will cause warning, can use option `ingressClassName` below instead. vks.vngcloud.vn/load-balancer-id: \"lb-6cdea8fd-4589-410e-933f-c3bc46fa9d25\" vks.vngcloud.vn/certificate-ids: \"secret-a6d20ec6-f3e5-499a-981b-b1484e340cec\" spec: ingressClassName: \"vngcloud\" defaultBackend: service: name: apache-service port: number: 80 tls: - hosts: - host.example.com rules: - host: host.example.com http: paths: - path: /path1 pathType: Exact backend: service: name: nginx-service port: number: 80 {% endhint %}\\n\\nĐể truy cập vào app nginx, bạn có thể sử dụng Endpoint của Load Balancer mà hệ thống đã tạo.\\n\\nhttp://Endpoint/\\n\\nBạn có thể lấy thông tin Public Endpoint của Load Balancer tại giao diện vLB. Cụ thể truy cập tại\\n\\nVí dụ, bên dưới tôi đã truy cập thành công vào app nginx với địa chỉ : http://180.93.181.129/\\n\\nBạn có thể xem thêm về ALB tại Working with Application Load Balancer (ALB).\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nViệc thay đổi tên hoặc kích thước (Rename, Resize) tài nguyên Load Balancer trên vServer Portal có thể gây ra sự không tương thích với tài nguyên trên Kubernetes Cluster. Điều này có thể dẫn đến việc các tài nguyên không hoạt động trên Cluster, hoặc tài nguyên bị đồng bộ lại hoặc thông tin tài nguyên giữa vServer Portal và Cluster không khớp. Để ngăn chặn vấn đề này, hãy sử dụng kubectlđể quản lý tài nguyên của Cluster. {% endhint %}'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/README.md'}, page_content='Bắt đầu với VKS\\n\\nBạn có thể sử dụng các hướng dẫn sau đây để bắt đầu làm việc với VKS. Trong quá trình sử dụng, nếu gặp vấn đề gì với dịch vụ, bạn hãy vui lòng liên hệ với VNG Cloud qua email support@vngcloud.vn hoặc hotline 19001549.'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-private-cluster.md'}, page_content='Khởi tạo một Private Cluster\\n\\nTrước đây, các public cluster trên VKS đang sử dụng địa chỉ Public IP để giao tiếp giữa nodes và control plane. Để nâng cao bảo mật cho cluster của bạn, chúng tôi đã cho ra mắt mô hình private cluster. Tính năng Private Cluster giúp cho cụm K8S của bạn được bảo mật nhất có thể, mọi kết nối hoàn toàn là private từ kết nối giữa nodes tới control plane, kết nối từ client tới control plane, hay kết nối từ nodes tới các sản phẩm dịch vụ khác trong VNG Cloud như: vStorage, vCR, vMonitor, VNGCloud APIs,...Private Cluster là lựa chọn lý tưởng cho các dịch vụ yêu cầu kiểm soát truy cập chặt chẽ, đảm bảo tuân thủ các quy định về bảo mật và quyền riêng tư dữ liệu.\\n\\nModel\\n\\nThành phần\\n\\nControl plane: Được quản lý bởi VNG Cloud, chịu trách nhiệm điều phối và quản lý toàn bộ cluster.\\n\\nNodes: Các Node trong Cluster khi được tạo ra sẽ chỉ có internal IP và không thể đi ra public internet. Nếu muốn node truy cập được internet, bạn cần sử dụng NAT Gateway. Chi tiết tham khảo thêm tại đây.\\n\\nPrivate Load Balancer: Được quản lý bởi VNG Cloud, chịu trách nhiệm giúp các Private Node giao tiếp với Control Plane.\\n\\nPrivate Service Endpoint: Khi bạn tạo một private cluster, hệ thống tự động tạo 4 endpoints giúp kết nối với các dịch vụ khác trên VNG Cloud bao gồm:\\n\\nEndpoint để kết nối tới dịch vụ IAM (Endpoint Name: vks-iam-endpoint-...)\\n\\nEndpoint để kết nối tới dịch vụ vCR (Endpoint Name: vks-vcr-endpoint-...)\\n\\nEndpoint để kết nối tới dịch vụ vServer (Endpoint Name: vks-vserver-endpoint-...)\\n\\nEndpoint để kết nối tới dịch vụ vStorage (Endpoint Name: vks-vstorage-endpoint-...)\\n\\nBạn có thể xem thông tin 4 private service endpoint thông qua portal vServer theo đường dẫn tại đây.\\n\\n{% hint style=\"info\" %} Warning:\\n\\nKhông xóa Private Service Endpoint: Để đảm bảo hoạt động ổn định của cluster, bạn không nên xóa 4 service endpoint đã được tạo sẵn. Nếu vô tình xóa hoặc chỉnh sửa 4 endpoint này, trong vòng tối đa 5 phút, hệ thống sẽ tự động tạo lại nhưng có thể gây gián đoạn đến các dịch vụ đang chạy. Lúc này, do service endpoint tạo lại có thể đã thay đổi Endpoint IP so với ban đầu nên để cluster hoạt động được, bạn cần thực hiện thêm Endpoint IP một cách thủ công cho những server đang chạy trước đó thông qua câu lệnh:\\n\\nvks-bootstraper add-host -i <IP> -d <DOMAIN>\\n\\nVí dụ, nếu bạn xóa private service endpoint của vCR thì bạn cần add host qua lệnh:\\n\\nvks-boostraper add-host -i 10.10.10.10 -d vcr.vngcloud.vn * Tái sử dụng Private Service Endpoint: Các service endpoint có thể được nhiều private cluster cùng sử dụng. Khi các private cluster chung VPC thì chúng tôi sẽ tái sử dụng chúng cho các cluster này. * Xóa Private Service Endpoint tự động: Khi bạn xóa cluster, nếu không còn cluster nào tái sử dụng các service endpoint này, hệ thống sẽ tự động xóa chúng. * Chi phí sử dụng Private Service Endpoint: Việc sử dụng private cluster sẽ phát sinh thêm chi phí cho 4 private service endpoint, nhưng nó mang lại nhiều lợi ích về bảo mật cho dự án của bạn. Bạn hãy cân nhắc kỹ lưỡng các yếu tố để đưa ra quyết định sử dụng public hay private cho cluster của mình. {% endhint %}\\n\\nĐiều kiện cần\\n\\nĐể có thể khởi tạo một Cluster và Deploy một Workload, bạn cần:\\n\\nCó ít nhất 1 VPC và 1 Subnet đang ở trạng thái ACTIVE. Nếu bạn chưa có VPC, Subnet nào, vui lòng khởi tạo VPC, Subnet theo hướng dẫn tại đây.\\n\\nCó ít nhất 1 SSH key đang ở trạng thái ACTIVE. Nếu bạn chưa có SSH key nào, vui lòng khởi tạo SSH key theo hướng dẫn tại đây.\\n\\nĐã cài đặt và cấu hình kubectl trên thiết bị của bạn. vui lòng tham khảo tại đây nếu bạn chưa rõ cách cài đặt và sử dụng kuberctl. Ngoài ra, bạn không nên sử dụng phiên bản kubectl quá cũ, chúng tôi khuyến cáo bạn nên sử dụng phiên bản kubectl sai lệch không quá một phiên bản với version của cluster.\\n\\nKhởi tạo Cluster\\n\\nCluster trong Kubernetes là một tập hợp gồm một hoặc nhiều máy ảo (VM) được kết nối lại với nhau để chạy các ứng dụng được đóng gói dạng container. Cluster cung cấp một môi trường thống nhất để triển khai, quản lý và vận hành các container trên quy mô lớn.\\n\\nĐể khởi tạo một Cluster, hãy làm theo các bước bên dưới:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn Activate.\\n\\nBước 3: Chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn. Sau khi Activate thành công, bạn hãy chọn Create a Cluster\\n\\nBước 4: Tại màn hình khởi tạo Cluster, chúng tôi đã thiết lập thông tin cho Cluster và một Default Node Group cho bạn, bạn có thể giữ các giá trị mặc định này hoặc điều chỉnh các thông số mong muốn cho Cluster và Node Group của bạn tại Cluster Configuration, Default Node Group Configuration, Plugin.\\n\\nBước 5: Chọn Create Kubernetes cluster. Hãy chờ vài phút để chúng tôi khởi tạo Cluster của bạn, trạng thái của Cluster lúc này là Creating.\\n\\nBước 6: Khi trạng thái Cluster là Active, bạn có thể xem thông tin Cluster, thông tin Node Group bằng cách chọn vào Cluster Name tại cột Name.\\n\\n{% hint style=\"info\" %} Warning:\\n\\nMột Cluster có thể có nhiều Node Group, mỗi Node Group có thể hoạt động ở mode Public/ Private tùy theo nhu cầu của bạn.\\n\\nDo Cluster của bạn được khởi tạo ở chế độ Private, để có thể truy cập vào kube-api của Control Plane thì bạn cần đứng trong VPC mà bạn chọn sử dụng cho Cluster của bạn. {% endhint %}\\n\\nKết nối và kiểm tra thông tin Cluster vừa tạo\\n\\nSau khi Cluster được khởi tạo thành công, bạn có thể thực hiện kết nối và kiểm tra thông tin Cluster vừa tạo theo các bước:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/k8s-cluster\\n\\nBước 2: Danh sách Cluster được hiển thị, chọn biểu tượng Download và chọn Download config file để thực hiện tải xuống file kubeconfig. File này sẽ giúp bạn có toàn quyền truy cập vào Cluster của bạn.\\n\\nBước 3: Đổi tên file này thành config và lưu nó vào thư mục \\\\~/.kube/config\\n\\nBước 4: Do Cluster của bạn được khởi tạo ở chế độ Private, để có thể truy cập vào kube-api, bạn cần đứng trong VPC mà bạn đã chọn sử dụng cho Cluster của bạn. Ví dụ, khi bạn không đứng ở VPC và thực hiện get nodes, kết quả sẽ hiển thị như sau:\\n\\nkubectl get nodes E0821 14:27:03.793829 23348 memcache.go:265] couldn\\'t get current server API group list: Get \"https://10.7.8.9:6443/api?timeout=32s\": dial tcp 10.7.8.9:6443: connectex: No connection could be made because the target machine actively refused it. E0821 14:27:05.866230 23348 memcache.go:265] couldn\\'t get current server API group list: Get \"https://10.7.8.9:6443/api?timeout=32s\": dial tcp 10.7.8.9:6443: connectex: No connection could be made because the target machine actively refused it. E0821 14:27:07.922272 23348 memcache.go:265] couldn\\'t get current server API group list: Get \"https://10.7.8.9:6443/api?timeout=32s\": dial tcp 10.7.8.9:6443: connectex: No connection could be made because the target machine actively refused it. E0821 14:27:09.989832 23348 memcache.go:265] couldn\\'t get current server API group list: Get \"https://10.7.8.9:6443/api?timeout=32s\": dial tcp 10.7.8.9:6443: connectex: No connection could be made because the target machine actively refused it. E0821 14:27:12.055864 23348 memcache.go:265] couldn\\'t get current server API group list: Get \"https://10.7.8.9:6443/api?timeout=32s\": dial tcp 10.7.8.9:6443: connectex: No connection could be made because the target machine actively refused it. Unable to connect to the server: dial tcp 10.7.8.9:6443: connectex: No connection could be made because the target machine actively refused it.\\n\\nTrong ví dụ bên dưới tôi sẽ đứng tại một server có VPC cùng với VPC được sử dụng cho Cluster. Bạn có thể thực hiện SSH vào server theo hướng dẫn tại đây. Sau khi SSH vào server, hãy cài đặt kubectl theo hướng dẫn tại đây.\\n\\nVí dụ, tôi đang sử dụng một server linux để thực hiện get nodes, tôi có thể cài đặt kubectl qua lệnh:\\n\\nsudo snap install kubectl --classic\\n\\nSau đó, tôi kiểm tra kubectl qua lệnh:\\n\\nkubectl version\\n\\nTạo thư mục .kube qua lệnh:\\n\\nmkdir -p .kube\\n\\nSau đó, bạn hãy nhập file kubeconfig qua lệnh:\\n\\nvim .kube/config\\n\\nSau đó, bạn nhập :wq để lưu file kubeconfig và thoát vim.\\n\\nChạy câu lệnh sau đây để kiểm tra cluster\\n\\nkubectl get svc\\n\\nBạn sẽ thấy kết quả trả về tương tự sau:\\n\\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 10m\\n\\nChạy câu lệnh sau đây để kiểm tra node\\n\\nkubectl get nodes\\n\\nNếu kết quả trả về như bên dưới tức là bạn Cluster của bạn được khởi tạo thành công với 1 node như bên dưới.\\n\\nNAME STATUS ROLES AGE VERSION vks-demo-cluster-nodegroup-demo-7c9aa Ready <none> 8m11s v1.28.8\\n\\nSử dụng Docker để Pull/Push image\\n\\nDo Private Cluster chỉ có thể kết nối private tới hệ thống vContainer Registry (vCR) và không thể kết nối ra các Container Registry khác ngoài internet, nên bạn cần pull/ push image về vCR để sử dụng theo hướng dẫn sau đây:\\n\\nBước 1: Cài đặt Docker\\n\\nThực hiện cài đặt docker theo hướng dẫn tại đây.\\n\\nBước 2: Khởi tạo Public Repository và Repository User trên vContainer Registry Portal:\\n\\nĐăng nhập portal vCR tại đường dẫn: https://vcr.console.vngcloud.vn/list\\n\\nThực hiện khởi tạo Repositoryvà Repository theo hướng dẫn tại đây. Ví dụ trong hình dưới, tôi đã khởi tạo demo_repo với demo_user có thể pull/ push image:\\n\\n{% hint style=\"info\" %} Warning:\\n\\nNếu bạn mong muốn khởi tạo Private Reposity, để pull image từ Private Reposity, bạn cần tạo secret key theo hướng dẫn tại đây. {% endhint %}\\n\\nBước 3: Thực hiện pull image nginx về theo lệnh:\\n\\ndocker pull nginx:latest\\n\\nBươc 4: Thực hiện login vào vCR qua lệnh:\\n\\ndocker login vcr.vngcloud.vn -u <repository_user>\\n\\nVí dụ, câu lệnh dưới tôi sử dụng để login vào repo demo:\\n\\ndocker login vcr.vngcloud.vn -u 53461-user_demo\\n\\nBước 5: Gán tag cho image nginx\\n\\ndocker tag SOURCE_IMAGE[:TAG] vcr.vngcloud.vn/REPO_NAME/IMAGE[:TAG]\\n\\nVí dụ, câu lệnh dưới tôi sử dụng để gán tag cho image nginx:\\n\\ndocker tag nginx:latest vcr.vngcloud.vn/53461-repo_demo/nginx-demo:latest\\n\\nBước 6: Thực hiện push image lên repo qua lệnh:\\n\\ndocker push vcr.vngcloud.vn/REPO_NAME/IMAGE[:TAG]\\n\\nVí dụ, câu lệnh dưới tôi sử dụng để push image lên demo_repo:\\n\\ndocker push vcr.vngcloud.vn/53461-repo_demo/nginx-demo:latest\\n\\nDeploy một Workload\\n\\nSau đây là hướng dẫn để bạn deploy service nginx và expose service này qua Network Load Balancer\\n\\nBước 1: Thực hiện tạo file nginx-service-lb4.yaml qua lệnh:\\n\\nvi nginx.yaml\\n\\nSau đó, bạn hãy nhập nội dung cho file này như sau: bạn cần thay image bằng đường dẫn image lưu trên vCR mà bạn đã push ở bước bên trên:\\n\\n``` apiVersion: apps/v1 kind: Deployment metadata: name: nginx-app spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: vcr.vngcloud.vn/53461-repo_demo/nginx-demo:latest ports: - containerPort: 80\\n\\napiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app: nginx type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 80 ```\\n\\nNhập :wq để lưu tệp tin này.\\n\\nDeploy Deployment này bằng lệch:\\n\\nkubectl apply -f nginx-service-lb4.yaml\\n\\nBước 2: Kiểm tra thông tin Deployment, Service trước khi expose ra Internet.\\n\\nChạy câu lệnh sau đây để kiểm tra Deployment\\n\\nkubectl get svc,deploy,pod -owide\\n\\nNếu kết quả trả về như bên dưới tức là bạn đã deploy service nginx thành công.\\n\\n``` NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1\\n\\nNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-app 1/1 1 1 3m32s nginx vcr.vngcloud.vn/53461-repo_demo/nginx-demo:latest app=nginx\\n\\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-app-56bbc8fdd8-4pz68 1/1 Running 0 3m32s 172.16.4.207 vks-demo-cluster-nodegroup-demo-7c9aa\\n\\nLúc này, hệ thống vLB sẽ khởi tạo môt Network Load Balancer, bạn có thể xem thông tin LB này thông qua portal vLB tại đây.\\n\\nBước 3: Để truy cập vào app nginx vừa export, bạn có thể sử dụng Endpoint của Load Balancer URL với định dạng:\\n\\nhttp://Endpoint/\\n\\nBạn có thể lấy thông tin Public Endpoint của Load Balancer tại giao diện vLB. Cụ thể truy cập tại https://hcm-3.console.vngcloud.vn/vserver/load-balancer/vlb/\\n\\nVí dụ, bên dưới tôi đã truy cập thành công vào app nginx với địa chỉ : http://116.118.88.236/\\n\\n{% hint style=\"info\" %} Một vài chú ý khác:\\n\\nBên trên là ví dụ hướng dẫn bạn thực hiện expose một service thông qua vLB Layer 4, bạn có thể thực hiện expose service thông qua vLB Layer 7 theo hướng dẫn tại đây.\\n\\nĐể đảm bảo private cluster hoạt động hiệu quả, chúng tôi đã tự động thêm Subnet mà bạn chọn sử dụng cho Cluster vào danh sách Whitelist của cụm. Bạn có thể sử dụng tính năng Whitelist để giới hạn các Subnet trong VPC có quyền truy cập vào kube-api. Chi tiết cách sử dụng tính năng Whitelist vui lòng tham khảo tại đây. {% endhint %}'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/lam-viec-voi-nvidia-gpu-nodegroups.md'}, page_content='Khởi tạo và làm việc với NVIDIA GPU Node Group\\n\\nTổng quan\\n\\nNVIDIA GPU Operator là một operator giúp đơn giản hóa việc triển khai và quản lý các node GPU trong các cụm Kubernetes. Nó cung cấp một tập hợp các tài nguyên tùy chỉnh và bộ điều khiển của Kubernetes cùng làm việc để tự động hóa quản lý tài nguyên GPU trong cụm Kubernetes.\\n\\nTrong hướng dẫn này, chúng tôi sẽ hướng dẫn bạn:\\n\\nKhởi tạo một node group với NVIDIA GPU\\n\\nCài đặt NVIDIA GPU Operator.\\n\\nTriển khai một GPU Workload.\\n\\nThiết lập GPU Sharing.\\n\\nGiám sát hoạt động GPU resources.\\n\\nAutoscale GPU resources.\\n\\nKhởi tạo một node group với NVIDIA GPU\\n\\nThực hiện khởi tạo một Cluster với ít nhất một node group sử dụng NVIDIA GPU theo hướng dẫn tại đây nếu bạn muốn khởi tạo public node group hoặc tại đây nếu bạn muốn khởi tạo private node group.\\n\\nNgoài ra, bạn cần đảm bảo đã cài đặt kubectl và helm tại thiết bị của bạn.\\n\\nBên cạnh đó, bạn cũng có thể cài đặt các công cụ và thư viện khác mà bạn có thể sử dụng để giám sát và quản lý tài nguyên Kubernetes của mình:\\n\\nkubectl-view-allocations plugin sử dụng để giám sát tài nguyên Kubernetes của bạn. Để biết thêm thông tin, vui lòng tham khảo thêm tại kubectl-view-allocations.\\n\\nBạn có thể thực hiện kiểm tra các cài đặt bên trên thông qua lệnh:\\n\\n```bash\\n\\nCheck kubectl CLI version\\n\\nkubectl version\\n\\nCheck Helm version\\n\\nhelm version\\n\\nCheck kubectl-view-allocations version\\n\\nkubectl-view-allocations --version ```\\n\\nTrên Cluster vừa tạo, thực hiện kiểm tra node trong node group của bạn qua lệnh:\\n\\nbash kubectl get nodes -owide\\n\\nCài đặt NVIDIA GPU Operator\\n\\nNội dung bên dưới chúng tôi sẽ trực tiếp hướng dẫn bạn cài đặt NVIDIA GPU Operator, để biết thêm thông tin về plugin này, vui lòng tham khảo thêm tại NVIDIA GPU Operator Documentation.\\n\\nTrên Cluster bạn đã tạo ở bước trên, thực hiện chạy lệnh:\\n\\nbash helm install nvidia-gpu-operator --wait --version v24.3.0 \\\\ -n gpu-operator --create-namespace \\\\ oci://vcr.vngcloud.vn/81-vks-public/vks-helm-charts/gpu-operator \\\\ --set dcgmExporter.serviceMonitor.enabled=true\\n\\nHệ thống mất khoảng 5 - 10 phút để thực hiện cài đặt operator này, bạn hãy đợi tới khi việc cài đặt hoàn thành. Trong thời gian này, bạn có thể kiểm tra tất cả các pods trong namespacegpu-operator đang chạy thông qua lệnh:\\n\\nbash kubectl -n gpu-operator get pods -owide\\n\\nOperator sẽ gán labelnvidia.com/gpucho node trong node group của bạn, lable này được NVIDIA GPU Operator sử dụng để identify nodes, bạn cũng có thể sử dụng label này để filter những node đang có NVIDIA GPU. Bạn có thể kiểm tra các node được gán nhãn này qua lệnh:\\n\\nbash kubectl get node -o json | jq \\'.items[].metadata.labels\\' | grep \"nvidia.com\"\\n\\nVí dụ, đối với kết quả bên dưới, node trong cụm có label nvidia.com/gpu, có nghĩa là node đó có GPU. Các label này cũng cho biết nút này đang sử dụng 1 card GPU RTX 2080Ti, số lượng GPU có sẵn, Bộ nhớ GPU và các thông tin khác.\\n\\nTrên pod nvidia-device-plugin-daemonsettrong namespacegpu-operator, bạn có thể chạy lệnh nvidia-smi để kiểm tra thông tin GPU trên node:\\n\\nbash POD_NAME=$(kubectl -n gpu-operator get pods -l app=nvidia-device-plugin-daemonset -o jsonpath=\\'{.items[0].metadata.name}\\') kubectl -n gpu-operator exec -it $POD_NAME -- nvidia-smi\\n\\nTriển khai một GPU workload\\n\\nDeploy Cuda VectorAdd Test\\n\\nBạn có thể thực hiện deploy cuda-vectoradd-test như một workload mẫu trên cluster của bạn. Cụ thể bạn có thể tham khảo thêm về workload mẫu này tại cuda-vectoradd-test.yaml.\\n\\nThực hiện chạy các lệnh bên dưới để deploy workload và kiểm tra các pods đã được khởi chạy:\\n\\n```bash\\n\\nApply the manifest\\n\\nkubectl apply -f \\\\ https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/main/nvidia-gpu/manifest/cuda-vectoradd-test.yaml\\n\\nCheck the pods\\n\\nkubectl get pods\\n\\nCheck the logs of the pod\\n\\nkubectl logs cuda-vectoradd\\n\\n[Optional] Clean the resources\\n\\nkubectl delete deploy cuda-vectoradd ```\\n\\nDeploy TensorFlow Test\\n\\nNgoài Cuda VectorAdd Test, bạn cũng có thể deploy TensorFlow như một workload trên Cluster của bạn. Cụ thể bạn có thể tham khảo về workload mẫu này tại tensorflow-gpu.yaml.\\n\\nThực hiện chạy các lệnh bên dưới để deploy workload và kiểm tra các pods đã được khởi chạy:\\n\\n```bash\\n\\nApply the manifest\\n\\nkubectl apply -f \\\\ https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/main/nvidia-gpu/manifest/tensorflow-gpu.yaml\\n\\nCheck the pods\\n\\nkubectl get pods\\n\\nCheck processes are running using nvidia-smi\\n\\nkubectl -n gpu-operator exec -it\\n\\nCheck the logs of the TensorFlow pod\\n\\nkubectl logs\\n\\n[Optional] Clean the resources\\n\\nkubectl delete deploy tensorflow-gpu ```\\n\\nThiết lập GPU Sharing\\n\\nGPU sharing strategies allow multiple containers to efficiently use your attached GPUs and save running costs. The following tables summarizes the difference between the GPU sharing modes supported by NVIDIA GPUs:\\n\\nChiến lược GPU Sharing cho phép nhiều containers có thể sử dụng chung một node GPU nhằm mục đích tiết kiệm chi phí của bạn. Bảng sau đây tóm tắt sự khác biệt giữa các chiến lược GPU Sharing được NVIDIA hỗ trợ:\\n\\nSharing mode Supported by VKS Workload isolation level Pros Cons Suitable for these workloads Multi-instance GPU (MIG) ❌ Best Processes are executed in parallel Full isolation (dedicated memory and compute resources) Supported by fewer GPU models (only Ampere or more recent architectures) Coarse-grained control over memory and compute resources Recommended for workloads running in parallel and that need certain resiliency and QoS. For example, when running AI inference workloads, multi-instance GPU multi-instance GPU allows multiple inference queries to run simultaneously for quick responses, without slowing each other down. GPU Time-slicing ✅ None Processes are executed concurrently Supported by older GPU architectures (Pascal or newer) No resource limits No memory isolation Lower performance due to context-switching overhead Recommended for bursty and interactive workloads that have idle periods. These workloads are not cost-effective with a fully dedicated GPU. By using time-sharing, workloads get quick access to the GPU when they are in active phases. GPU time-sharing is optimal for scenarios to avoid idling costly GPUs where full isolation and continuous GPU access might not be necessary, for example, when multiple users test or prototype workloads. Workloads that use time-sharing need to tolerate certain performance and latency compromises. Multi-process server (MPS) ✅ Medium Processes are executed parallel Fine-grained control over memory and compute resources allocation No error isolation and memory protection Recommended for batch processing for small jobs because MPS maximizes the throughput and concurrent use of a GPU. MPS allows batch jobs to efficiently process in parallel for small to medium sized workloads. NVIDIA MPS is optimal for cooperative processes acting as a single application. For example, MPI jobs with inter-MPI rank parallelism. With these jobs, each small CUDA process (typically MPI ranks) can run concurrently on the GPU to fully saturate the whole GPU. Workloads that use CUDA MPS need to tolerate the memory protection and error containment limitations .\\n\\nGPU time-slicing\\n\\nGPU time-slicing là kỹ thuật chia sẻ tài nguyên GPU giữa nhiều tiến trình hoặc pod trong Kubernetes bằng cách phân bổ thời gian sử dụng GPU cho từng tiến trình theo chu kỳ.\\n\\nConfigure GPU time-slicing\\n\\nĐể sử dụng GPU time-slicing cho cluster của bạn, bạn cần tạo tệp tin ConfigMap theo mẫu bên dưới để định nghĩa cấu hình time-slicing. Trong đó:\\n\\nreplicas: field chỉ định số lượng pods có thể share chung GPU, tối đa bạn có thể thiết lập 48 pods share chung một GPU.\\n\\nname: mặc định nvidia.com/gpu - lable sử dụng để filter node có GPU.\\n\\nminStrategy: mặc định none do GPU hiện tại chưa hỗ trợ MIG.\\n\\nyaml apiVersion: v1 kind: ConfigMap metadata: name: gpu-sharing-config data: any: |- version: v1 flags: migStrategy: none # Disable MIG, MUST be none in the case your GPU is not supported MIG sharing: timeSlicing: resources: - name: nvidia.com/gpu # Only apply for the node with the node.status contains \\'nvidia.com/gpu\\' replicas: 4 # Allow 4 pods to share the GPU, SHOULD less than 48 pods\\n\\nHoặc bạn có thể chạy câu lệnh bên dưới để apply cấu hình trên cho tất cả các node trong cluster của bạn có lable nvidia.com/gpu .\\n\\nbash kubectl -n gpu-operator create -f \\\\ https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/main/nvidia-gpu/manifest/time-slicing-config-all.yaml * Sau đó, sử dụng lệnh kubectl patch để patch ClusterPolicy và bật GPU time-slicing. Lệnh patch chỉ hoạt động khi ClusterPolicy có trạng thái Ready. * ```bash # Patch the ClusterPolicy kubectl patch clusterpolicies.nvidia.com/cluster-policy \\\\ -n gpu-operator --type merge \\\\ -p \\'{\"spec\": {\"devicePlugin\": {\"config\": {\"name\": \"gpu-sharing-config\", \"default\": \"any\"}}}}\\'\\n\\n# Disable DCGM exporter, time-slicing not support DCGM exporter kubectl patch clusterpolicies.nvidia.com/cluster-policy \\\\ -n gpu-operator --type merge \\\\ -p \\'{\"spec\": {\"dcgmExporter\": {\"enabled\": false}}}\\' ```\\n\\nVerify GPU time-slicing\\n\\nBây giờ, bạn có thể deploy một application có 5 pods sử dụng GPU bằng cách apply yaml theo mấu time-slicing-verification.yaml. Bởi vì cấu hình config map thiết lập bên trên đang có replica = 4 nên pod thứ 5 sẽ có trạng thái Pending state.\\n\\nLần lượt deploy và verify GPU time-slicing của bạn qua các lệnh:\\n\\n```bash\\n\\nApply the manifest\\n\\nkubectl apply -f \\\\ https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/main/nvidia-gpu/manifest/time-slicing-verification.yaml\\n\\nCheck the pods\\n\\nkubectl get pods\\n\\nCheck the logs of the TensorFlow pod\\n\\nkubectl logs\\n\\nGet the event of pending pod\\n\\nkubectl events | grep \"FailedScheduling\"\\n\\n[Optional] Clean the resources\\n\\nkubectl delete deploy time-slicing-verification ```\\n\\nMulti-process server (MPS)\\n\\nNVIDIA Multi-Process Server (MPS) là một giải pháp thay thế và tương thích nhị phân cho giao diện lập trình ứng dụng CUDA (API). Kiến trúc runtime của MPS được thiết kế để cho phép các ứng dụng CUDA đa quy trình hợp tác, thường là các tác vụ MPI, tận dụng khả năng Hyper-Q trên các GPU NVIDIA mới nhất (Kepler và sau đó). Hyper-Q cho phép các kernel CUDA được xử lý đồng thời trên cùng một GPU, điều này có thể mang lại lợi ích về hiệu suất khi dung lượng tính toán GPU không được sử dụng đầy đủ bởi một quy trình ứng dụng duy nhất. Bạn có thể tham khảo thêm về MPS tại Multi-Process Service (MPS).\\n\\nConfigure MPS\\n\\nĐể sử dụng MPS cho cluster của bạn, bạn cần tạo tệp tin ConfigMap theo mẫu bên dưới để định nghĩa cấu hình MPS. Trong đó:\\n\\nreplicas: field chỉ định số lượng pods có thể share chung GPU.\\n\\nname: mặc định nvidia.com/gpu - lable sử dụng để filter node có GPU.\\n\\nminStrategy: mặc định none do GPU hiện tại chưa hỗ trợ MIG.\\n\\nyaml apiVersion: v1 kind: ConfigMap metadata: name: gpu-sharing-config data: any-mps: |- version: v1 flags: migStrategy: none # MIG strategy is not used, this field SHOULD depends on your GPU model sharing: mps: # Enable MPS for the GPU resources: - name: nvidia.com/gpu # Only apply for the node with the node.status contains \\'nvidia.com/gpu\\' replicas: 4 # Allow 4 pods to share the GPU\\n\\nHoặc bạn có thể chạy câu lệnh bên dưới để apply cấu hình trên cho tất cả các node trong cluster của bạn có lable nvidia.com/gpu .\\n\\n```bash\\n\\nDelete the old configmap\\n\\nkubectl -n gpu-operator delete cm gpu-sharing-config kubectl -n gpu-operator create -f \\\\ https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/main/nvidia-gpu/manifest/mps-config-all.yaml `` * Sau đó, sử dụng lệnhkubectl patchđể patch ClusterPolicy và bật GPU MPS. Lệnh patch chỉ hoạt động khiClusterPolicycó trạng tháiReady`.\\n\\n```bash\\n\\nPatch the ClusterPolicy\\n\\nkubectl patch clusterpolicies.nvidia.com/cluster-policy \\\\ -n gpu-operator --type merge \\\\ -p \\'{\"spec\": {\"devicePlugin\": {\"config\": {\"name\": \"gpu-sharing-config\", \"default\": \"any-mps\"}}}}\\'\\n\\nDisable DCGM exporter, MPS not support DCGM exporter\\n\\nkubectl patch clusterpolicies.nvidia.com/cluster-policy \\\\ -n gpu-operator --type merge \\\\ -p \\'{\"spec\": {\"dcgmExporter\": {\"enabled\": false}}}\\'\\n\\nCheck MPS server is running or not\\n\\nkubectl -n gpu-operator get pods ```\\n\\nVerify MPS\\n\\nBây giờ, bạn có thể deploy một application có 5 pods sử dụng GPU bằng cách apply yaml theo mẫu mps-verification.yaml. Bởi vì cấu hình config map thiết lập bên trên đang có replica = 4 nên pod thứ 5 sẽ có trạng thái Pending state.\\n\\nLần lượt deploy và verify GPU MPS của bạn qua các lệnh:\\n\\n```bash\\n\\nApply the manifest\\n\\nkubectl apply -f \\\\ https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/main/nvidia-gpu/manifest/mps-verification.yaml\\n\\nCheck the pods\\n\\nkubectl get pods\\n\\nCheck the logs of the TensorFlow pod\\n\\nkubectl logs -l job-name=nbody-sample\\n\\n[Optional] Clean the resources\\n\\nkubectl delete job nbody-sample ```\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nTrên một node GPU chỉ có thể sử dụng một trong hai chiến lược chia sẻ GPU là Time Slicing hoặc MPS, không thể sử dụng đồng thời cả hai. Lý do là vì cả hai chiến lược đều sử dụng cùng một tài nguyên GPU vật lý, và việc sử dụng cả hai đồng thời sẽ dẫn đến xung đột và hiệu suất không mong muốn.\\n\\nTuy nhiên, nếu bạn có một node group gồm nhiều node GPU, bạn có thể sử dụng hai chiến lược khác nhau trên hai node riêng biệt. Ví dụ: bạn có thể sử dụng Time Slicing trên một node để chia sẻ GPU và sử dụng MPS trên node còn lại để chia sẻ GPU cho các ứng dụng khác. Chi tiết tham khảo mục bên dưới. {% endhint %}\\n\\nApplying Multiple Node-Specific Configurations\\n\\nNếu bạn có một node group gồm nhiều node GPU, bạn có thể sử dụng hai chiến lược khác nhau trên hai node riêng biệt. Ví dụ: bạn có thể sử dụng Time Slicing trên một node để chia sẻ GPU và sử dụng MPS trên node còn lại để chia sẻ GPU cho các ứng dụng khác. Ví dụ, tôi đã tạo 2 node group trong một Cluster với các thông số như sau:\\n\\nNodeGroup 1 có instance GPU RTX 2080Ti.\\n\\nNodeGroup 2 có instance GPU RTX 4090.\\n\\nHiện tại, bạn mong muốn:\\n\\nNodeGroup 1 có instance GPU RTX 2080Ti sẽ chạy 4 pods sharing GPU sử dụng time-slicing.\\n\\nNodeGroup 2 có instance GPU RTX 4090 sẽ chạy 8 pods sharing GPU sử dụng MPS.\\n\\nConfigure Multiple Node-Specific Configurations\\n\\nĐể sử dụng GPU time-slicing, MPS cho cluster của bạn, bạn cần tạo tệp tin ConfigMap theo mẫu bên dưới để định nghĩa cấu hình time-slicing, MPS mong muốn.\\n\\nyaml apiVersion: v1 kind: ConfigMap metadata: name: gpu-multi-sharing-config data: rtx-2080ti: |- # Same the name with the name that you label the GPU node before version: v1 flags: migStrategy: none # MIG strategy is not used, this field SHOULD depends on your GPU model sharing: timeSlicing: resources: - name: nvidia.com/gpu replicas: 4 # Allow the node using this GPU to be shared by 4 pods rtx-4090: |- # Same the name with the name that you label the GPU node before version: v1 flags: migStrategy: none # MIG strategy is not used, this field SHOULD depends on your GPU model sharing: mps: resources: - name: nvidia.com/gpu replicas: 8 # Allow the node using this GPU to be shared by 8 pods * Hoặc bạn có thể chạy câu lệnh bên dưới để apply cấu hình trên cho tất cả các node trong cluster của bạn có lable nvidia.com/gpu .\\n\\nbash kubectl -n gpu-operator create -f \\\\ https://raw.githubusercontent.com/vngcloud/kubernetes-sample-apps/main/nvidia-gpu/manifest/multiple-gpu-sharing.yaml * Sau đó, sử dụng lệnh kubectl patch để patch ClusterPolicy và bật GPU time-slicing, MPS. Lệnh patch chỉ hoạt động khi ClusterPolicy có trạng thái Ready.\\n\\n```bash\\n\\nPatch the ClusterPolicy\\n\\nkubectl patch clusterpolicies.nvidia.com/cluster-policy \\\\ -n gpu-operator --type merge \\\\ -p \\'{\"spec\": {\"devicePlugin\": {\"config\": {\"name\": \"gpu-multi-sharing-config\"}}}}\\'\\n\\nDisable DCGM exporter\\n\\nkubectl patch clusterpolicies.nvidia.com/cluster-policy \\\\ -n gpu-operator --type merge \\\\ -p \\'{\"spec\": {\"dcgmExporter\": {\"enabled\": false}}}\\'\\n\\nCheck the ClusterPolicy\\n\\nkubectl get clusterpolicy ```\\n\\nBây giờ, bạn cần thêm label cho node với tên mà bạn chỉ định tại trong file ConfigMap:\\n\\n```bash\\n\\nGet the node names\\n\\nkubectl get nodes\\n\\nLabel the node with the name that you specified in the ConfigMap\\n\\nkubectl label node\\n\\nVerify Multiple Node-Specific Configurations\\n\\nBên dưới đây là câu lệnh chúng tôi hướng dẫn bạn training MNIST model trong TensorFlow sử dụng 2 node GPU RTX 2080Ti và RTX 4090. Node RTX 2080Ti sẽ được share chung cho 4 pods sử dụng Time-slicing và node RTX 4090 sẽ được share chung cho 8 pods sử dụng MPS. Tham khảo thêm về TensorFlow tại tensorflow-mnist-sample.yaml.\\n\\n```bash\\n\\nApply the manifest\\n\\nkubectl apply -f \\\\ https://github.com/vngcloud/kubernetes-sample-apps/raw/main/nvidia-gpu/manifest/tensorflow-mnist-sample.yaml\\n\\nCheck the pods\\n\\nkubectl get pods -owide\\n\\nCheck the logs of the TensorFlow pod\\n\\nkubectl logs\\n\\n[Optional] Clean the resources\\n\\nkubectl delete deploy tensorflow-mnist ```\\n\\nGiám sát hoạt động GPU Resources\\n\\nGiám sát tài nguyên GPU NVIDIA trong cụm Kubernetes là điều cần thiết để đảm bảo hiệu suất tối ưu, sử dụng tài nguyên hiệu quả và giải quyết vấn đề một cách chủ động. Sau đây là hướng dẫn của chúng tôi về việc thiết lập và sử dụng Prometheus và NVIDIA Data Center GPU Manager (DCGM) để giám sát tài nguyên GPU trong môi trường Kubernetes.\\n\\nĐầu tiên, bạn cần cài đặt Prometheus Stack và Prometheus Adapter để tích hợp với Kubernetes API server qua lệnh:\\n\\n```bash\\n\\nInstall Prometheus Stack using Helm\\n\\nhelm install --wait prometheus-stack \\\\ --namespace prometheus --create-namespace \\\\ oci://vcr.vngcloud.vn/81-vks-public/vks-helm-charts/kube-prometheus-stack \\\\ --version 60.0.2 \\\\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false\\n\\nInstall and configure Prometheus Adapter using Helm\\n\\nprometheus_service=$(kubectl get svc -n prometheus -lapp=kube-prometheus-stack-prometheus -ojsonpath=\\'{range .items[*]}{.metadata.name}{\"\\\\n\"}{end}\\') helm install --wait prometheus-adapter \\\\ --namespace prometheus --create-namespace \\\\ oci://vcr.vngcloud.vn/81-vks-public/vks-helm-charts/prometheus-adapter \\\\ --version 4.10.0 \\\\ --set prometheus.url=http://${prometheus_service}.prometheus.svc.cluster.local ```\\n\\nSau khi cài đặt thành công, chạy lệnh bên dưới để kiểm tra các resource Prometheus đang chạy:\\n\\n```bash\\n\\nCheck the resources of Prometheus are running\\n\\nkubectl -n prometheus get all ```\\n\\nTiếp theo, bạn cần chạy lệnh bên dưới để enable DCGM exporter để giám sát GPU resources trên cluster của bạn:\\n\\n```bash\\n\\nEnable the DCGM exporter\\n\\nkubectl patch clusterpolicies.nvidia.com/cluster-policy \\\\ -n gpu-operator --type merge \\\\ -p \\'{\"spec\": {\"dcgmExporter\": {\"enabled\": true}}}\\'\\n\\nConfirm Prometheus can scrape the DCGM exporter metrics, sometime you MUST wait for a few minutes\\n\\n(about 1-3 mins) for the DCGM exporter to be ready\\n\\nkubectl get --raw /apis/custom.metrics.k8s.io/v1beta1 | jq -r . | grep DCGM ```\\n\\nBây giờ, bạn hãy chạy lệnh bên dưới để chuyển Prometheus Adapter tới máy localhost của bạn và sau đó kiểm tra các metrics đã thu thập được thông qua http://localhost:9090\\n\\n```bash\\n\\nForward the Prometheus Adapter to your local machine\\n\\nkubectl -n prometheus \\\\ port-forward svc/prometheus-stack-kube-prom-prometheus 9090:9090 ```\\n\\nBên dưới là danh sách một vài GPU Metrics được sử dụng thường xuyên. Tham khảo danh sách GPU metric đầy đủ tại Field Identifiers.\\n\\nBảng 1: Usage\\n\\nMetric Name Metric Type Unit Description DCGM_FI_DEV_GPU_UTIL Gauge Percentage GPU usage. DCGM_FI_DEV_MEM_COPY_UTIL Gauge Percentage Memory usage. DCGM_FI_DEV_ENC_UTIL Gauge Percentage Encoder usage. DCGM_FI_DEV_DEC_UTIL Gauge Percentage Decoder usage.\\n\\nBảng 2: Memory\\n\\nMetric Name Metric Type Unit Description DCGM_FI_DEV_FB_FREE Gauge MB Number of remaining frame buffers. The frame buffer is called VRAM. DCGM_FI_DEV_FB_USED Gauge MB Number of used frame buffers. The value is the same as the value of memory-usage in the nvidia-smi command.\\n\\nBảng 3: Temperature and power\\n\\nMetric Name Metric Type Unit Description DCGM_FI_DEV_GPU_TEMP Gauge °C Current GPU temperature of the device. DCGM_FI_DEV_POWER_USAGE Gauge W Power usage of the device.\\n\\nAutoscaling GPU Resources\\n\\nĐể enable autoscaling GPU Resource, bạn cần:\\n\\nBật tính năng Autoscale cho GPU Nodegroups theo hướng dẫn tại đây.\\n\\nCài đặt Keda thông qua Helm chart trên Cluster của bạn qua lệnh:\\n\\n```bash helm install --wait kedacore \\\\ --namespace keda --create-namespace \\\\ oci://vcr.vngcloud.vn/81-vks-public/vks-helm-charts/keda \\\\ --version 2.14.2\\n\\nkubectl -n keda get all ```\\n\\nNếu BẠN KHÔNG cài đặt Keda trong cụm của mình, tính năng Auto-scale của VKS sẽ can thiệp vào và:\\n\\nPhát hiện các pod đang ở trạng thái chờ (Pending).\\n\\nTự động mở rộng Nodegroup chứa GPU để xử lý các pod này.\\n\\nĐiều này xảy ra khi số replica Deployment lớn hơn số GPU khả dụng mà bạn đã cấu hình trong ConfigMap. * Nếu BẠN CÓ cài đặt Keda trong cụm của mình, bạn có thể sử dụng ScaledObject để scale GPU Nodegroup dựa trên metrics mà bạn mong muốn. Ví dụ, bạn có thể scale GPU Nodegroup dựa trên GPU usage, memory usage hoặc các metrics khác. Ví dụ:\\n\\nyaml apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: scaled-object spec: scaleTargetRef: name: scaling-app # The name of the Deployment, MUST in same namespace minReplicaCount: 1 # Optional. Default: 0 maxReplicaCount: 3 # Optional. Default: 100 triggers: # Will be trigger if either of these triggers is true - type: prometheus metadata: # prometheus-stack-kube-prom-prometheus serverAddress: http://prometheus-stack-kube-prom-prometheus.prometheus.svc.cluster.local:9090 metricName: engine_active query: sum(DCGM_FI_DEV_GPU_UTIL) / count(DCGM_FI_DEV_GPU_UTIL) / 100 threshold: \\'0.5\\' # Scale the GPU Nodegroup when the GPU usage is greater than 50% - type: prometheus metadata: # prometheus-stack-kube-prom-prometheus serverAddress: http://prometheus-stack-kube-prom-prometheus.prometheus.svc.cluster.local:9090 metricName: engine_active query: sum(DCGM_FI_DEV_MEM_COPY_UTIL) / count(DCGM_FI_DEV_MEM_COPY_UTIL) / 100 threshold: \\'0.5\\' # Scale the GPU Nodegroup when the GPU memory usage is greater than 50% * Cấu hình này điều chỉnh số lượng GPU trong Nodegroup dựa trên GPU usage và memory usage. Trong đó: * Fieldquery chỉ định truy vấn để lấy số liệu từ Prometheus. * Fieldthreshold chỉ định giá trị ngưỡng để điều chỉnh số lượng GPU trong Nodegroup. * Field minReplicaCount và maxReplicaCount chỉ định số lượng tối thiểu và tối đa mà Nodegroup GPU có thể điều chỉnh đến. * Apply scaling-app.yaml manifest to generate resources for testing the autoscaling feature. This manifest run 1 pod of CUDA VectorAdd Test and the GPU Nodegroup will be scaled to 3 when the GPU usage is greater than 50%. * Thực hiện apply tệp tin scaling-app.yaml để generate resources sử dụng verify cho tính năng autoscaling:\\n\\nbash kubectl apply -f \\\\ https://github.com/vngcloud/kubernetes-sample-apps/raw/main/nvidia-gpu/manifest/scaling-app.yaml * Tiếp tục thực hiện apply tệp tin scale-gpu.yaml để tạo ScaleObject cho deplyment của bạn:\\n\\n```bash kubectl apply -f \\\\ https://github.com/vngcloud/kubernetes-sample-apps/raw/main/nvidia-gpu/manifest/scale-gpu.yaml\\n\\nkubectl get deploy\\n\\nCheck the ScaledObject\\n\\nkubectl get scaledobject ```\\n\\nKhi trạng thái ScaledObject Ready làTrue, GPU Nodegroup được scale dựa trên GPU usage.'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/huong-dan-cai-dat-va-cau-hinh-cong-cu-kubectl-trong-kubernetes.md'}, page_content='Hướng dẫn cài đặt và cấu hình công cụ kubectl trong Kubernetes\\n\\nĐiều kiện cần\\n\\nBạn cần phải sử dụng phiên bản kubectl sai lệch không quá một phiên bản với version của cluster. Ví dụ, một client v1.2 nên được hoạt động với master v1.1, v1.2 và v1.3. Sử dụng phiên bản mới nhất của kubectl giúp tránh được các vấn đề không lường trước được.\\n\\nCài đặt kubectl trên Linux\\n\\nCài đặt kubectl binary với curl trên Linux\\n\\nBước 1: Tải về phiên bản mới nhất với câu lệnh:\\n\\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl\\n\\nĐể tải về phiên bản cụ thể, hãy thay thế phần $(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt) trong câu lệnh với một phiên bản cụ thể.\\n\\nVí dụ, để tải về phiên bản v1.17.0 trên Linux, hãy chạy lệnh:\\n\\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.17.0/bin/linux/amd64/kubectl\\n\\nBước 2: Tạo kubectl binary thực thi qua lệnh:\\n\\nchmod +x ./kubectl\\n\\nBước 3: Đưa bản binary vào biến môi trường PATH của bạn qua lệnh:\\n\\nsudo mv ./kubectl /usr/local/bin/kubectl\\n\\nBước 4: Kiểm tra chắc chắn rằng phiên bản bạn cài là mới nhất qua lệnh:\\n\\nkubectl version\\n\\nCài đặt kubectl sử dụng trình quản lý gói\\n\\nVới các hệ điều hành CentOS, RHEL hoặc Fedora, bạn có thể chạy lệnh:\\n\\nsudo apt-get update && sudo apt-get install -y apt-transport-httpscurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.listsudo apt-get updatesudo apt-get install -y kubectl\\n\\nCài đặt kubectl với snap\\n\\nNếu bạn đang sử dụng Ubuntu hoặc distro Linux khác hỗ trợ trình quản lý gói snap, thì kubectl đã có sẵn trong snap.\\n\\nBước 1: Chuyển sang user snap và thực thi câu lệnh cài đặt:\\n\\nsudo snap install kubectl --classic\\n\\nBước 2: Kiểm tra phiên bản bạn vừa cài là mới nhất:\\n\\nkubectl version\\n\\nCài đặt kubectl trên macOS\\n\\nCài đặt kubectl binary với curl trên macOS\\n\\nBước 1: Tải về phiên bản mới nhất:\\n\\nkubectl versioncurl -LO \" https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl \"\\n\\nĐể tải về phiên bản cụ thể, hãy thay thế phần $(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt) trong câu lệnh với phiên bản cụ thể.\\\\ Ví dụ, để tải về phiên bản v1.17.0 trên macOS, hãy chạy lệnh:\\n\\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.17.0/bin/darwin/amd64/kubectl\\n\\nBước 2: Tạo kubectl binary thực thi qua lệnh:\\n\\nchmod +x ./kubectl\\n\\nBước 3: Đưa bản binary vào biến môi trường PATH của bạn qua lệnh:\\n\\nsudo mv ./kubectl /usr/local/bin/kubectl\\n\\nBước 4: Kiểm tra chắc chắn rằng phiên bản bạn cài là mới nhất qua lệnh:\\n\\nkubectl version\\n\\nCài đặt với Homebrew trên macOS\\n\\nNếu bạn đang trên macOS và sử dụng trình quản lý gói Homebrew, bạn có thể cài đặt kubectl với Homebrew.\\n\\nBước 1: Chạy câu lệnh cài đặt:\\n\\nbrew install kubernetes-cli\\n\\nhoặc lệnh:\\n\\nbrew install kubectl\\n\\nBước 2: Kiểm tra chắc chắn rằng phiên bản bạn cài là mới nhất:\\n\\nkubectl version\\n\\nCài đặt với Macports trên macOS\\n\\nNếu bạn đang trên macOS và sử dụng trình quản lý gói Macports, bạn có thể cài đặt kubectl với Macports.\\n\\nBước 1: Chạy câu lệnh cài đặt:\\n\\nsudo port selfupdatesudo port install kubectl\\n\\nBước 2: Kiểm tra chắc chắn rằng phiên bản bạn cài là mới nhất:\\n\\nsudo port selfupdatesudo port install kubectl\\n\\nCài đặt kubectl trên Windows\\n\\nCài đặt kubectl binary với curl trên Windows\\n\\nBước 1: Tải về phiên bản mới nhất v1.17.0 từ đường dẫn này.Hoặc nếu bạn đã cài đặt curl, hãy sử dụng câu lệnh sau:\\n\\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.17.0/bin/windows/amd64/kubectl.exe\\n\\nĐể tìm ra phiên bản ổn định mới nhất, hãy xem https://storage.googleapis.com/kubernetes-release/release/stable.txt.\\n\\nBước 2: Đưa bản binary vào biến môi trường PATH của bạn.\\n\\nBước 3: Kiểm tra chắn chắn phiên bản kubectl giống với bản đã tải về:\\n\\nkubectl version\\n\\nGhi chú: Docker Desktop cho Windows thêm phiên bản kubectl riêng của nó vào PATH. Nếu bạn đã cài đặt Docker Desktop trước đây, bạn có thể cần đặt đường dẫn PATH của bạn trước khi bản cài đặt Docker Desktop thêm 1 PATH vào hoặc loại bỏ kubectl của Docker Desktop.\\n\\nCài đặt với Powershell từ PSGallery\\n\\nNếu bạn đang ở trên Windows và sử dụng trình quản lý gói Powershell Gallery, bạn có thể cài đặt và cập nhật kubectl với Powershell.\\n\\nBước 1: Thực thi các câu lệnh cài đặt sau (hãy đảm bảo bạn tự định nghĩa DownloadLocation):\\n\\nInstall-Script -Name install-kubectl -Scope CurrentUser -Forceinstall-kubectl.ps1 [-DownloadLocation <path>]\\n\\nGhi chú: Nếu bạn không định nghĩa DownloadLocation, kubectl sẽ được cài đặt ở thư mục temp của user.\\n\\nBản cài đặt sẽ tạo ra $HOME/.kube và hướng dẫn để tạo ra file cấu hình\\n\\nBước 1: Kiểm tra chắc chắn rằng phiên bản bạn cài là mới nhất:\\n\\nkubectl version\\n\\nGhi chú: Cập nhật của bản cài đặt sẽ được thực hiện khi chạy lại các câu lệnh từ bước 1.\\n\\nCài đặt trên Windows sử dụng Chocolatey hoặc Scoop\\n\\nBước 1: Để cài đặt kubectl trên Windows bạn có thể sử dụng trình quản lý gói Chocolatey hoặc bộ cài đặt câu lệnh Scoop.\\n\\nNếu bạn dùng Choco\\n\\nchoco install kubernetes-cli\\n\\nNếu bạn dùng Scoop\\n\\nscoop install kubectl\\n\\nBước 2: Kiểm tra chắc chắn rằng phiên bản bạn cài là mới nhất:\\n\\nkubectl version\\n\\nBước 3: Di chuyển tới thư mục home của bạn:\\n\\ncd %USERPROFILE%\\n\\nBước 4: Tạo thư mục .kube:\\n\\nmkdir .kube\\n\\nBước 5: Di chuyển tới thư mục .kube bạn vừa mới tạo:\\n\\ncd .kube\\n\\nBước 6: Cấu hình kubectl để sử dụng Kubernetes cluster từ xa:\\n\\nNew-Item config -type file\\n\\nGhi chú: Chỉnh sửa file cấu hình với trình soạn thảo văn bản, như là Notepad.\\n\\nTải xuống từ một phần của Google Cloud SDK\\n\\nBạn có thể cài đặt kubectl từ một phần của Google Cloud SDK.\\n\\nBước 1: Cài đặt Google Cloud SDK.\\n\\nBước 2: Thực thi câu lệnh cài đặt kubectl:\\n\\ngcloud components install kubectl\\n\\nBước 3: Kiểm tra chắc chắn rằng phiên bản bạn cài là mới nhất:\\n\\nkubectl version\\n\\nXác minh cấu hình kubectl\\n\\nĐể kubectl tìm kiếm và truy cập Kubernetes cluster, nó cần một kubeconfig file, được tự động tạo ra khi bạn tạo mới một cluster sử dụng kube-up.sh hoặc triển khai thành công một Minikube cluster. Mặc định thì cấu hình của kubectl được xác định tại ~/.kube/config.\\n\\nKiểm tra kubectl được cấu hình đúng bằng việc xem trạng thái của cluster:\\\\ kubectl cluster-info\\n\\nkubectl cluster-info\\n\\nNếu bạn trông thấy một URL phản hồi, thì kubectl đã được cấu hình đúng để truy cập vào cluster của bạn.\\n\\nNếu bạn trông thấy một tin nhắn tương tự bên dưới, thì kuberctl chưa được cấu hình đúng hoặc chưa thể kết nối với Kubernetes cluster.\\\\ The connection to the server <server-name:port> was refused - did you specify the right host or port?\\\\ Ví dụ như, nếu bạn đang định chạy một Kubernetes cluster trên laptop của bạn (locally), bạn sẽ cần một công cụ như Minikube được cài trước đó và chạy lại các câu lệnh bên trên.\\\\ Nếu kubectl cluster-info trả về url nhưng bạn không thể truy cập vào cluster của bạn, thì hãy kiểm tra nó đã được cấu hình đúng hay chưa, bằng cách:\\n\\nkubectl cluster-info dump\\n\\nCác lựa chọn cấu hình kubectl\\n\\nKích hoạt shell autocompletion\\n\\nkubectl cung cấp autocompletion hỗ trợ cho Bash and Zsh, giúp bạn giảm thiểu việc phải gõ nhiều câu lệnh.\\n\\nBên dưới đây là các bước để thiết lập autocompletion cho Bash (bao gồm sự khác nhau giữa Linux và macOS) và Zsh.\\n\\nBash trên Linux\\n\\nGiới thiệu\\n\\nKubelet completion script cho Bash được tạo ra với câu lệnh kubectl completion bash. Sau khi script được tạo ra, bạn cần source (thực thi) script đó để kích hoạt tính năng autocompletion.\\n\\nTuy nhiên, completion script phụ thuộc vào bash-completion, nên bạn phải cài đặt bash-completion trước đó (kiểm tra bash-completion tồn tại với câu lệnh type _init_completion).\\n\\nCài đặt bash-completion\\n\\nbash-completion được cung cấp bởi nhiều trình quản lý gói (xem tại đây). Bạn có thể cài đặt với lệnh apt-get install bash-completion hoặc yum install bash-completion.\\n\\nCác lệnh trên tạo ra /usr/share/bash-completion/bash_completion, đây là script chính của bash-completion. Tùy thuộc vào trình quản lý gói của bạn, mà bạn phải source (thực thi) file này trong file ~/.bashrc.\\n\\nĐể tìm ra file này, reload lại shell hiện tại của bạn và chạy lệnh type _init_completion. Nếu thành công, bạn đã thiết lập xong, không thì hãy thêm đoạn sau vào file ~/.bashrc của bạn:\\n\\nsource /usr/share/bash-completion/bash_completion\\n\\nReload lại shell của bạn và xác nhận bash-completion được cài đặt đúng bằng lệnh type _init_completion.\\n\\nKích hoạt kubectl autocompletion\\n\\nBây giờ bạn cần đảm bảo rằng kubectl completion script được sourced trên tất cả các session của shell. Có 2 cách để làm việc này:\\n\\nSource script trong file ~/.bashrc:\\n\\necho \\'source <(kubectl completion bash)\\' >>~/.bashrc\\n\\nThêm script vào thư mục /etc/bash_completion.d:\\n\\nkubectl completion bash >/etc/bash_completion.d/kubectl\\n\\nNếu bạn có một alias cho kubectl, bạn có thể thêm một shell completion nữa cho alias đó:\\n\\necho \\'alias k=kubectl\\' >>~/.bashrcecho \\'complete -F __start_kubectl k\\' >>~/.bashrc\\n\\nGhi chú: bash-completion sources tất cả completion scripts trong /etc/bash_completion.d.\\n\\nCác cách trên đều hiệu quả tương đương nhau. Sau khi reload lại shell, kubectl autocompletion sẽ làm việc.\\n\\nBash trên macOS\\n\\nGiới thiệu\\n\\nKubectl completion script trên Bash được tạo ra bởi kubectl completion bash. Source script này sẽ kích hoạt tính năng kubectl completion.\\n\\nTuy nhiên, kubectl completion script phụ thuộc vào bash-completion mà bạn cài trước đó.\\n\\nCảnh báo: Có 2 phiên bản của bash-completion là v1 và v2. V1 dành cho Bash 3.2 (Bash mặc định trên macOS), và v2 dành cho Bash 4.1+. Kubectl completion script không làm việc phù hợp với bash-completion v1 và Bash 3.2. Nó tương thích với bash-completion v2 và Bash 4.1+. Vì vậy, để sử dụng kubectl completion một cách chính xác trên macOS thì bạn phải cài đặt Bash 4.1+ (hướng dẫn). Hướng dẫn tiếp theo giả định rằng bạn đang sử dụng Bash 4.1+ (nghĩa là, bất kỳ phiên bản Bash nào từ 4.1 trở lên).\\n\\nCài đặt bash-completion\\n\\nGhi chú: Như đã đề cập, những hướng dẫn này giả định bạn đang sử dụng Bash 4.1+, có nghĩa rằng bạn sẽ cài đặt bash-completion v2 (trái ngược với Bash 3.2 và bash-completion v1, trong trường hợp đó, kubectl completion sẽ không hoạt động).\\n\\nBạn có thể kiểm tra bash-completion v2 đã cài đặt trước đó chưa với lệnh type _init_completion. Nếu chưa, bạn có thể cài đặt nó với Homebrew:\\n\\nbrew install bash-completion@2\\n\\nTừ đầu ra của lệnh này, hãy thêm đoạn sau vào file ~/.bashrc của bạn:\\\\ export BASH_COMPLETION_COMPAT_DIR=\"/usr/local/etc/bash_completion.d\"\\n\\nexport BASH_COMPLETION_COMPAT_DIR=\"/usr/local/etc/bash_completion.d\"[[ -r \"/usr/local/etc/profile.d/bash_completion.sh\" ]] && . \"/usr/local/etc/profile.d/bash_completion.sh\"\\n\\nTải lại shell của bạn và xác minh rằng bash-completion v2 được cài đặt chính xác chưa bằng lệnh type _init_completion.\\n\\nKích hoạt kubectl autocompletion\\n\\nBây giờ bạn phải đảm bảo rằng kubectl completion script đã được sourced trong tất cả các phiên shell của bạn. Có nhiều cách để đạt được điều này:\\n\\nSource completion script trong file ~/.bashrc:\\n\\necho \\'source <(kubectl completion bash)\\' >>~/.bashrc\\n\\nThêm completion script vào thư mục /usr/local/etc/bash_completion.d:\\n\\nkubectl completion bash >/usr/local/etc/bash_completion.d/kubectl\\n\\nNếu bạn có alias cho kubectl, bạn có thể mở rộng shell completion để làm việc với alias đó:\\n\\necho \\'alias k=kubectl\\' >>~/.bashrcecho \\'complete -F __start_kubectl k\\' >>~/.bashrc * Nếu bạn đã cài kubectl với Homebrew (như đã giới thiệu bên trên)) thì kubectl completion script sẽ có trong /usr/local/etc/bash_completion.d/kubectl. Trong trường hợp này thì bạn không cần làm gì cả.\\n\\nGhi chú: Cài đặt theo cách Homebrew đã source tất cả các tệp trong thư mục BASH_COMPLETION_COMPAT_DIR, đó là lý do tại sao hai phương thức sau hoạt động.\\n\\nTrong mọi trường hợp, sau khi tải lại shell của bạn, kubectl completion sẽ hoạt động.\\n\\nZsh\\n\\nKubectl completion script cho Zsh được tạo ra với lệnh kubectl completion zsh. Source completion script trong shell của bạn sẽ kích hoạt kubectl autocompletion.\\\\ Để nó hoạt động cho tất cả các shell, thêm dòng sau vào file ~/.zshrc:\\n\\nsource <(kubectl completion zsh)\\n\\nNếu bạn có alias cho kubectl, bạn có thể mở rộng shell completion để làm việc với alias đó:\\n\\necho \\'alias k=kubectl\\' >>~/.zshrcecho \\'complete -F __start_kubectl k\\' >>~/.zshrc\\n\\nSau khi tải lại shell, kubectl autocompletion sẽ hoạt động.\\n\\nNếu bạn nhận được lỗi complete:13: command not found: compdef, thêm dòng sau vào đầu file ~/.zshrc:\\n\\nautoload -Uz compinitcompinit\\n\\nChi tiết tham khảo thêm tại kubernetes.io'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/su-dung-terraform-de-khoi-tao-cluster-va-node-group.md'}, page_content='Sử dụng Terraform để khởi tạo Cluster và Node Group\\n\\nTổng quan\\n\\nTerraform là một công cụ mã nguồn mở được sử dụng để tự động hóa việc cung cấp và quản lý cơ sở hạ tầng như máy ảo, mạng, lưu trữ và Kubernetes.\\n\\nVới Terraform, bạn có thể mô tả cơ sở hạ tầng mong muốn bằng mã, sau đó Terraform sẽ thực hiện các thao tác cần thiết để tạo hoặc cập nhật cơ sở hạ tầng cho phù hợp với mô tả của bạn.\\n\\nKhởi tạo Cluster và Node Group\\n\\nĐể khởi tạo một Cluster Kubernetes bằng Terraform, bạn cần thực hiện các bước sau:\\n\\nTruy cập IAM Portal tại đây, thực hiện tạo Service Account với quyền hạn VKS Full Access. Cụ thể, tại trang IAM, bạn có thể:\\n\\nChọn \"Create a Service Account\", điền tên cho Service Account và nhấn Next Step để gắn quyền cho Service Account.\\n\\nTìm và chọn Policy: VKSFullAccess sau đó nhấn \"Create a Service Account\" để tạo Service Account, Policy: VKSFullAccess do VNG Cloud tạo ra, bạn không thể xóa các policy này.\\n\\nSau khi tạo thành công bạn cần phải lưu lại Client_ID và Secret_Key của Service Account để thực hiện bước tiếp theo.\\n\\nTruy cập VKS Portal tại đây, thực hiện Activate dịch vụ VKS ở tab Overview. Hãy chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn.\\n\\nCài đặt Terraform:\\n\\nTải xuống và cài đặt Terraform cho hệ điều hành của bạn từ https://developer.hashicorp.com/terraform/install.\\n\\nKhởi tạo cấu hình Terraform:\\n\\nTạo tệp variable.tf và khai báo thông tin Service Account trong file này.\\n\\nTạo tệp main.tf và định nghĩa các tài nguyên Kubernetes Cluster mà bạn muốn tạo.\\n\\nVí dụ:\\n\\nTệp variable.tf:bạn cần thay thế Client ID và Client Secret đã khởi tạo ở bước 1 ở file này.\\n\\nvariable \"client_id\" { type = string default = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" } variable \"client_secret\" { type = string default = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }\\n\\nTệp main.tf: trong ví dụ này tôi thực hiện khởi tạo một Cluster và một Node Group có thông tin sau:\\n\\nTên Cluster: my-cluster\\n\\nK8S Version: v1.28.8\\n\\nMode: Public Cluster và Public Node Group\\n\\nTên Node Group: my-nodegroup\\n\\nBật AutoScaling: scale từ 0 tới 5 nodes\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nTrong file main.tf, để khởi tạo một cluster với một node group, bạn bắt buộc cần truyền vào các thông số sau:\\n\\nhcl vpc_id = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" {% endhint %}\\n\\nFile main.tf mẫu giúp bạn tạo Cluster và Node Group theo cấu hình bên trên:\\n\\n``` terraform { required_providers { vngcloud = { source = \"vngcloud/vngcloud\" version = \"1.2.2\" } } }\\n\\nprovider \"vngcloud\" { token_url = \"https://iamapis.vngcloud.vn/accounts-api/v2/auth/token\" client_id = var.client_id client_secret = var.client_secret vserver_base_url = \"https://hcm-3.api.vngcloud.vn/vserver/vserver-gateway\" vlb_base_url = \"https://hcm-3.api.vngcloud.vn/vserver/vlb-gateway\" }\\n\\nresource \"vngcloud_vks_cluster\" \"primary\" { name = \"my-cluster\" description = \"VNGCLOUD uses terraform\" version = \"v1.28.8\" cidr = \"172.16.0.0/16\" enable_private_cluster = false network_type = \"CALICO\" vpc_id = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" enabled_load_balancer_plugin = true enabled_block_store_csi_plugin = true }\\n\\nresource \"vngcloud_vks_cluster_node_group\" \"primary\" { cluster_id= vngcloud_vks_cluster.primary.id name= \"my-nodegroup\" num_nodes auto_scale_config { min_size = 0 max_size = 5 } upgrade_config { strategy = \"SURGE\" max_surge = 1 max_unavailable = 0 } image_id = \"img-983d55cf-9b5b-44cf-aa72-23f3b25d43ce\" flavor_id = \"flav-9e88cfb4-ec31-4ad4-8ba5-243459f6dc4b\" disk_size = 20 disk_type = \"vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018\" enable_private_nodes = false ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" labels = { \"mylabel\" = \"vngcloud\" } taint { key = \"mykey\" value = \"myvalue\" effect = \"PreferNoSchedule\" } } ```\\n\\nKhởi tạo Terraform:\\n\\nChạy lệnh terraform init.Lệnh này sẽ tải xuống các plugin cần thiết và khởi tạo trạng thái Terraform.\\n\\nÁp dụng cấu hình Terraform:\\n\\nChạy lệnh terraform apply. Lệnh này sẽ tạo Cluster Kubernetes theo mô tả trong tệp main.tf.\\n\\nTham khảo thêm về cách sử dụng Terraform để làm việc với VKS tại đây.'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/expose-mot-service-thong-qua-vlb-layer7/tu-dong-quan-ly-certificate-trong-vks-voi-nginx-ingress-controller-cert-manager-va-lets-encrypt.md'}, page_content=\"Tự động quản lý Certificate trong VKS với Nginx Ingress Controller, Cert-Manager, và Let's Encrypt\\n\\nĐiều kiện cần\\n\\nBạn đã thực hiện khởi tạo Cluster trên hệ thống VKS theo các hướng dẫn tại đây và trên cụm của bạn đã được cài đặt VNGCloud LoadBalancer Controller.\\n\\nTiếp theo, hãy đảm bảo bận có một domain đã được đăng ký và sử dụng.\\n\\nCuối cùng, bạn cần một địa chỉ email để thực hiện kiểm tra việc quản lý Certificate.\\n\\nTiếp theo, bạn cần thực hiện cài đặt nginx-ingress-controller theo lệnh:\\n\\nbash helm install nginx-ingress-controller oci://ghcr.io/nginxinc/charts/nginx-ingress --namespace kube-system\\n\\nCài đặt Cert-Manager\\n\\nCert-Manager chịu trách nhiệm tự động cấp phát và gia hạn chứng chỉ từ Let's Encrypt.\\n\\nSử dụng Helm để cài đặt Cert-Manager qua lệnh:\\n\\n```bash helm install \\\\ cert-manager jetstack/cert-manager \\\\ --namespace cert-manager \\\\ --create-namespace \\\\ --version v1.16.2 \\\\ --set crds.enabled=true\\n\\n```\\n\\nCác bước thực hiện\\n\\nDeploy sample app\\n\\nBạn hãy thực hiện deploy một sample app, ví dụ:\\n\\nbash kubectl create deployment echo-server --image=mccutchen/go-httpbin kubectl expose deployment echo-server --name=clusterip --port=80 --target-port=8080 --type=ClusterIP\\n\\nCấu hình Issuer\\n\\nIssuer là thành phần giúp Cert-Manager giao tiếp với Let's Encrypt để cấp phát chứng chỉ.\"),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/expose-mot-service-thong-qua-vlb-layer4/README.md'}, page_content='Expose một service thông qua vLB Layer4\\n\\nĐiều kiện cần\\n\\nĐể có thể khởi tạo một Cluster và Deploy một Workload, bạn cần:\\n\\nCó ít nhất 1 VPC và 1 Subnet đang ở trạng thái ACTIVE. Nếu bạn chưa có VPC, Subnet nào, vui lòng khởi tạo VPC, Subnet theo hướng dẫn tại đây.\\n\\nCó ít nhất 1 SSH key đang ở trạng thái ACTIVE. Nếu bạn chưa có SSH key nào, vui lòng khởi tạo SSH key theo hướng dẫn tại đây.\\n\\nĐã cài đặt và cấu hình kubectl trên thiết bị của bạn. vui lòng tham khảo tại đây nếu bạn chưa rõ cách cài đặt và sử dụng kuberctl. Ngoài ra, bạn không nên sử dụng phiên bản kubectl quá cũ, chúng tôi khuyến cáo bạn nên sử dụng phiên bản kubectl sai lệch không quá một phiên bản với version của cluster.\\n\\nKhởi tạo Cluster\\n\\nCluster trong Kubernetes là một tập hợp gồm một hoặc nhiều máy ảo (VM) được kết nối lại với nhau để chạy các ứng dụng được đóng gói dạng container. Cluster cung cấp một môi trường thống nhất để triển khai, quản lý và vận hành các container trên quy mô lớn.\\n\\nĐể khởi tạo một Cluster, hãy làm theo các bước bên dưới:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn Activate.\\n\\nBước 3: Chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn. Sau khi Activate thành công, bạn hãy chọn Create a Cluster\\n\\nBước 4: Tại màn hình khởi tạo Cluster, chúng tôi đã thiết lập thông tin cho Cluster và một Default Node Group cho bạn, bạn có thể giữ các giá trị mặc định này hoặc điều chỉnh các thông số mong muốn cho Cluster và Node Group của bạn tại Cluster Configuration, Default Node Group Configuration, Plugin. Khi bạn chọn bật option , mặc định chúng tôi sẽ cài sẵn plugin này vào Cluster của bạn.\\n\\nBước 5: Chọn Create Kubernetes cluster. Hãy chờ vài phút để chúng tôi khởi tạo Cluster của bạn, trạng thái của Cluster lúc này là Creating.\\n\\nBước 6: Khi trạng thái Cluster là Active, bạn có thể xem thông tin Cluster, thông tin Node Group bằng cách chọn vào Cluster Name tại cột Name.\\n\\nKết nối và kiểm tra thông tin Cluster vừa tạo\\n\\nSau khi Cluster được khởi tạo thành công, bạn có thể thực hiện kết nối và kiểm tra thông tin Cluster vừa tạo theo các bước:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/k8s-cluster\\n\\nBước 2: Danh sách Cluster được hiển thị, chọn biểu tượng và chọn Download Config File để thực hiện tải xuống file kubeconfig. File này sẽ giúp bạn có toàn quyền truy cập vào Cluster của bạn.\\n\\nBước 3: Đổi tên file này thành config và lưu nó vào thư mục \\\\~/.kube/config\\n\\nBước 4: Thực hiện kiểm tra Cluster thông qua lệnh:\\n\\nChạy câu lệnh sau đây để kiểm tra node\\n\\nkubectl get nodes\\n\\nNếu kết quả trả về như bên dưới tức là bạn Cluster của bạn được khởi tạo thành công với 3 node như bên dưới.\\n\\nNAME STATUS ROLES AGE VERSION ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-834b7 Ready <none> 50m v1.28.8 ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-cf652 Ready <none> 23m v1.28.8 ng-0f4ed631-1252-49f7-8dfc-386fa0b2d29b-a8ef0 Ready <none> 28m v1.28.8\\n\\nKhởi tạo Service Account và cài đặt VNGCloud LoadBalancer Controller\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi bạn thực hiện khởi tạo Cluster theo hướng dẫn bên trên, nếu bạn chưa bật option Enable vLB Native Integration Driver, mặc định chúng tôi sẽ không cài sẵn plugin này vào Cluster của bạn. Bạn cần tự thực hiện Khởi tạo Service Account và cài đặt VNGCloud LoadBalancer Controller theo hướng dẫn bên dưới. Nếu bạn đã bật option Enable vLB Native Integration Driver, thì chúng tôi đã cài sẵn plugin này vào Cluster của bạn, hãy bỏ qua bước Khởi tạo Service Account, cài đặt VNGCloud LoadBalancer Controller và tiếp tục thực hiện theo hướng dẫn kể từ Deploy một Workload. {% endhint %}\\n\\nDeploy một Workload\\n\\nSau đây là hướng dẫn để bạn deploy service nginx trên Kubernetes.\\n\\nBước 1: Tạo Deployment, Service cho Nginx app.\\n\\nTạo file nginx-service-lb4.yaml với nội dung sau:\\n\\n``` apiVersion: apps/v1 kind: Deployment metadata: name: nginx-app spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.19.1 ports: - containerPort: 80\\n\\napiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app: nginx type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 80 ```\\n\\nDeploy Service này bằng lệch:\\n\\nkubectl apply -f nginx-service-lb4.yaml\\n\\nBước 2: Kiểm tra thông tin Deployment, Service vừa deploy\\n\\nChạy câu lệnh sau đây để kiểm tra Deployment\\n\\nkubectl get svc,deploy,pod -owide\\n\\nNếu kết quả trả về như bên dưới tức là bạn đã deploy Deployment thành công.\\n\\n``` NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1\\n\\nNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-app 0/1 1 0 2s nginx nginx:1.19.1 app=nginx\\n\\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-app-7f45b65946-bmrcf 0/1 ContainerCreating 0 2s\\n\\nLúc này, hệ thống vLB sẽ tự động tạo một LB tương ứng cho nginx app đã deployment, ví dụ:\\n\\nBước 3: Để truy cập vào app nginx vừa export, bạn có thể sử dụng URL với định dạng:\\n\\nhttp://Endpoint/\\n\\nBạn có thể lấy thông tin Public Endpoint của Load Balancer tại giao diện vLB. Cụ thể truy cập tại https://hcm-3.console.vngcloud.vn/vserver/load-balancer/vlb/\\n\\nVí dụ, bên dưới tôi đã truy cập thành công vào app nginx với địa chỉ : http://180.93.181.20/\\n\\nBạn có thể xem thêm về ALB tại Working with Network load balancing (NLB).\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nViệc thay đổi tên hoặc kích thước (Rename, Resize) tài nguyên Load Balancer trên vServer Portal có thể gây ra sự không tương thích với tài nguyên trên Kubernetes Cluster. Điều này có thể dẫn đến việc các tài nguyên không hoạt động trên Cluster, hoặc tài nguyên bị đồng bộ lại hoặc thông tin tài nguyên giữa vServer Portal và Cluster không khớp. Để ngăn chặn vấn đề này, hãy sử dụng kubectlđể quản lý tài nguyên của Cluster. {% endhint %}'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/expose-mot-service-thong-qua-vlb-layer4/preserve-source-ip-khi-su-dung-vlb-layer4-va-nginx-ingress-controller.md'}, page_content='Preserve Source IP khi sử dụng vLB Layer4 và Nginx Ingress Controller\\n\\nPreserve Source IP khi sử dụng vLB Layer 4 và Nginx Ingress Controller trong Kubernetes là quá trình duy trì địa chỉ IP gốc của client khi traffic được chuyển tiếp qua load balancer và vào cụm Kubernetes. Điều này rất quan trọng trong một số trường hợp khi bạn cần các thông tin chi tiết về kết nối của client, chẳng hạn như địa chỉ IP gốc và port gốc của client, để có thể thực hiện các quyết định xử lý traffic hoặc logging chính xác. Bên dưới là hướng dẫn cụ thể của chúng tôi để giúp bạn có thể thực hiện usecase này.\\n\\nĐiều kiện cần\\n\\nBạn đã thực hiện khởi tạo Cluster trên hệ thống VKS theo các hướng dẫn tại đây và trên cụm của bạn đã được cài đặt VNGCloud LoadBalancer Controller.\\n\\nTiếp theo, bạn cần thực hiện cài đặt nginx-ingress-controller theo lệnh:\\n\\nhelm install nginx-ingress-controller oci://ghcr.io/nginxinc/charts/nginx-ingress --namespace kube-system\\n\\nCấu hình ConfigMap cho Nginx Ingress Controller\\n\\nThêm vào ConfigMap của Nginx Ingress Controller các thiết lập để kích hoạt proxy protocol thông qua lệnh:\\n\\nkubectl edit cm -n kube-system nginx-ingress-controller\\n\\nNếu bạn không sử dụng cert-manager, bạn cần thêm đoạn code sau vào tệp tin ConfigMap:\\n\\ndata: proxy-protocol: \"True\" real-ip-header: proxy_protocol real-ip-recursive: \"True\" set-real-ip-from: 0.0.0.0/0\\n\\nNếu bạn có sử dụng cert-manager, bạn cần thêm đoạn code sau vào tệp tin ConfigMap:\\n\\ndata: proxy-protocol: \"True\" real-ip-header: proxy_protocol real-ip-recursive: \"True\" set-real-ip-from: 0.0.0.0/0 use-proxy-protocol: \"True\"\\n\\nCấu hình vLB Layer 4\\n\\nTiếp theo, bạn cần cấu hình vLB Layer4 cho phép sử dụng proxy protocol cho service Load Balancer Nginx. Giá trị truyền vào là danh sách các service name trong Load Balancer sử dụng Proxy Protocol.\\n\\nkubectl annotate service -n kube-system nginx-ingress-controller-controller \\\\ vks.vngcloud.vn/enable-proxy-protocol=\"http,https\"\\n\\nCuối cùng, bạn hãy thực hiện kiểm tra NLB trên vLB Portal cho tới khi các Load Balancer này được ACTIVE với đầy đủ listener, pool.\\n\\nCách sử dụng\\n\\nGiả sử, bạn có một service prometheus-node-exporter với port 9100 trong namespace default, bạn có thể apply yaml sau để có thể truy cập thông qua NLB\\n\\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-ingress namespace: default spec: ingressClassName: nginx rules: - host: kkk.example.com http: paths: - backend: service: name: prometheus-node-exporter port: number: 9100 path: /metrics pathType: Exact\\n\\nSau đó tôi sử dụng IP 103.245.252.75 để curl vào host kkk.example.com như sau:\\n\\nKết quả log ghi nhận được đã có thông tin Client IP này như hình:'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-public-cluster/README.md'}, page_content='Khởi tạo một Public Cluster\\n\\nModel\\n\\nKhi bạn khởi tạo một Public Cluster với Public Node Group, hệ thống VKS sẽ:\\n\\nTạo VM có Floating IP ( tức có IP Public). Lúc này các VM (Node) này có thể join trực tiếp vào cụm K8S thông qua Public IP này. Bằng cách sử dụng Public Cluster và Public Node Group, bạn có thể dễ dàng tạo các cụm Kubernetes và thực hiện expose service mà không cần sử dụng Load Balancer. Việc này sẽ góp phần tiết kiệm chi phí cho cụm của bạn.\\n\\nKhi bạn khởi tạo một Public Cluster với Private Node Group, hệ thống VKS sẽ:\\n\\nTạo VM không có Floating IP ( tức không có IP Public). Lúc này các VM (Node) này không thể join trực tiếp vào cụm K8S. Để các VM này có thể join vào cụm K8S, bạn cần phải sử dụng một NAT Gateway (NATGW). NATGW hoạt động như một trạm chuyển tiếp, cho phép các VM kết nối với cụm K8S mà không cần IP Public. Với VNG Cloud, chúng tôi khuyến cáo bạn sử dụng Pfsense hoặc Palo Alto như một NATGW cho Cluster của bạn. Pfsense sẽ giúp bạn quản lý lưu lượng mạng đến và đi (inbound và outbound traffic) một cách hiệu quả, đảm bảo an ninh mạng và quản lý truy cập. Bên cạnh đó, việc sử dụng Private Node Group sẽ giúp bạn kiểm soát các ứng dụng trong cụm được bảo mật hơn, cụ thể bạn có thể thực hiện giới hạn quyền truy cập control plane thông qua tính năng Whitelist IP.'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-public-cluster/khoi-tao-mot-public-cluster-voi-public-node-group.md'}, page_content='Khởi tạo một Public Cluster với Public Node Group\\n\\nĐiều kiện cần\\n\\nĐể có thể khởi tạo một Cluster và Deploy một Workload, bạn cần:\\n\\nCó ít nhất 1 VPC và 1 Subnet đang ở trạng thái ACTIVE. Nếu bạn chưa có VPC, Subnet nào, vui lòng khởi tạo VPC, Subnet theo hướng dẫn tại đây.\\n\\nCó ít nhất 1 SSH key đang ở trạng thái ACTIVE. Nếu bạn chưa có SSH key nào, vui lòng khởi tạo SSH key theo hướng dẫn tại đây.\\n\\nĐã cài đặt và cấu hình kubectl trên thiết bị của bạn. vui lòng tham khảo tại đây nếu bạn chưa rõ cách cài đặt và sử dụng kuberctl. Ngoài ra, bạn không nên sử dụng phiên bản kubectl quá cũ, chúng tôi khuyến cáo bạn nên sử dụng phiên bản kubectl sai lệch không quá một phiên bản với version của cluster.\\n\\nKhởi tạo Cluster\\n\\nCluster trong Kubernetes là một tập hợp gồm một hoặc nhiều máy ảo (VM) được kết nối lại với nhau để chạy các ứng dụng được đóng gói dạng container. Cluster cung cấp một môi trường thống nhất để triển khai, quản lý và vận hành các container trên quy mô lớn.\\n\\nĐể khởi tạo một Cluster, hãy làm theo các bước bên dưới:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn Activate.\\n\\nBước 3: Chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn. Sau khi Activate thành công, bạn hãy chọn Create a Cluster\\n\\nBước 4: Tại màn hình khởi tạo Cluster, chúng tôi đã thiết lập thông tin cho Cluster và một Default Node Group cho bạn, bạn có thể giữ các giá trị mặc định này hoặc điều chỉnh các thông số mong muốn cho Cluster và Node Group của bạn tại Cluster Configuration, Default Node Group Configuration, Plugin. Mặc định chúng tôi sẽ khởi tạo cho bạn một Public Cluster với Public Node Group.\\n\\nBước 5: Chọn Create Kubernetes cluster. Hãy chờ vài phút để chúng tôi khởi tạo Cluster của bạn, trạng thái của Cluster lúc này là Creating.\\n\\nBước 6: Khi trạng thái Cluster là Active, bạn có thể xem thông tin Cluster, thông tin Node Group bằng cách chọn vào Cluster Name tại cột Name.\\n\\nKết nối và kiểm tra thông tin Cluster vừa tạo\\n\\nSau khi Cluster được khởi tạo thành công, bạn có thể thực hiện kết nối và kiểm tra thông tin Cluster vừa tạo theo các bước:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/k8s-cluster\\n\\nBước 2: Danh sách Cluster được hiển thị, chọn biểu tượng và chọn Download Config File để thực hiện tải xuống file kubeconfig. File này sẽ giúp bạn có toàn quyền truy cập vào Cluster của bạn.\\n\\nBước 3: Đổi tên file này thành config và lưu nó vào thư mục \\\\~/.kube/config\\n\\nBước 4: Thực hiện kiểm tra Cluster thông qua lệnh:\\n\\nChạy câu lệnh sau đây để kiểm tra node\\n\\nkubectl get nodes\\n\\nNếu kết quả trả về như bên dưới tức là bạn Cluster của bạn được khởi tạo thành công với 3 node như bên dưới.\\n\\nNAME STATUS ROLES AGE VERSION ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-834b7 Ready <none> 50m v1.28.8 ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-cf652 Ready <none> 23m v1.28.8 ng-0f4ed631-1252-49f7-8dfc-386fa0b2d29b-a8ef0 Ready <none> 28m v1.28.8\\n\\nDeploy một Workload\\n\\nSau đây là hướng dẫn để bạn deploy service nginx trên Kubernetes.\\n\\nBước 1: Tạo Deployment và Service cho Nginx app\\n\\nTạo file nginx-service.yaml với nội dung sau:\\n\\n``` apiVersion: apps/v1 kind: Deployment metadata: name: nginx-app spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.19.1 ports: - containerPort: 80\\n\\napiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 80 ```\\n\\nDeploy Deployment này bằng lệch:\\n\\nkubectl apply -f nginx-service.yaml\\n\\nBước 2: Kiểm tra thông tin Deployment, Service trước khi expose ra Internet.\\n\\nChạy câu lệnh sau đây để kiểm tra Deployment\\n\\nkubectl get svc,deploy,pod -owide\\n\\nNếu kết quả trả về như bên dưới tức là bạn đã deploy service nginx thành công.\\n\\n``` NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1\\n\\nNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-app 1/1 1 1 74s nginx nginx:1.19.1 app=nginx\\n\\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-app-7f45b65946-5pcvz 1/1 Running 0 74s 172.16.24.201 ng-3f06013a-f6a5-47ba-a51f-bc5e9c2b10a7-ecea1\\n\\nExpose Nginx Service ra Internet\\n\\nChạy câu lệnh sau đây để expose nginx-service ra internet:\\n\\nkubectl expose deployment nginx-app --type=NodePort --port=30080 --target-port=80\\n\\nNếu kết quả trả về như bên dưới tức là bạn đã expose Service ra Internet thành công.\\n\\n``` NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1\\n\\nNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-app 1/1 1 1 2m43s nginx nginx:1.19.1 app=nginx\\n\\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-app-7f45b65946-5pcvz 1/1 Running 0 2m43s 172.16.24.201 ng-3f06013a-f6a5-47ba-a51f-bc5e9c2b10a7-ecea1\\n\\nĐể truy cập vào app nginx vừa export, bạn có thể sử dụng URL với định dạng:\\n\\nhttp://<node_ip>:31289/\\n\\nTrong đó node_ip có thể là địa chỉ node_port của bất kỳ node nào trong cluster. Bạn có thể lấy thông tin External IP của Node tại giao diện vServer. Cụ thể truy cập tại https://hcm-3.console.vngcloud.vn/vserver/v-server/cloud-server.\\n\\nVí dụ, bên dưới tôi đã truy cập thành công vào app nginx với địa chỉ : http://61.28.231.65:31007/\\n\\nNếu bạn muốn expose service này thông qua vLB Layer4, vLB Layer7, vui lòng tham khảo tại:\\n\\nExpose một service thông qua vLB Layer4\\n\\nExpose một service thông qua vLB Layer7'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-public-cluster/khoi-tao-mot-public-cluster-voi-private-node-group/README.md'}, page_content='Khởi tạo một Public Cluster với Private Node Group\\n\\nĐiều kiện cần\\n\\nĐể có thể khởi tạo một Cluster và Deploy một Workload, bạn cần:\\n\\nCó ít nhất 1 VPC và 1 Subnet đang ở trạng thái ACTIVE. Nếu bạn chưa có VPC, Subnet nào, vui lòng khởi tạo VPC, Subnet theo hướng dẫn tại đây.\\n\\nCó ít nhất 1 SSH key đang ở trạng thái ACTIVE. Nếu bạn chưa có SSH key nào, vui lòng khởi tạo SSH key theo hướng dẫn tại đây.\\n\\nĐã cài đặt và cấu hình kubectl trên thiết bị của bạn. vui lòng tham khảo tại đây nếu bạn chưa rõ cách cài đặt và sử dụng kuberctl. Ngoài ra, bạn không nên sử dụng phiên bản kubectl quá cũ, chúng tôi khuyến cáo bạn nên sử dụng phiên bản kubectl sai lệch không quá một phiên bản với version của cluster.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nĐể đảm bảo các VM trong các NodeGroup trên subnet có thể đi outbound ra internet và kết nối được tới Control Plane thì bạn bắt buộc phải thiết lập NAT Gateway. Cụ thể tham khảo thêm tại mục bên dưới. {% endhint %}\\n\\nKhởi tạo Palo Alto hoặc Pfsense thay thế cho NAT Gateway\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nĐể được hỗ trợ tốt nhất khi sử dụng Palo Alto hoặc Pfsense, vui lòng liên hệ với đội ngũ chuyên viên của chúng tôi qua Hotline 1900 1549 hoặc email support@vngcloud.vn. {% endhint %}\\n\\nHoặc bạn có thể chọn sử dụng Palo Alto hoặc Pfsense để làm việc với Private Node Group theo hướng dẫn tại:\\n\\nPalo Alto as a NAT Gateway\\n\\nPfsense as a NAT Gateway\\n\\nKhởi tạo Route Table\\n\\nSau khi Palo Alto, Pfsense được khởi tạo thành công, bạn cần tạo một Route table để kết nối tới các mạng khác nhau. Cụ thể thực hiện theo các bước sau để tạo Route table:\\n\\nBước 1: Truy cập vào https://hcm-3.console.vngcloud.vn/vserver/network/route-table\\n\\nBước 2: Tại thanh menu điều hướng, chọn Tab Network/ Route table.\\n\\nBước 3: Chọn Create Route table.\\n\\nBước 4: Nhập tên mô tả cho Route table. Tên Route table có thể bao gồm các chữ cái (a-z, A-Z, 0-9, \\'_\\', \\'-\\'). Độ dài dữ liệu đầu vào nằm trong khoảng từ 5 đến 50. Nó không được bao gồm khoảng trắng ở đầu hoặc ở cuối.\\n\\nBước 5: Chọn VPC cho Route table của bạn, nếu chưa có VPC cần tạo mới một VPC theo hướng dẫn tại Trang VPC. VPC sử dụng để thiết lập Route table phải là VPC được chọn sử dụng cho Palo Alto hoặc Pfsense và Cluster của bạn.\\n\\nBước 6: Chọn Create để tạo mới Route table.\\n\\nBước 7: Chọn tại Route table vừa tạo sau đó chọn Edit Routes.\\n\\nBước 8: Tại phần thêm mới Route hãy nhập vào các thông tin:\\n\\nĐối với Destination, hãy nhập Destination CIDR là 0.0.0.0/0\\n\\nĐối với Target, hãy nhập Target CIDR là địa chỉ IP Network Interface của Palo Alto hoặc Pfsense tương ứng.\\n\\nKhởi tạo Cluster\\n\\nCluster trong Kubernetes là một tập hợp gồm một hoặc nhiều máy ảo (VM) được kết nối lại với nhau để chạy các ứng dụng được đóng gói dạng container. Cluster cung cấp một môi trường thống nhất để triển khai, quản lý và vận hành các container trên quy mô lớn.\\n\\nĐể khởi tạo một Cluster, hãy làm theo các bước bên dưới:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn Activate.\\n\\nBước 3: Chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn. Sau khi Activate thành công, bạn hãy chọn Create a Cluster\\n\\nBước 4: Tại màn hình khởi tạo Cluster, chúng tôi đã thiết lập thông tin cho Cluster và một Default Node Group cho bạn, bạn có thể giữ các giá trị mặc định này hoặc điều chỉnh các thông số mong muốn cho Cluster và Node Group của bạn tại Cluster Configuration, Default Node Group Configuration, Plugin. Mặc định chúng tôi sẽ khởi tạo cho bạn một Public Cluster với Public Node Group. Bạn cần thay đổi lựa chọn của bạn thành Private Node Group.\\n\\nBước 5: Chọn Create Kubernetes cluster. Hãy chờ vài phút để chúng tôi khởi tạo Cluster của bạn, trạng thái của Cluster lúc này là Creating.\\n\\nBước 6: Khi trạng thái Cluster là Active, bạn có thể xem thông tin Cluster, thông tin Node Group bằng cách chọn vào Cluster Name tại cột Name.\\n\\nKết nối và kiểm tra thông tin Cluster vừa tạo\\n\\nSau khi Cluster được khởi tạo thành công, bạn có thể thực hiện kết nối và kiểm tra thông tin Cluster vừa tạo theo các bước:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/k8s-cluster\\n\\nBước 2: Danh sách Cluster được hiển thị, chọn biểu tượng và chọn Download config file để thực hiện tải xuống file kubeconfig. File này sẽ giúp bạn có toàn quyền truy cập vào Cluster của bạn.\\n\\nBước 3: Đổi tên file này thành config và lưu nó vào thư mục \\\\~/.kube/config\\n\\nBước 4: Thực hiện kiểm tra Cluster thông qua lệnh:\\n\\nChạy câu lệnh sau đây để kiểm tra node\\n\\nkubectl get nodes\\n\\nNếu kết quả trả về như bên dưới tức là bạn Cluster của bạn được khởi tạo thành công với 3 node như bên dưới.\\n\\nNAME STATUS ROLES AGE VERSION ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-834b7 Ready <none> 50m v1.28.8 ng-0e10592c-e70e-404d-a4e8-5e3b80f805e4-cf652 Ready <none> 23m v1.28.8 ng-0f4ed631-1252-49f7-8dfc-386fa0b2d29b-a8ef0 Ready <none> 28m v1.28.8\\n\\nDeploy một Workload\\n\\nSau đây là hướng dẫn để bạn deploy service nginx trên Kubernetes.\\n\\nBước 1: Tạo Deployment cho Nginx app.\\n\\nTạo file nginx-service-lb4.yaml với nội dung sau:\\n\\n``` apiVersion: apps/v1 kind: Deployment metadata: name: nginx-app spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.19.1 ports: - containerPort: 80\\n\\napiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app: nginx type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 80 ```\\n\\nDeploy Deployment này bằng lệch:\\n\\nkubectl apply -f nginx-service-lb4.yaml\\n\\nBước 2: Kiểm tra thông tin Deployment, Service trước khi expose ra Internet.\\n\\nChạy câu lệnh sau đây để kiểm tra Deployment\\n\\nkubectl get svc,deploy,pod -owide\\n\\nNếu kết quả trả về như bên dưới tức là bạn đã deploy service nginx thành công.\\n\\n``` NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1\\n\\nNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-app 1/1 1 1 2m16s nginx nginx:1.19.1 app=nginx\\n\\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-app-7f45b65946-t7d7k 1/1 Running 0 2m16s 172.16.24.202 ng-3f06013a-f6a5-47ba-a51f-bc5e9c2b10a7-ecea1\\n\\nBước 3: Để truy cập vào app nginx vừa export, bạn có thể sử dụng URL với định dạng:\\n\\nhttp://Endpoint/\\n\\nBạn có thể lấy thông tin Public Endpoint của Load Balancer tại giao diện vLB. Cụ thể truy cập tại https://hcm-3.console.vngcloud.vn/vserver/load-balancer/vlb/\\n\\nVí dụ, bên dưới tôi đã truy cập thành công vào app nginx với địa chỉ : http://180.93.181.20/'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-public-cluster/khoi-tao-mot-public-cluster-voi-private-node-group/pfsense-as-a-nat-gateway.md'}, page_content='Pfsense as a NAT Gateway\\n\\nSử dụng hướng dẫn bên dưới dể làm việc với Private Node group thông qua Pfsense\\n\\nĐiều kiện cần\\n\\nĐể có thể sử dụng Pfsense làm NAT Gateway cho Cluster trên hệ thống VKS, bạn cần có:\\n\\nMột server (VM) Pfsense được khởi tạo trên hệ thống vMarketPlace theo hướng dẫn bên dưới với cấu hình như sau:\\n\\nItem Cấu hình Flavor 2x4 Volume 80 GB VPC 10.3.0.0/16 Network Interface 1 10.3.0.3\\n\\nKhởi tạo Pfsense\\n\\nBước 1: Truy cập vào https://marketplace.console.vngcloud.vn/\\n\\nBước 2: Tại màn hình chính, thực hiện tìm kiếm Pfsense, tại dịch vụ Pfsense, chọn Launch.\\n\\nBước 3: Lúc này, bạn cần thiết lập cấu hình cho Pfsense. Cụ thể, bạn có thể chọn Volume, IOPS, Network, Security Group mong muốn. Bạn cần lựa chọn VPC và Subnet giống với VPC và Subnet mà bạn lựa chọn sử dụng cho Cluster của bạn. Ngoài ra bạn cũng cần chọn Một Server Group đã tồn tại hoặc chọn Dedicated SOFT ANTI AFFINITY group để chúng tôi tự động tạo một server group mới.\\n\\nBước 4: Tiến hành thanh toán như các tài nguyên bình thường trên VNG Cloud.\\n\\nCấu hình thông số cho Pfsense\\n\\nBước 1: Sau khi khởi tạo Pfsense từ vMarketPlace theo hướng dẫn bên trên, bạn có thể truy cập vào giao diện vServer tại đây để kiểm tra server chạy Pfsense đã được khởi tạo xong chưa. Tiếp theo, bạn mở rule Any trên Security Group cho server Pfsense vừa tạo. Việc mở rule Any trên Security Group sẽ cho phép tất cả lưu lượng truy cập đến server Pfsense.\\n\\nBước 2: Sau khi server chạy Pfsense được khởi tạo thành công. Để vào GUI của Pfsense, bạn cần sử dụng địa chỉ IP của External Interface đăng nhập với Tên đăng nhập và mật khẩu mặc định là admin/pfsense.\\n\\nĐể lấy thông tin IP này, bạn vào phần Network Interface của Pfsense để xem thông tin\\n\\nBước 3: Mở rule trên firewall\\n\\nTiến hành Add rule\\n\\nBạn có thể mở rule như bên dưới để truy cập vào GUI bằng External Interface.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nBạn nên giới hạn lại Range IP được phép kết nối tới GUI Pfsense để hạn chế user được phép truy cập vào GUI Pfsense {% endhint %}\\n\\nChọn Save\\n\\nSau đó chọn Apply change\\n\\nBước 4: Tiến hành General Setup, bạn vui lòng thực hiện như bên dưới\\n\\nCấu hình cho WAN Interface\\n\\nThay đổi password vào GUI\\n\\nTiến hành reload\\n\\nĐã hoàn thành General Setup\\n\\nBước 5: Cấu hình Interface LAN\\n\\nVào phần Interfaces -> Assignments để gắn thêm Interface LAN\\n\\nNhấn vào Add\\n\\nSau đó nhấn vào Save\\n\\nVào phần Interfaces -> Assignments để tiến hành enable LAN Interface\\n\\nBạn thực hiện cấu hình như bên dưới\\n\\nCấu hình IP cho LAN\\n\\nSau đó tiến hành Add a new gateway: tiến hành nhập Gateway cho LAN Interface\\n\\nĐể lấy thông tin IP này, bạn vào mục Network Interface của server Pfsense để xem thông tin:\\n\\nTiến hành Save lại\\n\\nBước 6: Xem lại thông tin cấu hình\\n\\nBước 7: Mở rule đi ra Internet cho interface LAN\\n\\nTại source bạn chọn dải IP được phép đi ra Internet\\n\\nBước 8: Cấu hình NAT để các vServer có thể đi ra được Internet\\n\\nVào mục Firewall -> NAT\\n\\nChọn mode NAT sau đó tiến hành cấu hình NAT\\n\\nNhấn vào Add để thêm rule\\n\\nChọn source, destination NAT\\n\\nKhởi tạo Route Table\\n\\nSau khi Pfsense được khởi tạo và cấu hình thành công, bạn cần tạo một Route table để kết nối tới các mạng khác nhau. Cụ thể thực hiện theo các bước sau để tạo Route table:\\n\\nBước 1: Truy cập vào https://hcm-3.console.vngcloud.vn/vserver/network/route-table\\n\\nBước 2: Tại thanh menu điều hướng, chọn Tab Network/ Route table.\\n\\nBước 3: Chọn Create Route table.\\n\\nBước 4: Nhập tên mô tả cho Route table. Tên Route table có thể bao gồm các chữ cái (a-z, A-Z, 0-9, \\'_\\', \\'-\\'). Độ dài dữ liệu đầu vào nằm trong khoảng từ 5 đến 50. Nó không được bao gồm khoảng trắng ở đầu hoặc ở cuối.\\n\\nBước 5: Chọn VPC cho Route table của bạn, nếu chưa có VPC cần tạo mới một VPC theo hướng dẫn tại Trang VPC. VPC sử dụng để thiết lập Route table phải là VPC được chọn sử dụng cho Pfsense và Cluster của bạn.\\n\\nBước 6: Chọn Create để tạo mới Route table.\\n\\nBước 7: Chọn tại Route table vừa tạo sau đó chọn Edit Routes.\\n\\nBước 8: Tại phần thêm mới Route hãy nhập vào các thông tin:\\n\\nĐối với Destination, hãy nhập Destination CIDR là 0.0.0.0/0\\n\\nĐối với Target, hãy nhập Target CIDR là địa chỉ IP Network Interface 2 của Pfsense.\\n\\nVí dụ:\\n\\nKiểm tra kết nối\\n\\nTiến hành ping google.com hoặc 8.8.8.8 để kiểm tra\\n\\nTrước khi Enable NAT server không ra được internet\\n\\nSau khi cấu hình NAT tiến hành ping 8.8.8.8 để kiểm tra'),\n",
       " Document(metadata={'source': './../../data/vks/bat-dau-voi-vks/khoi-tao-mot-public-cluster/khoi-tao-mot-public-cluster-voi-private-node-group/palo-alto-as-a-nat-gateway.md'}, page_content=\"Palo Alto as a NAT Gateway\\n\\nSử dụng hướng dẫn bên dưới dể làm việc với Private Node group thông qua Palo Alto.\\n\\nĐiều kiện cần\\n\\nĐể có thể sử dụng Palo Alto làm NAT Gateway cho Cluster trên hệ thống VKS, bạn cần có:\\n\\nMột server (VM) Windows đã được khởi tạo trên hệ thống vServer với cấu hình như sau:\\n\\nItem Cấu hình Flavor 2x4 Volume 20 GB VPC 10.76.0.0/16 Subnet 10.76.0.4/24 Network Interface 1 10.76.0.3\\n\\nMột server (VM) Palo Alto được khởi tạo trên hệ thống vMarketPlace theo hướng dẫn bên dưới với cấu hình như sau:\\n\\nItem Cấu hình Flavor 2x8 Volume 60 GB VPC 10.76.0.0/16 Network Interface 1 10.76.255.4 Network Interface 2 10.76.0.4\\n\\nKhởi tạo Palo Alto\\n\\nBước 1: Truy cập vào https://marketplace.console.vngcloud.vn/\\n\\nBước 2: Tại màn hình chính, thực hiện tìm kiếm Palo Alto, tại dịch vụ Palo Alto, chọn Launch.\\n\\nBước 3: Lúc này, bạn cần thiết lập cấu hình cho Palo Alto. Cụ thể, bạn có thể chọn Volume, IOPS, Network, Security Group mong muốn. Bạn cần lựa chọn VPC và Subnet giống với VPC và Subnet mà bạn lựa chọn sử dụng cho Cluster của bạn. Ngoài ra bạn cũng cần chọn Một Server Group đã tồn tại hoặc chọn Dedicated SOFT ANTI AFFINITY group để chúng tôi tự động tạo một server group mới.\\n\\nBước 4: Tiến hành thanh toán như các tài nguyên bình thường trên VNG Cloud.\\n\\nCấu hình thông số cho Palo Alto\\n\\nBước 1: Sau khi khởi tạo Palo Alto từ vMarketPlace theo hướng dẫn bên trên, bạn có thể truy cập vào giao diện vServer tại đây để kiểm tra server chạy Palo Alto đã được khởi tạo xong chưa. Tiếp theo, bạn mở rule Any trên Security Group cho server Palo Alto vừa tạo. Việc mở rule Any trên Security Group sẽ cho phép tất cả lưu lượng truy cập đến server Palo Alto.\\n\\nBước 2: Sau khi server chạy Palo Alto được khởi tạo thành công. Để vào GUI của Palo Alto bạn cần có 1 vServer chạy Windows. Sau đó bạn truy cập vào bằng IP Internal Interface với tên đăng nhập và mật khẩu mặc định là: admin/admin\\n\\nLưu ý: Về phần Network của vServer Windows để truy cập vào GUI của Palo Alto. Bạn cần tạo cùng VPC và sử dụng subnet khác với subnet có priority là 1 khi khởi tạo Palo Alto\\n\\nBước 3: Sau khi đăng nhập xong, bạn cần thực hiện thay đổi mật khẩu lần đầu. Hãy nhập mật khẩu mới theo mong muốn của bạn.\\n\\nBước 4: Bạn cần tiến hành khởi tạo 1 Zone Inside và 1 Zone Outside theo hướng dẫn bên dưới:\\n\\nChọn bút Add\\n\\nĐặt tên cho Zone: Inside sau đó chọn OK\\n\\nLàm tương tự đối với Zone Outside\\n\\nBước 5: Cấu hình cho External Interface\\n\\nInterface Type: Layer 3\\n\\nVirtual Router: default\\n\\nSecurity Zone: Outside\\n\\nChuyển sang Tab IPv4 và chọn Add để nhập Static IP cho External Interface\\n\\nĐể lấy thông tin IP này bạn vào phần Network Interface của Palo Alto để xem thông tin\\n\\nChuyển sang tab Advanced, ở phần MTU bạn cần chỉnh thành 1400\\n\\nBước 6: Thực hiện cấu hình tương tự cho các Internal Interface\\n\\nTại tab IPv4: bạn tiến hành thiết lập Static IP\\n\\nChuyển sang tab Advanced, ở phần MTU bạn chỉnh thành 1400\\n\\nBước 7: Tạo static route\\n\\nVào phần Network -> Virtual Routers-> Chọn default-> Chuyển sang mục Static Routes\\n\\nThực hiện tạo 1 route như hình bên dưới\\n\\nBước 8: Tạo Security Policy Rule\\n\\nVào phần Policies -> Security ->Add\\n\\nTại tab General, bạn cần đặt tên cho rule\\n\\nTại tab Source, thiết lập các thông tin như Source Zone, Source Address, Source User, Source Device\\n\\nTại tab Destination, thiết lập các thông tin như Destination Zone, Destination Address, Destination Device\\n\\nTại tab Application, thiết lập các thông tin như Application, Depend On\\n\\nTại tab Service/URL Category, thiết lập các thông tin như Service, URL Category\\n\\nTại tab Actions, thiết lập các thông tin như Action, Log, Profile, Other Settings\\n\\nBước 9: Tạo rule NAT để các VM có thể đi ra Internet\\n\\nVào phần Policies -> NAT -> Add\\n\\nTại tab General đặt tên cho NAT rule\\n\\nTại tab Original Packet chọn Source Zone, Destination Zone, Destination Interface, Service, Source Address, Destination Address\\n\\nTạo tab Translated Packet thực hiện cấu hình như hình bên dưới\\n\\nLưu ý: Cần thay đổi IP Address thành địa chỉ Static IP mà bạn đã cấu hình ở bước 6\\n\\nBước 10: Tiến hành Commit\\n\\nKhởi tạo Route Table\\n\\nSau khi Palo Alto được khởi tạo và cấu hình thành công, bạn cần tạo một Route table để kết nối tới các mạng khác nhau. Cụ thể thực hiện theo các bước sau để tạo Route table:\\n\\nBước 1: Truy cập vào https://hcm-3.console.vngcloud.vn/vserver/network/route-table\\n\\nBước 2: Tại thanh menu điều hướng, chọn Tab Network/ Route table.\\n\\nBước 3: Chọn Create Route table.\\n\\nBước 4: Nhập tên mô tả cho Route table. Tên Route table có thể bao gồm các chữ cái (a-z, A-Z, 0-9, '_', '-'). Độ dài dữ liệu đầu vào nằm trong khoảng từ 5 đến 50. Nó không được bao gồm khoảng trắng ở đầu hoặc ở cuối.\\n\\nBước 5: Chọn VPC cho Route table của bạn, nếu chưa có VPC cần tạo mới một VPC theo hướng dẫn tại Trang VPC. VPC sử dụng để thiết lập Route table phải là VPC được chọn sử dụng cho Palo Alto và Cluster của bạn.\\n\\nBước 6: Chọn Create để tạo mới Route table.\\n\\nBước 7: Chọn tại Route table vừa tạo sau đó chọn Edit Routes.\\n\\nBước 8: Tại phần thêm mới Route hãy nhập vào các thông tin:\\n\\nĐối với Destination, hãy nhập Destination CIDR là 0.0.0.0/0\\n\\nĐối với Target, hãy nhập Target CIDR là địa chỉ IP Network Interface 2 của Palo Alto.\\n\\nVí dụ:\\n\\nKiểm tra kết nối\\n\\nTiến hành ping 8.8.8.8 hoặc google.com\"),\n",
       " Document(metadata={'source': './../../data/vks/network/README.md'}, page_content='Network'),\n",
       " Document(metadata={'source': './../../data/vks/network/lam-viec-voi-network-load-balancing-nlb/README.md'}, page_content='Làm việc với Network load balancing (NLB)\\n\\nNLB là gì?\\n\\nNetwork Load Balancer (NLB) là một bộ cân bằng tải được cung cấp bởi VNGCloud giúp phân phối lưu lượng truy cập mạng đến nhiều máy chủ back-end (backend servers) trong một nhóm máy tính (instance group). NLB hoạt động ở layer 4 của mô hình OSI, giúp cân bằng tải dựa trên địa chỉ IP và cổng TCP/UDP. Để biết thêm thông tin chi tiết về NLB, vui lòng tham khảo tại [How it works (NLB)]\\n\\nMô hình triển khai\\n\\nVNGCloud LoadBalancer Controller: VNGCloud LoadBalancer Controller là một bộ điều khiển chạy trên các cụm Kubernetes được triển khai trên VNG Cloud. Nó chịu trách nhiệm cho việc quản lý các tài nguyên VNG Cloud cho các cụm Kubernetes, bao gồm:\\n\\nTạo và quản lý Network Load Balancer (NLB) cho các Service Kubernetes có service type = Load Balancer.'),\n",
       " Document(metadata={'source': './../../data/vks/network/lam-viec-voi-network-load-balancing-nlb/integrate-with-network-load-balancer.md'}, page_content='Integrate with Network Load Balancer\\n\\nĐể tích hợp một Network Load Balancer với một Kubernetes cluster, bạn có thể sử dụng một Service với type là LoadBalancer. Khi bạn tạo một Service như vậy, VNGCloud LoadBalancer Controller sẽ tự động một NLB để chuyển tiếp lưu lượng đến các pod trên node của bạn. Bạn cũng có thể sử dụng các annotation để tùy chỉnh các thuộc tính của Network Load Balancer, như port, protocol,...\\n\\nChuẩn bị\\n\\nTạo một Kubernetes cluster trên VNGCloud, hoặc sử dụng một cluster đã có. Lưu ý: đảm bảo bạn đã tải xuống cluster configuration file sau khi cluster được khởi tạo thành công và truy cập vào cluster của bạn.\\n\\nKhởi tạo hoặc sử dụng một service account đã tạo trên IAM và gắn policy: vLBFullAccess, vServerFullAccess. Để tạo service account bạn truy cập tại đây và thực hiện theo các bước sau:\\n\\nChọn \"Create a Service Account\", điền tên cho Service Account và nhấn Next Step để gắn quyền cho Service Account\\n\\nTìm và chọn Policy: vLBFullAccess và Policy: vServerFullAccess, sau đó nhấn \"Create a Service Account\" để tạo Service Account, Policy: vLBFullAccess vàPolicy: vServerFullAccess do VNG Cloud tạo ra, bạn không thể xóa các policy này.\\n\\nSau khi tạo thành công bạn cần phải lưu lại Client_ID và Secret_Key của Service Account để thực hiện bước tiếp theo.\\n\\nKhởi tạo Service Account và cài đặt VNGCloud LoadBalancer Controller\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi bạn thực hiện khởi tạo Cluster theo hướng dẫn bên trên, nếu bạn chưa bật option Enable vLB Native Integration Driver, mặc định chúng tôi sẽ không cài sẵn plugin này vào Cluster của bạn. Bạn cần tự thực hiện Khởi tạo Service Account và cài đặt VNGCloud LoadBalancer Controller theo hướng dẫn bên dưới. Nếu bạn đã bật option Enable vLB Native Integration Driver, thì chúng tôi đã cài sẵn plugin này vào Cluster của bạn, hãy bỏ qua bước Khởi tạo Service Account, cài đặt VNGCloud LoadBalancer Controller và tiếp tục thực hiện theo hướng dẫn kể từ Deploy một Workload. {% endhint %}\\n\\nDeploy một Workload\\n\\n1.Nếu bạn chưa có sẵn một Network Load Balancer đã khởi tạo trước đó trên hệ thống vLB.\\n\\nLúc này, bạn cần thực hiện:\\n\\nBước 1: Tạo Deployment, Service cho Nginx app.\\n\\nTạo file nginx-service-lb4.yaml với nội dung sau:\\n\\n``` apiVersion: apps/v1 kind: Deployment metadata: name: nginx-app spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.19.1 ports: - containerPort: 80\\n\\napiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app: nginx type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 80 ```\\n\\nHoặc sử dụng file mấu sau đây để deploy HTTP Apache Service với Internal LoadBalancer cho phép truy cập nội bộ trên cổng 8080:\\n\\n``` apiVersion: apps/v1 kind: Deployment metadata: name: internal-http-apache2-deployment spec: replicas: 2 selector: matchLabels: app: apache2 template: metadata: labels: app: apache2 spec: containers: - name: apache2 image: httpd ports: - containerPort: 80\\n\\napiVersion: v1 kind: Service metadata: name: internal-http-apache2-service annotations: vks.vngcloud.vn/scheme: \"internal\" # MUST set like this to create an internal loadbalancer spec: selector: app: apache2 type: LoadBalancer # MUST set like this to create an internal loadbalancer ports: - name: http protocol: TCP port: 8080 # CAN be accessed via this port with other service in the same VPC targetPort: 80 ```\\n\\nHoặc tập tin YAML mẫu để tạo Deployment và Service cho ứng dụng máy chủ UDP trong một cụm Kubernetes:\\n\\n``` apiVersion: apps/v1 kind: Deployment metadata: name: udp-server-deployment spec: selector: matchLabels: name: udp-server replicas: 5 template: metadata: labels: name: udp-server spec: containers: - name: udp-server image: vcr.vngcloud.vn/udp-server imagePullPolicy: Always ports: - containerPort: 10001 protocol: UDP\\n\\napiVersion: v1 kind: Service metadata: name: udp-server-service annotations: vks.vngcloud.vn/pool-algorithm: \"source-ip\" labels: app: udp-server spec: type: LoadBalancer sessionAffinity: ClientIP ports: - port: 10001 protocol: UDP selector: name: udp-server\\n\\n```\\n\\n2.Nếu bạn đã có sẵn một Network Load Balancer đã khởi tạo trước đó trên hệ thống vLB và bạn muốn tái sử dụng NLB cho cluster của bạn.\\n\\nLúc này, bạn hãy nhập thông tin Load Balancer ID vào annotation vks.vngcloud.vn/load-balancer-id. Ví dụ dưới đây là tập tin YAML mẫu để triển khai Nginx với External LoadBalancer sử dụng vngcloud-controller-manager để tự động expose dịch vụ tới internet bằng bộ cân bằng tải L4 sử dụng một NLB có sẵn với ID = lb-2b9d8974-3760-4d60-8203-9671f229fb96\\n\\n``` apiVersion: apps/v1 kind: Deployment metadata: name: external-http-nginx-deployment spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80\\n\\nkind: Service apiVersion: v1 metadata: name: external-http-nginx-service annotations: vks.vngcloud.vn/package-id: \"lbp-ddbf9313-3f4c-471b-afd5-f6a3305159fc\" # ID of the load balancer package vks.vngcloud.vn/load-balancer-id: \"lb-2b9d8974-3760-4d60-8203-9671f229fb96\" spec: selector: app: nginx type: LoadBalancer ports: - name: http port: 80 targetPort: 80 ```\\n\\n3.Sau khi một NLB mới đã được chúng tôi tự động khởi tạo, lúc này bạn có thể thực hiện\\n\\nChỉnh sửa cấu hình NLB của bạn theo hướng dẫn cụ thể tại Configure for a Network Load Balancer. Ví dụ như bên dưới, tôi đã thực hiện chỉnh sửa protocol và port như sau:\\n\\n``` apiVersion: apps/v1 kind: Deployment metadata: name: http-apache2-deployment spec: replicas: 2 selector: matchLabels: app: apache2 template: metadata: labels: app: apache2 spec: containers: - name: apache2 image: httpd ports: - containerPort: 80\\n\\napiVersion: v1 kind: Service metadata: name: http-apache2-service annotations: vks.vngcloud.vn/load-balancer-id: \"lb-f8c0d85b-cb0c-4c77-b382-37982c4d98af\" spec: selector: app: apache2 type: LoadBalancer ports: - name: http protocol: TCP port: 8000 targetPort: 80 ```\\n\\nCũng giống như các tài nguyên Kubernetes khác, vngcloud-controller-manager có cấu trúc gồm các trường thông tin như sau:\\n\\napiVersion: Phiên bản API cho Ingress.\\n\\nkind: Loại tài nguyên, trong trường hợp này là \"Service\".\\n\\nmetadata: Thông tin mô tả Ingress, bao gồm tên, annotations.\\n\\nspec: Cấu hình điều kiện của các incoming request.\\n\\nĐể biết thông tin chung về cách làm việc với vngcloud-controller-manager,, hãy xem tại [Configure for a Network Load Balancer]\\n\\nDeploy Service này bằng lệch:\\n\\nkubectl apply -f nginx-service-lb4.yaml\\n\\nKiểm tra thông tin Deployment, Service vừa deploy\\n\\nChạy câu lệnh sau đây để kiểm tra Deployment\\n\\nkubectl get svc,deploy,pod -owide\\n\\nNếu kết quả trả về như bên dưới tức là bạn đã deploy Deployment thành công.\\n\\n``` NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1\\n\\nNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-app 0/1 1 0 2s nginx nginx:1.19.1 app=nginx\\n\\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-app-7f45b65946-bmrcf 0/1 ContainerCreating 0 2s\\n\\nLúc này, hệ thống vLB sẽ tự động tạo một LB tương ứng cho nginx app đã deployment, ví dụ:\\n\\nBước 3: Để truy cập vào app nginx vừa export, bạn có thể sử dụng URL với định dạng:\\n\\nhttp://Endpoint/\\n\\nBạn có thể lấy thông tin Public Endpoint của Load Balancer tại giao diện vLB. Cụ thể truy cập tại https://hcm-3.console.vngcloud.vn/vserver/load-balancer/vlb/\\n\\nVí dụ, bên dưới tôi đã truy cập thành công vào app nginx với địa chỉ : http://180.93.181.20/'),\n",
       " Document(metadata={'source': './../../data/vks/network/lam-viec-voi-network-load-balancing-nlb/cau-hinh-cho-mot-network-load-balancer.md'}, page_content='Cấu hình cho một Network Load Balancer\\n\\nTại trang [Integrate with Network Load Balancer], chúng tôi đã hướng dẫn bạn cách thực hiện cài đặt VNGCloud LoadBalancer Controller, tạo và apply yaml file. Sau đây là chi tiết các ý nghĩa các thông tin bạn có thể thiết lập trong yaml file:\\n\\nAnnotation\\n\\nSử dụng các annotation dưới đây để tuỳ chỉnh Load Balancer phù hợp với nhu cầu của bạn:\\n\\nAnnotation Bắt buộc/ Không bắt buộc Ý nghĩa vks.vngcloud.vn/load-balancer-id Không bắt buộc Nếu bạn chưa có sẵn một Network Load Balancer đã khởi tạo trước đó trên hệ thống vLB. Chúng tôi sẽ tự động tạo 1 NLB trên cluster của bạn. NLB này sẽ hiển thị trên vLB Portal, chi tiết truy cập tại đây Nếu bạn đã có sẵn một Network Load Balancer đã khởi tạo trước đó trên hệ thống vLB và bạn muốn tái sử dụng NLB cho cluster của bạn. Lúc này, bạn hãy nhập thông tin Load Balancer ID vào annotation này. vks.vngcloud.vn/load-balancer-name Không bắt buộc Annotation vks.vngcloud.vn/load-balancer-name sẽ được sử dụng nếu bạn không sử dụng annotation load-balancer-id . Annotation vks.vngcloud.vn/load-balancer-name chỉ có ý nghĩa khi bạn tạo mới một load balancer. Sau khi load balancer được tạo thành công, annotation này sẽ tự động bị xóa . Việc sử dụng annotation này sau khi load balancer được tạo sẽ không có tác dụng . Khi bạn sử dụng annotation này, nếu bạn chưa có sẵn một Network Load Balancer đã khởi tạo trước đó trên hệ thống vLB. Chúng tôi sẽ tự động tạo 1 NLB trên cluster của bạn. ALB này sẽ hiển thị trên vLB Portal, chi tiết truy cập tại đây Nếu bạn đã có sẵn một Network Load Balancer đã khởi tạo trước đó trên hệ thống vLB và bạn muốn tái sử dụng NLB cho cluster của bạn. Lúc này, bạn hãy nhập thông tin Load Balancer Name vào annotation này. vks.vngcloud.vn/package-id Không bắt buộc Nếu bạn không nhập thông tin này thì mặc định chúng tôi sẽ sử dụng cấu hình NLB Small. Nếu bạn đã có sẵn host vLB đang ACTIVE và bạn muốn tích hợp host này vô cụm K8S của bạn, vui lòng bỏ qua trường thông tin này. vks.vngcloud.vn/tags Không bắt buộc Tag được gắn thêm cho NLB của bạn. vks.vngcloud.vn/scheme Không bắt buộc Mặc định internet-facing , bạn có thể đổi thành internal tùy theo nhu cầu sử dụng. vks.vngcloud.vn/security-groups Không bắt buộc Mặc định sẽ tạo một security group default theo Cluster của bạn. vks.vngcloud.vn/inbound-cidrs Không bắt buộc Mặc định All CIRD: 0.0.0.0/0 vks.vngcloud.vn/healthy-threshold-count Không bắt buộc Mặc định 3 vks.vngcloud.vn/unhealthy-threshold-count Không bắt buộc Mặc định 3 vks.vngcloud.vn/healthcheck-interval-seconds Không bắt buộc Mặc định 30 vks.vngcloud.vn/healthcheck-timeout-seconds Không bắt buộc Mặc định 5 vks.vngcloud.vn/healthcheck-protocol Không bắt buộc Mặc định TCP . Người dùng có thể chọn một trong các giá trị TCP/ HTTP/ HTTPS/ PING-UDP vks.vngcloud.vn/healthcheck-http-method Không bắt buộc Mặc định GET . Người dùng có thể chọn một trong các giá trị GET / POST / PUT vks.vngcloud.vn/healthcheck-path Không bắt buộc Mặc định / vks.vngcloud.vn/healthcheck-http-version Không bắt buộc Mặc định 1.0 . Người dùng có thể chọn một trong các giá trị 1.0, 1.1 vks.vngcloud.vn/healthcheck-http-domain-name Không bắt buộc Mặc định trống vks.vngcloud.vn/healthcheck-port Không bắt buộc Mặc định traffic port vks.vngcloud.vn/success-codes Không bắt buộc Mặc định 200 vks.vngcloud.vn/idle-timeout-client Không bắt buộc Mặc định 50 vks.vngcloud.vn/idle-timeout-member Không bắt buộc Mặc định 50 vks.vngcloud.vn/idle-timeout-connection Không bắt buộc Mặc định 5 vks.vngcloud.vn/pool-algorithm Không bắt buộc Mặc định ROUND_ROBIN . Người dùng có thể chọn một trong các giá trị ROUND_ROBIN / LEAST_CONNECTIONS / SOURCE_IP vks.vngcloud.vn/target-node-labels Không bắt buộc Mặc định trống vks.vngcloud.vn/enable-proxy-protocol Không bắt buộc Mặc định trống. Người dùng chỉ định danh sách các service name trong Load Balancer mà Proxy Protocol sẽ được áp dụng.'),\n",
       " Document(metadata={'source': './../../data/vks/network/lam-viec-voi-network-load-balancing-nlb/gioi-han-va-han-che-nlb.md'}, page_content='Giới hạn và hạn chế NLB\\n\\nGiới hạn\\n\\nMột vài lưu ý về giới hạn của việc intergrate a NLB vào một cluster:\\n\\nMột NLB có thể được sử dụng chung cho nhiều cluster nhưng phải đảm bảo các cluster này có chung VPC.\\n\\nMột NLB có thể bao gồm nhiều listener, nhiều pool, nhiều policy. Các giới hạn về số lượng listener, số lượng pool, số lượng policy vui lòng tham khảo tại Hạn mức tài nguyên.\\n\\nHạn chế\\n\\nViệc thay đổi tên hoặc kích thước (Rename, Resize) tài nguyên Load Balancer trên vServer Portal có thể gây ra sự không tương thích với tài nguyên trên Kubernetes Cluster. Điều này có thể dẫn đến việc các tài nguyên không hoạt động trên Cluster, hoặc tài nguyên bị đồng bộ lại hoặc thông tin tài nguyên giữa vServer Portal và Cluster không khớp. Để ngăn chặn vấn đề này, hãy sử dụng kubectlđể quản lý tài nguyên của Cluster.'),\n",
       " Document(metadata={'source': './../../data/vks/network/cni/README.md'}, page_content='CNI\\n\\nTổng quan\\n\\nCNI (Container Network Interface) là một bộ công cụ tiêu chuẩn cung cấp khả năng kết nối mạng cho các container trong một cụm Kubernetes. Nói một cách đơn giản, CNI là một lớp trừu tượng, giúp Kubernetes quản lý và cấu hình mạng cho các pod (một tập hợp các container chia sẻ cùng một mạng) một cách linh hoạt và hiệu quả.\\n\\nCNI hoạt động như thế nào?\\n\\nKhi bạn tạo một pod mới, Kubernetes sẽ gọi đến CNI để tạo một mạng interface cho pod đó. Plugin CNI sẽ thực hiện các tác vụ sau:\\n\\nCấp phát địa chỉ IP: Gán một địa chỉ IP duy nhất cho pod.\\n\\nCấu hình routing: Thiết lập các quy tắc routing cho phép giao tiếp giữa các pod,...\\n\\nBên cạnh đó, các kết nối hoạt động như sau:\\n\\nKết nối trong cùng một VPC: Các node trong cùng một VPC sẽ kết nối trực tiếp với nhau.\\n\\nKết nối giữa các VPC khác nhau: Sử dụng VPC Peering để kết nối các node giữa các VPC khác nhau.\\n\\nKết nối tới hạ tầng bên ngoài: Sử dụng các giải pháp kết nối mạng như VPN site-to-site hoặc Direct Connect để kết nối từ các node trong VPC tới các hạ tầng bên ngoài (On Cloud, On-premise).\\n\\nĐiều này giúp duy trì một hạ tầng mạng liên tục, linh hoạt và bảo mật trong môi trường multi-cloud hoặc hybrid-cloud.\\n\\nSo sánh giữa các plugin CNI\\n\\nHiện tại, VKS đang cung cấp 3 plugin CNI phổ biến là Calico Overlay, Cilium Overlay, Cilium VPC Native Routing. Trong đó:\\n\\nCalico Overlay: Sử dụng mô hình overlay thông qua tunneling (IP-in-IP). Tương thích với nhiều hạ tầng nhưng hiệu suất có thể bị ảnh hưởng bởi overhead từ tunnel.\\n\\nCilium Overlay: Cũng sử dụng mô hình overlay nhưng có sự tích hợp mạnh mẽ với eBPF, giúp cải thiện hiệu suất, bảo mật, và khả năng mở rộng.\\n\\nCilium VPC Native Routing: Sử dụng eBPF và không cần overlay, tận dụng khả năng routing của hạ tầng VPC, mang lại hiệu suất và khả năng mở rộng tốt nhất.\\n\\nKhi nào nên sử dụng Calico Overlay: đơn giản trong việc sử dụng, không yêu cầu hiệu suất quá cao.\\n\\nKhi nào nên sử dụng Cilium Overlay: đơn giản trong việc sử dụng, không yêu cầu hiệu suất quá cao tuy nhiên có nhu cầu monitor chuyên sâu (Hubble).\\n\\nKhi nào nên sử dụng Cilium VPC Native Routing: yêu cầu hiệu năng cao, khả năng kết nối với các hệ thống bên ngoài dễ dàng và có nhu cầu monitor chuyên sâu (Hubble).'),\n",
       " Document(metadata={'source': './../../data/vks/network/cni/su-dung-cni-cilium-vpc-native-routing.md'}, page_content='Sử dụng CNI Cilium VPC Native Routing\\n\\nTổng quan\\n\\nCNI (Container Network Interface) Cilium VPC Native Routing là một cơ chế giúp Kubernetes quản lý mạng mà không cần sử dụng overlay networks. Thay vì dùng các lớp mạng ảo, CNI Cilium VPC Native Routing tận dụng khả năng routing trực tiếp từ VPC (Virtual Private Cloud) của các nhà cung cấp dịch vụ đám mây để tối ưu hóa việc truyền dữ liệu giữa các node và pod trong cụm Kubernetes.\\n\\nModel\\n\\nTrên VKS, CNI (Container Network Interface) Cilium VPC Native Routing hoạt động theo mô hình sau:\\n\\nTrong đó:\\n\\nMỗi Node có một dải địa chỉ IP riêng cho các pod (Pod CIDR). Các pod trong mỗi node sử dụng địa chỉ từ CIDR này và giao tiếp qua mạng ảo.\\n\\nCilium và eBPF thực hiện quản lý mạng cho tất cả các pod trên mỗi node, bao gồm việc xử lý lưu lượng đi từ pod này đến pod khác, hoặc từ node này sang node khác. Khi cần, eBPF thực hiện masquerading để ẩn địa chỉ IP nội bộ của pod khi giao tiếp với mạng ngoài.\\n\\nCilium đảm bảo rằng các pod có thể giao tiếp với nhau cả bên trong cùng node và giữa các node khác nhau.\\n\\nĐiều kiện cần\\n\\nĐể có thể khởi tạo một Cluster và Deploy một Workload, bạn cần:\\n\\nCó ít nhất 1 VPC và 1 Subnet đang ở trạng thái ACTIVE. Nếu bạn chưa có VPC, Subnet nào, vui lòng khởi tạo VPC, Subnet theo hướng dẫn bên dưới:\\n\\nBước 1: Truy cập vào trang chủ vServer tại link https://hcm-3.console.vngcloud.vn/vserver\\n\\nBước 2: Chọn menu VPCs ở menu bên trái màn hình.\\n\\nBước 3: Tại đây, nếu bạn chưa có VPC nào, vui lòng chọn Create VPC bằng cách nhập VPC name và định nghĩa dãy CIDR/16 mong muốn.\\n\\nBước 4: Sau khi đã có ít nhất 1 VPC, để tạo subnet, bạn cần chọn View Detail để mở rộng bảng điều khiển ở phía dưới, trong đó có mục Subnet.\\n\\nBước 5: Tại mục Subnet, chọn Add Subnet. Lúc này, bạn cần nhập:\\n\\nSubnet name: tên gợi nhớ của subnet\\n\\nPrimary CIDR: Đây là dải địa chỉ IP chính của subnet. Mọi địa chỉ IP nội bộ của các máy ảo (VM) trong subnet này sẽ được lấy từ dải địa chỉ này. Giả sử, nếu bạn đặt Primary CIDR là 10.1.0.0/24, các địa chỉ IP của các VM sẽ nằm trong khoảng từ 10.1.0.1 đến 10.1.0.254.\\n\\nSecondary CIDR: Đây là dải địa chỉ IP phụ, được sử dụng để cung cấp thêm địa chỉ IP hoặc để phân chia các dịch vụ khác nhau trong cùng một subnet. Mỗi Node có một dải địa chỉ IP riêng cho các pod (Pod CIDR). Các pod trong mỗi node sử dụng địa chỉ từ CIDR này và giao tiếp qua mạng ảo.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nCác dải địa chỉ IP của Primary CIDR và Secondary CIDR không được trùng lặp. Điều này có nghĩa là dải địa chỉ của Secondary CIDR phải nằm ngoài phạm vi của Primary CIDR và ngược lại. Giả sử, nếu Primary CIDR là 10.1.0.0/24, thì Secondary CIDR không thể là 10.1.0.0/20 vì nó nằm trong phạm vi của Primary CIDR. Thay vào đó, bạn có thể sử dụng một dải địa chỉ khác như 10.1.16.0/20. {% endhint %}\\n\\nCó ít nhất 1 SSH key đang ở trạng thái ACTIVE. Nếu bạn chưa có SSH key nào, vui lòng khởi tạo SSH key theo hướng dẫn tại đây.\\n\\nĐã cài đặt và cấu hình kubectl trên thiết bị của bạn. vui lòng tham khảo tại đây nếu bạn chưa rõ cách cài đặt và sử dụng kuberctl. Ngoài ra, bạn không nên sử dụng phiên bản kubectl quá cũ, chúng tôi khuyến cáo bạn nên sử dụng phiên bản kubectl sai lệch không quá một phiên bản với version của cluster.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi bạn sử dụng loại network Cilium Native Routing cho cluster của bạn, bạn cần thực hiện cấu hình các Security Groups cho phép các kết nối cần thiết. Ví dụ, khi bạn chạy một NGINX pod trên một node, bạn phải cho phép traffic trên port 80 để đảm bảo các requests từ các node khác có thể kết nối tới. Việc cấu hình này chỉ bắt buộc nếu bạn sử dụng Cilium Native Routing, đối với Calico Overlay và Cilium Overlay thì việc cấu hình Security Groups này là không cần thiết. {% endhint %}\\n\\nKhởi tạo một Cluster sử dụng CNI Cilium VPC Native Routing\\n\\nĐể khởi tạo một Cluster, hãy làm theo các bước bên dưới:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn Activate.\\n\\nBước 3: Chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn. Sau khi Activate thành công, bạn hãy chọn Create a Cluster.\\n\\nBước 4: Tại màn hình khởi tạo Cluster, chúng tôi đã thiết lập thông tin cho Cluster và một Default Node Group cho bạn. Để sử dụng CNI Cilium VPC Native Routing cho Cluster của bạn, vui lòng chọn:\\n\\nNetwork type: Cilium VPC Native Routing và các thông số khác như sau:\\n\\nField Ý nghĩa Ví dụ minh họa VPC Dải địa chỉ IP mà các node của Cluster sẽ sử dụng để giao tiếp. Trong hình, chúng tôi lựa chọn VPC có IP range là 10.111.0.0/16 , tương ứng với 65536 IP Subnet Dải địa chỉ IP nhỏ hơn thuộc VPC. Mỗi node trong Cluster sẽ được gán một IP từ Subnet này. Subnet phải nằm trong dải IP của VPC đã chọn. Trong hình, chúng tôi lựa chọn Subnet có Primary IP range là 10.111.0.0/24 , tương ứng với 256 IP Default Pod IP range Đây là dải địa chỉ IP thứ cấp được sử dụng cho các pod. Nó được gọi là Secondary IP range vì nó không trùng với dải IP chính của node (Primary IP range). Các pod trong Cluster sẽ được gán IP từ dải này. Trong hình, chúng tôi lựa chọn Secondary IP range là 10.111.160.0/20 - Tương ứng với 4096 IP cho các pod Node CIDR mask size Kích thước của CIDR dành cho các node. Thông số này cho biết mỗi node sẽ được gán bao nhiêu địa chỉ IP từ dải pod IP range. Kích thước này cần được chọn sao cho đảm bảo có đủ địa chỉ IP cho tất cả các pod trên mỗi node. Bạn có thể tham khảo bảng bên dưới để hiểu các tính số lượng IP có thể sử dụng để cấp phát cho node, pod trong cluster của bạn. Trong hình, chúng tôi lựa chọn Node CIDR mask size là /25 - Mỗi node sẽ có 128 địa chỉ IP , phù hợp với số lượng pod bạn mong muốn chạy trên một node.\\n\\nCác tính toán số lượng IP cho pod và node:\\n\\nGiả sử, khi khởi tạo cluster, tôi lựa chọn:\\n\\nVPC: 10.111.0.0/16\\n\\nSubnet:\\n\\nPrimary IP Range: 10.111.0.0/24\\n\\nSecondary IP Range: 10.111.160.0/20\\n\\nNode CIDR mask size: Các giá trị có thể chọn từ /24 đến /26.\\n\\nNode CIDR mask size Số lượng IP cho mỗi node Số lượng node có thể tạo trong dải /20 (4096 IP) Số lượng IP phân bổ cho pod trên mỗi node Số lượng pod thực tế có thể tạo /24 256 16 256 128 /25 128 32 128 64 /26 64 64 64 32\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nChỉ một loại networktype: Trong một cluster, bạn chỉ có thể sử dụng một trong ba loại networktype: Calico Overlay, Cilium Overlay, hoặc Cilium VPC Native Routing\\n\\nMultiple subnet cho một cluster: VKS hỗ trợ việc sử dụng nhiều subnet cho một cluster. Điều này cho phép bạn cấu hình mỗi node group trong cluster nằm ở các subnet khác nhau trong cùng một VPC, giúp tối ưu hóa việc phân bổ tài nguyên và quản lý mạng.\\n\\nCilium VPC Native Routing và Secondary IP Range: Khi sử dụng Cilium VPC Native Routing cho một cluster, bạn có thể sử dụng nhiều Secondary IP Range. Tuy nhiên, mỗi Secondary IP Range chỉ có thể được sử dụng bởi một cluster duy nhất. Điều này giúp tránh xung đột địa chỉ IP và đảm bảo tính nhất quán trong quản lý mạng.\\n\\nKhi không đủ địa chỉ IP trong Node CIDR range hoặc Secondary IP range để tạo thêm node, cụ thể:\\n\\nNếu bạn không thể sử dụng Node mới do hết dải địa chỉ IP trong Secondary IP range. Lúc này, các node mới vẫn sẽ được tạo và được join vào cụm nhưng bạn không thể sử dụng chúng. Các pod được yêu cầu khởi chạy trên node mới này sẽ bị kẹt trong trạng thái \"ContainerCreating\" do không thể tìm thấy node phù hợp để triển khai. Lúc này, bạn cần tạo node group mới với secondary range IP chưa được sử dụng trên cluster nào. {% endhint %}\\n\\nBước 5: Chọn Create Kubernetes cluster. Hãy chờ vài phút để chúng tôi khởi tạo Cluster của bạn, trạng thái của Cluster lúc này là Creating.\\n\\nBước 6: Khi trạng thái Cluster là Active, bạn có thể xem thông tin Cluster, thông tin Node Group bằng cách chọn vào Cluster Name tại cột Name.\\n\\nDeploy một Workload\\n\\nBên dưới là hướng dẫn triển khai một deployment nginx và kiểm tra việc phân chia IP cho các pod được triển khai trong cluster của bạn.\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/k8s-cluster\\n\\nBước 2: Danh sách Cluster được hiển thị, chọn biểu tượng Download và chọn Download Config File để thực hiện tải xuống file kubeconfig. File này sẽ giúp bạn có toàn quyền truy cập vào Cluster của bạn.\\n\\nBước 3: Đổi tên file này thành config và lưu nó vào thư mục \\\\~/.kube/config\\n\\nBước 4: Thực hiện kiểm tra Cluster thông qua lệnh:\\n\\nChạy câu lệnh sau đây để kiểm tra node\\n\\nbash kubectl get nodes\\n\\nNếu kết quả trả về như bên dưới tức là bạn Cluster của bạn được khởi tạo thành công với 3 node:\\n\\nbash NAME STATUS ROLES AGE VERSION vks-cluster-democilium-nodegroup-558f4-39206 Ready <none> 5m35s v1.28.8 vks-cluster-democilium-nodegroup-558f4-63344 Ready <none> 5m45s v1.28.8 vks-cluster-democilium-nodegroup-558f4-e6e4d Ready <none> 6m24s v1.28.8\\n\\nTiếp tục thực hiện chạy lệnh sau đây để kiểm tra các pod đã được triển khai trên namespace kube-system của bạn:\\n\\nbash kubectl get pods -A\\n\\nNếu kết quả trả về như bên dưới tức là trạng thái các pods hỗ trợ chạy Cilium VPC Native đều thành công (Running):\\n\\nbash NAMESPACE NAME READY STATUS RESTARTS AGE kube-system cilium-envoy-2g22l 1/1 Running 0 6m41s kube-system cilium-envoy-h9mjb 1/1 Running 0 5m53s kube-system cilium-envoy-ngz89 1/1 Running 0 6m3s kube-system cilium-ft98g 1/1 Running 1 (5m33s ago) 6m2s kube-system cilium-operator-5fc5c56c4c-66l6d 1/1 Running 0 10m kube-system cilium-operator-5fc5c56c4c-qfnw2 1/1 Running 0 10m kube-system cilium-rfrr7 1/1 Running 1 (6m10s ago) 6m41s kube-system cilium-xmlq5 1/1 Running 1 (5m24s ago) 5m53s kube-system coredns-1727334052-85db76748b-fpmfr 1/1 Running 0 6m22s kube-system coredns-1727334052-85db76748b-jqv79 1/1 Running 0 6m22s kube-system hubble-relay-8578649fdb-bgzzz 1/1 Running 1 (4m35s ago) 10m kube-system hubble-ui-574c5bb99b-g7l6c 2/2 Running 0 10m kube-system konnectivity-agent-hmf2x 1/1 Running 0 5m24s kube-system konnectivity-agent-q69n2 1/1 Running 0 6m15s kube-system konnectivity-agent-wgqbw 1/1 Running 0 5m14s kube-system vngcloud-controller-manager-d4d4f7b84-m65nb 1/1 Running 0 11m kube-system vngcloud-csi-controller-565c55dbcc-88pt4 7/7 Running 8 (4m59s ago) 11m kube-system vngcloud-csi-controller-565c55dbcc-v22q4 7/7 Running 8 (4m59s ago) 11m kube-system vngcloud-csi-node-665r2 3/3 Running 3 (5m15s ago) 6m41s kube-system vngcloud-csi-node-8x542 3/3 Running 3 (52s ago) 6m3s kube-system vngcloud-csi-node-gx7zd 3/3 Running 2 (83s ago) 5m53s kube-system vngcloud-ingress-controller-0 1/1 Running 1 (5m55s ago) 11m\\n\\nBước 2: Triển khai nginx trên cluster vừa khởi tạo:\\n\\nThực hiện khởi tạo tệp tin nginx-deployment.yaml với nội dung tương tự bên dưới:\\n\\nbash apiVersion: apps/v1 kind: Deployment metadata: name: nginx-app spec: selector: matchLabels: app: nginx replicas: 20 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80\\n\\nThực hiện triển khai deployment này qua lệnh:\\n\\nbash kubectl apply -f nginx-deployment.yaml\\n\\nBước 3: Kiểm tra các pod nginx đã được triển khai và địa chỉ IP được gán cho mỗi pod\\n\\nThực hiện kiểm tra các pod qua lệnh:\\n\\nbash kubectl get pods -o wide\\n\\nBạn có thể quan sát bên dưới, các pod nginx được gán các IP 10.111.16x.x thỏa mãn điều kiện Secondary IP range và Node CIDR mask size mà chúng tôi đã chỉ định bên trên:\\n\\nbash NAME READY STATUS RESTARTS AGE IP NODE nginx-app-7c79c4bf97-6v88s 1/1 Running 0 31s 10.111.161.53 vks-cluster-democilium-nodegroup-558f4-39206 nginx-app-7c79c4bf97-754m7 1/1 Running 0 31s 10.111.161.1 vks-cluster-democilium-nodegroup-558f4-39206 nginx-app-7c79c4bf97-9tjw7 1/1 Running 0 31s 10.111.160.212 vks-cluster-democilium-nodegroup-558f4-63344 nginx-app-7c79c4bf97-c6vx7 1/1 Running 0 31s 10.111.160.46 vks-cluster-democilium-nodegroup-558f4-e6e4d nginx-app-7c79c4bf97-c7nch 1/1 Running 0 31s 10.111.161.3 vks-cluster-democilium-nodegroup-558f4-39206 nginx-app-7c79c4bf97-cggfq 1/1 Running 0 31s 10.111.161.74 vks-cluster-democilium-nodegroup-558f4-39206 nginx-app-7c79c4bf97-cz4xc 1/1 Running 0 31s 10.111.160.115 vks-cluster-democilium-nodegroup-558f4-e6e4d nginx-app-7c79c4bf97-d84rb 1/1 Running 0 31s 10.111.160.152 vks-cluster-democilium-nodegroup-558f4-63344 nginx-app-7c79c4bf97-dbmt7 1/1 Running 0 31s 10.111.160.184 vks-cluster-democilium-nodegroup-558f4-63344 nginx-app-7c79c4bf97-gtx8b 1/1 Running 0 31s 10.111.161.57 vks-cluster-democilium-nodegroup-558f4-39206 nginx-app-7c79c4bf97-km7tx 1/1 Running 0 31s 10.111.160.94 vks-cluster-democilium-nodegroup-558f4-e6e4d nginx-app-7c79c4bf97-lmk7c 1/1 Running 0 31s 10.111.161.26 vks-cluster-democilium-nodegroup-558f4-39206 nginx-app-7c79c4bf97-mc24h 1/1 Running 0 31s 10.111.160.98 vks-cluster-democilium-nodegroup-558f4-e6e4d nginx-app-7c79c4bf97-n4zvf 1/1 Running 0 31s 10.111.160.204 vks-cluster-democilium-nodegroup-558f4-63344 nginx-app-7c79c4bf97-n84tc 1/1 Running 0 31s 10.111.161.106 vks-cluster-democilium-nodegroup-558f4-39206 nginx-app-7c79c4bf97-qtjjx 1/1 Running 0 31s 10.111.160.32 vks-cluster-democilium-nodegroup-558f4-e6e4d nginx-app-7c79c4bf97-rp4bt 1/1 Running 0 31s 10.111.160.202 vks-cluster-democilium-nodegroup-558f4-63344 nginx-app-7c79c4bf97-sk7tf 1/1 Running 0 31s 10.111.160.196 vks-cluster-democilium-nodegroup-558f4-63344 nginx-app-7c79c4bf97-x8jxm 1/1 Running 0 31s 10.111.160.135 vks-cluster-democilium-nodegroup-558f4-63344 nginx-app-7c79c4bf97-zlstg 1/1 Running 0 31s 10.111.160.121 vks-cluster-democilium-nodegroup-558f4-e6e4d\\n\\nBạn cũng có thể thực hiện xem mô tả chi tiết mỗi pod để kiểm tra thông tin pod này qua lệnh:\\n\\nbash kubectl describe pod nginx-app-7c79c4bf97-6v88s\\n\\nBước 4: Bạn có thể thực hiện một vài bước để kiểm tra chuyên sâu việc hoạt động của Cilium. Cụ thể:\\n\\nĐầu tiên, bạn cần cài đặt Cilium CLI theo hướng dẫn tại đây.\\n\\nSau khi cài đặt Cilium CLI, thực hiện kiểm tra trạng thái của Cilium trong cluster của bạn qua lệnh:\\n\\nbash cilium status wait\\n\\nNếu kết quả hiển thị như bên dưới tức là Cilium đang hoạt động đúng và đầy đủ:\\n\\n```bash /¯¯\\\\ /¯¯_/¯¯\\\\ Cilium: OK _/¯¯_/ Operator: OK /¯¯_/¯¯\\\\ Envoy DaemonSet: OK _/¯¯_/ Hubble Relay: OK __/ ClusterMesh: disabled\\n\\nDaemonSet cilium-envoy Desired: 3, Ready: 3/3, Available: 3/3 Deployment hubble-relay Desired: 1, Ready: 1/1, Available: 1/1 Deployment hubble-ui Desired: 1, Ready: 1/1, Available: 1/1 Deployment cilium-operator Desired: 2, Ready: 2/2, Available: 2/2 DaemonSet cilium Desired: 3, Ready: 3/3, Available: 3/3 Containers: hubble-ui Running: 1 cilium-operator Running: 2 cilium Running: 3 cilium-envoy Running: 3 hubble-relay Running: 1 Cluster Pods: 32/32 managed by Cilium Helm chart version: Image versions cilium vcr.vngcloud.vn/81-vks-public/cilium/cilium:v1.16.1: 3 cilium-envoy vcr.vngcloud.vn/81-vks-public/cilium/cilium-envoy:v1.29.7-39a2a56bbd5b3a591f69dbca51d3e30ef97e0e51: 3 hubble-relay vcr.vngcloud.vn/81-vks-public/cilium/hubble-relay:v1.16.1: 1 hubble-ui vcr.vngcloud.vn/81-vks-public/cilium/hubble-ui-backend:v0.13.1: 1 hubble-ui vcr.vngcloud.vn/81-vks-public/cilium/hubble-ui:v0.13.1: 1 cilium-operator vcr.vngcloud.vn/81-vks-public/cilium/operator-generic:v1.16.1: 2 ```\\n\\nBước 5: Bạn có thể thực hiện healthy check kiểm tra Cilium trong cluster của bạn\\n\\nChạy lệnh sau để thực hiện healthy check:\\n\\nbash kubectl -n kube-system exec ds/cilium -- cilium-health status --probe\\n\\nKết quả healthy check mong muốn sẽ như sau:\\n\\nbash Probe time: 2024-09-26T07:11:57Z Nodes: vks-cluster-democilium-nodegroup-558f4-e6e4d (localhost): Host connectivity to 10.111.0.8: ICMP to stack: OK, RTT=306.523µs HTTP to agent: OK, RTT=206.191µs Endpoint connectivity to 10.111.160.91: ICMP to stack: OK, RTT=307.205µs HTTP to agent: OK, RTT=365.113µs vks-cluster-democilium-nodegroup-558f4-39206: Host connectivity to 10.111.0.14: ICMP to stack: OK, RTT=1.90859ms HTTP to agent: OK, RTT=344.725µs Endpoint connectivity to 10.111.161.9: ICMP to stack: OK, RTT=1.889682ms HTTP to agent: OK, RTT=549.887µs vks-cluster-democilium-nodegroup-558f4-63344: Host connectivity to 10.111.0.9: ICMP to stack: OK, RTT=1.920985ms HTTP to agent: OK, RTT=706.376µs Endpoint connectivity to 10.111.160.223: ICMP to stack: OK, RTT=1.919709ms HTTP to agent: OK, RTT=1.090877ms\\n\\nNgoài ra, bạn cũng có thể thực hiện thêm các bài kiểm tra kết nối End-to-End hoặc kiểm tra Network performance theo hướng dẫn tại End-To-End Connectivity Testing hoặc Network Performance Test.\\n\\nBước 6: Kiểm tra kết nối giữa các Pod\\n\\nThực hiện kiểm tra kết nối giữa các pod, đảm bảo rằng các pod có thể giao tiếp qua địa chỉ IP của VPC mà không cần qua overlay networks. Ví dụ bên dưới tôi thực hiện ping từ pod nginx-app-7c79c4bf97-6v88s có địa chỉ IP: 10.111.161.53 tới một server trong cùng VPC có địa chỉ IP: 10.111.0.10:\\n\\nbash kubectl exec -it nginx-app-7c79c4bf97-6v88s -- ping 10.111.0.10\\n\\nNếu kết quả như sau tức là kết nối đã thông:\\n\\nbash PING 10.111.0.10 (10.111.0.10): 56 data bytes 64 bytes from 10.111.0.10: seq=0 ttl=62 time=3.327 ms 64 bytes from 10.111.0.10: seq=1 ttl=62 time=0.541 ms 64 bytes from 10.111.0.10: seq=2 ttl=62 time=0.472 ms 64 bytes from 10.111.0.10: seq=3 ttl=62 time=0.463 ms --- 10.111.0.10 ping statistics --- 4 packets transmitted, 4 packets received, 0% packet loss round-trip min/avg/max = 0.463/1.200/3.327 ms'),\n",
       " Document(metadata={'source': './../../data/vks/network/cni/su-dung-cni-cilium-overlay.md'}, page_content='Sử dụng CNI Cilium Overlay\\n\\nTổng quan\\n\\nCNI Cilium Overlay trong VKS là một loại overlay network sử dụng eBPF (extended Berkeley Packet Filter) để tăng cường hiệu suất và bảo mật mạng. Nếu so sánh với các giải pháp như Calico Overlay, Cilium mang lại hiệu suất cao hơn nhờ khả năng xử lý lưu lượng trực tiếp trong kernel bằng eBPF. Ngoài ra, với Calico thường sử dụng iptables để quản lý lưu lượng, trong khi Cilium với eBPF có thể xử lý các chính sách mạng và các hành vi ứng dụng cụ thể (Layer 7).\\n\\nModel\\n\\nTrên VKS, Cilium Overlay hoạt động theo mô hình sau:\\n\\nTrong đó:\\n\\nPod (eth0) -> lxc01/lxc02: Các Pod giao tiếp thông qua mạng ảo được tạo bởi Cilium.\\n\\nlxc01/lxc02 -> cilium_host: Các gói tin từ các Pod được chuyển đến cilium_host, là lớp trung gian giữa mạng của Pod và mạng vật lý.\\n\\ncilium_host -> ens3: Sau khi được xử lý bởi Cilium (và eBPF), các gói tin được gửi đến mạng vật lý thông qua ens3.\\n\\nens3 -> VPC Network: Cuối cùng, các gói tin được truyền qua mạng vật lý để đến các node khác hoặc ra khỏi cluster.\\n\\nĐiều kiện cần\\n\\nĐể có thể khởi tạo một Cluster và Deploy một Workload, bạn cần:\\n\\nCó ít nhất 1 VPC và 1 Subnet đang ở trạng thái ACTIVE. Nếu bạn chưa có VPC, Subnet nào, vui lòng khởi tạo VPC, Subnet theo hướng dẫn tại đây.\\n\\nCó ít nhất 1 SSH key đang ở trạng thái ACTIVE. Nếu bạn chưa có SSH key nào, vui lòng khởi tạo SSH key theo hướng dẫn tại đây.\\n\\nĐã cài đặt và cấu hình kubectl trên thiết bị của bạn. vui lòng tham khảo tại đây nếu bạn chưa rõ cách cài đặt và sử dụng kuberctl. Ngoài ra, bạn không nên sử dụng phiên bản kubectl quá cũ, chúng tôi khuyến cáo bạn nên sử dụng phiên bản kubectl sai lệch không quá một phiên bản với version của cluster.\\n\\nKhởi tạo một Cluster sử dụng Cilium Overlay\\n\\nĐể khởi tạo một Cluster, hãy làm theo các bước bên dưới:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn Activate.\\n\\nBước 3: Chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn. Sau khi Activate thành công, bạn hãy chọn Create a Cluster.\\n\\nBước 4: Tại màn hình khởi tạo Cluster, chúng tôi đã thiết lập thông tin cho Cluster và một Default Node Group cho bạn. Để sử dụng Cilium Overlay cho Cluster của bạn, vui lòng chọn:\\n\\nNetwork type: Cilium Overlay\\n\\nField Ý nghĩa Ví dụ minh họa VPC Dải địa chỉ IP mà các node của Cluster sẽ sử dụng để giao tiếp. Trong hình, chúng tôi lựa chọn VPC có IP range là 10.111.0.0/16 , tương ứng với 65536 IP Subnet Dải địa chỉ IP nhỏ hơn thuộc VPC. Mỗi node trong Cluster sẽ được gán một IP từ Subnet này. Subnet phải nằm trong dải IP của VPC đã chọn. Trong hình, chúng tôi lựa chọn Subnet có Primary IP range là 10.111.0.0/24 , tương ứng với 256 IP IP-IP encapsulation mode Chế độ IP-IP encapsulation trong VKS là Always Trong hình, chúng tôi lựa chọn chế độ Always để luôn encapsulate các gói tin. CIDR Dải mạng ảo mà các pod sẽ sử dụng Trong hình, chúng tôi lựa chọn dải mạng ảo là 172.16.0.0/16 . Các pod sẽ lấy IP từ dải IP này.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nChỉ một loại networktype: Trong một cluster, bạn chỉ có thể sử dụng một trong ba loại networktype: Calico Overlay, Cilium Overlay, hoặc Cilium VPC Native Routing\\n\\nMultiple subnet cho một cluster: VKS hỗ trợ việc sử dụng nhiều subnet cho một cluster. Điều này cho phép bạn cấu hình mỗi node group trong cluster nằm ở các subnet khác nhau trong cùng một VPC, giúp tối ưu hóa việc phân bổ tài nguyên và quản lý mạng. {% endhint %}\\n\\nBước 5: Chọn Create Kubernetes cluster. Hãy chờ vài phút để chúng tôi khởi tạo Cluster của bạn, trạng thái của Cluster lúc này là Creating.\\n\\nBước 6: Khi trạng thái Cluster là Active, bạn có thể xem thông tin Cluster, thông tin Node Group bằng cách chọn vào Cluster Name tại cột Name.\\n\\nDeploy một Workload\\n\\nBên dưới là hướng dẫn triển khai một deployment nginx và kiểm tra việc phân chia IP cho các pod được triển khai trong cluster của bạn.\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/k8s-cluster\\n\\nBước 2: Danh sách Cluster được hiển thị, chọn biểu tượng Download và chọn Download Config File để thực hiện tải xuống file kubeconfig. File này sẽ giúp bạn có toàn quyền truy cập vào Cluster của bạn.\\n\\nBước 3: Đổi tên file này thành config và lưu nó vào thư mục \\\\~/.kube/config\\n\\nBước 4: Thực hiện kiểm tra Cluster thông qua lệnh:\\n\\nChạy câu lệnh sau đây để kiểm tra node\\n\\nbash kubectl get nodes\\n\\nNếu kết quả trả về như bên dưới tức là bạn Cluster của bạn được khởi tạo thành công với 3 node:\\n\\nbash NAME STATUS ROLES AGE VERSION vks-cluster-02-nodegroup-7fb09-3a594 Ready <none> 5m48s v1.29.1 vks-cluster-02-nodegroup-7fb09-3cb67 Ready <none> 5m34s v1.29.1 vks-cluster-02-nodegroup-7fb09-430aa Ready <none> 5m52s v1.29.1\\n\\nTiếp tục thực hiện chạy lệnh sau đây để kiểm tra các pod đã được triển khai trên namespace kube-system của bạn:\\n\\nbash kubectl get pods -A\\n\\nNếu kết quả trả về như bên dưới tức là các pods hỗ trợ chạy Cilium Overlay đã được running:\\n\\nbash NAMESPACE NAME READY STATUS RESTARTS AGE kube-system cilium-8xtwz 1/1 Running 1 (5m36s ago) 6m7s kube-system cilium-cpxvv 1/1 Running 0 5m53s kube-system cilium-envoy-b95pg 1/1 Running 0 6m7s kube-system cilium-envoy-dx8qg 1/1 Running 0 6m11s kube-system cilium-envoy-sqdn8 1/1 Running 0 5m53s kube-system cilium-operator-75b8c6f6d4-7x4f6 1/1 Running 0 9m19s kube-system cilium-operator-75b8c6f6d4-k7j45 1/1 Running 0 9m19s kube-system cilium-zs2cm 1/1 Running 1 (5m35s ago) 6m11s kube-system coredns-1727408780-5fcf89468-7hmvp 1/1 Running 0 9m24s kube-system coredns-1727408780-5fcf89468-v9nbd 1/1 Running 0 9m24s kube-system hubble-relay-8899f8cdc-976zf 1/1 Running 1 (4m3s ago) 9m20s kube-system hubble-ui-574c5bb99b-gg7jx 2/2 Running 0 9m20s kube-system konnectivity-agent-46nvd 1/1 Running 0 5m27s kube-system konnectivity-agent-qhq4m 1/1 Running 0 5m24s kube-system konnectivity-agent-xs7bq 1/1 Running 0 5m21s kube-system vngcloud-controller-manager-7c47d64584-z8827 1/1 Running 0 9m21s kube-system vngcloud-csi-controller-848f68f46-2hkxl 7/7 Running 2 (4m58s ago) 9m23s kube-system vngcloud-csi-controller-848f68f46-bkvkg 7/7 Running 2 (4m56s ago) 9m23s kube-system vngcloud-csi-node-8rxbx 3/3 Running 2 (5m1s ago) 6m7s kube-system vngcloud-csi-node-mxknq 3/3 Running 3 (4m54s ago) 5m53s kube-system vngcloud-csi-node-tfrsp 3/3 Running 2 (5m2s ago) 6m11s kube-system vngcloud-ingress-controller-0 1/1 Running 1 (5m16s ago) 9m7s\\n\\nBước 2: Triển khai nginx trên cluster vừa khởi tạo:\\n\\nThực hiện khởi tạo tệp tin nginx-deployment.yaml với nội dung tương tự bên dưới:\\n\\nbash apiVersion: apps/v1 kind: Deployment metadata: name: nginx-app spec: selector: matchLabels: app: nginx replicas: 20 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80\\n\\nThực hiện triển khai deployment này qua lệnh:\\n\\nbash kubectl apply -f nginx-deployment.yaml\\n\\nBước 3: Kiểm tra các pod nginx đã được triển khai và địa chỉ IP được gán cho mỗi pod\\n\\nThực hiện kiểm tra các pod qua lệnh:\\n\\nbash kubectl get pods -o wide\\n\\nBạn có thể quan sát bên dưới, các pod nginx được gán các IP 172.16.x.x thỏa mãn điều kiện Cilium CIDR 172.16.0.0/16 mà chúng tôi đã chỉ định bên trên:\\n\\nbash NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-app-7c79c4bf97-4lcbn 1/1 Running 0 83s 172.16.0.6 vks-cluster-02-nodegroup-7fb09-430aa <none> <none> nginx-app-7c79c4bf97-669z9 1/1 Running 0 83s 172.16.1.115 vks-cluster-02-nodegroup-7fb09-3a594 <none> <none> nginx-app-7c79c4bf97-7hqp5 1/1 Running 0 83s 172.16.1.32 vks-cluster-02-nodegroup-7fb09-3a594 <none> <none> nginx-app-7c79c4bf97-8fjhm 1/1 Running 0 83s 172.16.2.51 vks-cluster-02-nodegroup-7fb09-3cb67 <none> <none> nginx-app-7c79c4bf97-8xmfm 1/1 Running 0 83s 172.16.0.100 vks-cluster-02-nodegroup-7fb09-430aa <none> <none> nginx-app-7c79c4bf97-9b4px 1/1 Running 0 83s 172.16.2.19 vks-cluster-02-nodegroup-7fb09-3cb67 <none> <none> nginx-app-7c79c4bf97-b7vlg 1/1 Running 0 83s 172.16.1.128 vks-cluster-02-nodegroup-7fb09-3a594 <none> <none> nginx-app-7c79c4bf97-bc6r4 1/1 Running 0 83s 172.16.0.160 vks-cluster-02-nodegroup-7fb09-430aa <none> <none> nginx-app-7c79c4bf97-flkz5 1/1 Running 0 83s 172.16.2.253 vks-cluster-02-nodegroup-7fb09-3cb67 <none> <none> nginx-app-7c79c4bf97-k55j6 1/1 Running 0 83s 172.16.2.76 vks-cluster-02-nodegroup-7fb09-3cb67 <none> <none> nginx-app-7c79c4bf97-l9p8p 1/1 Running 0 83s 172.16.2.187 vks-cluster-02-nodegroup-7fb09-3cb67 <none> <none> nginx-app-7c79c4bf97-llnfq 1/1 Running 0 83s 172.16.1.30 vks-cluster-02-nodegroup-7fb09-3a594 <none> <none> nginx-app-7c79c4bf97-mg9t8 1/1 Running 0 83s 172.16.0.221 vks-cluster-02-nodegroup-7fb09-430aa <none> <none> nginx-app-7c79c4bf97-mlh7g 1/1 Running 0 83s 172.16.2.191 vks-cluster-02-nodegroup-7fb09-3cb67 <none> <none> nginx-app-7c79c4bf97-n946h 1/1 Running 0 83s 172.16.1.82 vks-cluster-02-nodegroup-7fb09-3a594 <none> <none> nginx-app-7c79c4bf97-p9k42 1/1 Running 0 83s 172.16.0.112 vks-cluster-02-nodegroup-7fb09-430aa <none> <none> nginx-app-7c79c4bf97-sl4b8 1/1 Running 0 83s 172.16.1.22 vks-cluster-02-nodegroup-7fb09-3a594 <none> <none> nginx-app-7c79c4bf97-tdtjc 1/1 Running 0 83s 172.16.2.109 vks-cluster-02-nodegroup-7fb09-3cb67 <none> <none> nginx-app-7c79c4bf97-zwxps 1/1 Running 0 83s 172.16.2.209 vks-cluster-02-nodegroup-7fb09-3cb67 <none> <none> nginx-app-7c79c4bf97-zxx87 1/1 Running 0 83s 172.16.0.212 vks-cluster-02-nodegroup-7fb09-430aa <none> <none>\\n\\nBạn cũng có thể thực hiện xem mô tả chi tiết mỗi pod để kiểm tra thông tin pod này qua lệnh:\\n\\nbash kubectl describe pod nginx-app-7c79c4bf97-4lcbn'),\n",
       " Document(metadata={'source': './../../data/vks/network/cni/su-dung-cni-calico-overlay.md'}, page_content='Sử dụng CNI Calico Overlay\\n\\nTổng quan\\n\\nCNI Calico Overlay trong VKS là một loại overlay network sử dụng giao thức IP-in-IP encapsulation tạo ra một mạng overlay. Điều này cho phép các pod giao tiếp với nhau mà không cần thay đổi cấu hình mạng vật lý bên dưới. Các pod sẽ nhận địa chỉ IP từ dải địa chỉ IP được cấu hình cho Calico, thường là khác với địa chỉ IP của VPC hoặc subnet của bạn.\\n\\nModel\\n\\nTrên VKS, Calico Overlay hoạt động theo mô hình sau:\\n\\nTrong đó:\\n\\nCác pod trên mỗi node giao tiếp với nhau thông qua cali interface và bridge cni0.\\n\\nKhi các pod cần giao tiếp với các pod trên các node khác, gói tin sẽ được gói (encapsulated) vào trong gói overlay và gửi qua mạng vật lý (VPC Network).\\n\\nCalico trên mỗi node chịu trách nhiệm thực hiện gói hóa (encapsulation) và giải gói (decapsulation) để các pod có thể liên lạc xuyên qua các node khác nhau.\\n\\nĐiều kiện cần\\n\\nĐể có thể khởi tạo một Cluster và Deploy một Workload, bạn cần:\\n\\nCó ít nhất 1 VPC và 1 Subnet đang ở trạng thái ACTIVE. Nếu bạn chưa có VPC, Subnet nào, vui lòng khởi tạo VPC, Subnet theo hướng dẫn tại đây.\\n\\nCó ít nhất 1 SSH key đang ở trạng thái ACTIVE. Nếu bạn chưa có SSH key nào, vui lòng khởi tạo SSH key theo hướng dẫn tại đây.\\n\\nĐã cài đặt và cấu hình kubectl trên thiết bị của bạn. vui lòng tham khảo tại đây nếu bạn chưa rõ cách cài đặt và sử dụng kuberctl. Ngoài ra, bạn không nên sử dụng phiên bản kubectl quá cũ, chúng tôi khuyến cáo bạn nên sử dụng phiên bản kubectl sai lệch không quá một phiên bản với version của cluster.\\n\\nKhởi tạo một Cluster sử dụng Calico Overlay\\n\\nĐể khởi tạo một Cluster, hãy làm theo các bước bên dưới:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn Activate.\\n\\nBước 3: Chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn. Sau khi Activate thành công, bạn hãy chọn Create a Cluster.\\n\\nBước 4: Tại màn hình khởi tạo Cluster, chúng tôi đã thiết lập thông tin cho Cluster và một Default Node Group cho bạn. Để sử dụng Calico Overlay cho Cluster của bạn, vui lòng chọn:\\n\\nNetwork type: Calico Overlay\\n\\nField Ý nghĩa Ví dụ minh họa VPC Dải địa chỉ IP mà các node của Cluster sẽ sử dụng để giao tiếp. Trong hình, chúng tôi lựa chọn VPC có IP range là 10.111.0.0/16 , tương ứng với 65536 IP Subnet Dải địa chỉ IP nhỏ hơn thuộc VPC. Mỗi node trong Cluster sẽ được gán một IP từ Subnet này. Subnet phải nằm trong dải IP của VPC đã chọn. Trong hình, chúng tôi lựa chọn Subnet có Primary IP range là 10.111.0.0/24 , tương ứng với 256 IP IP-IP encapsulation mode Chế độ IP-IP encapsulation trong VKS là Always Trong hình, chúng tôi lựa chọn chế độ Always để luôn encapsulate các gói tin. CIDR Dải mạng ảo mà các pod sẽ sử dụng Trong hình, chúng tôi lựa chọn dải mạng ảo là 172.16.0.0/16 . Các pod sẽ lấy IP từ dải IP này.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nChỉ một loại networktype: Trong một cluster, bạn chỉ có thể sử dụng một trong ba loại networktype: Calico Overlay, Cilium Overlay, hoặc Cilium VPC Native Routing\\n\\nMultiple subnet cho một cluster: VKS hỗ trợ việc sử dụng nhiều subnet cho một cluster. Điều này cho phép bạn cấu hình mỗi node group trong cluster nằm ở các subnet khác nhau trong cùng một VPC, giúp tối ưu hóa việc phân bổ tài nguyên và quản lý mạng. {% endhint %}\\n\\nBước 5: Chọn Create Kubernetes cluster. Hãy chờ vài phút để chúng tôi khởi tạo Cluster của bạn, trạng thái của Cluster lúc này là Creating.\\n\\nBước 6: Khi trạng thái Cluster là Active, bạn có thể xem thông tin Cluster, thông tin Node Group bằng cách chọn vào Cluster Name tại cột Name.\\n\\nDeploy một Workload\\n\\nBên dưới là hướng dẫn triển khai một deployment nginx và kiểm tra việc phân chia IP cho các pod được triển khai trong cluster của bạn.\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/k8s-cluster\\n\\nBước 2: Danh sách Cluster được hiển thị, chọn biểu tượng Download và chọn Download Config File để thực hiện tải xuống file kubeconfig. File này sẽ giúp bạn có toàn quyền truy cập vào Cluster của bạn.\\n\\nBước 3: Đổi tên file này thành config và lưu nó vào thư mục \\\\~/.kube/config\\n\\nBước 4: Thực hiện kiểm tra Cluster thông qua lệnh:\\n\\nChạy câu lệnh sau đây để kiểm tra node\\n\\nbash kubectl get nodes\\n\\nNếu kết quả trả về như bên dưới tức là bạn Cluster của bạn được khởi tạo thành công với 5 node:\\n\\nbash NAME STATUS ROLES AGE VERSION vks-cluster01-nodegroup-536d9-452f1 Ready <none> 15h v1.28.8 vks-cluster01-nodegroup-998b1-14f64 Ready <none> 16h v1.28.8 vks-cluster01-nodegroup01-22e98 Ready <none> 19h v1.28.8 vks-cluster01-nodegroup01-36911 Ready <none> 19h v1.28.8 vks-cluster01-nodegroup01-9102e Ready <none> 19h v1.28.8\\n\\nTiếp tục thực hiện chạy lệnh sau đây để kiểm tra các pod đã được triển khai trên namespace kube-system của bạn:\\n\\nkubectl get pods -A\\n\\nNếu kết quả trả về như bên dưới tức là các pods hỗ trợ chạy Calico Overlay đã running:\\n\\n```bash NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-868b574465-wbxlx 1/1 Running 0 19h kube-system calico-node-65ql2 1/1 Running 0 16h kube-system calico-node-d9hc7 1/1 Running 0 19h kube-system calico-node-gp2s7 1/1 Running 0 19h kube-system calico-node-hgk86 1/1 Running 0 19h kube-system calico-node-vj9ts 1/1 Running 0 15h kube-system calico-typha-74d79bf5f6-zzdn9 1/1 Running 0 19h kube-system coredns-1727334072-86776977c9-l9tcp 1/1 Running 0 19h kube-system coredns-1727334072-86776977c9-xqcn9 1/1 Running 0 19h kube-system konnectivity-agent-bj7wc 1/1 Running 0 15h kube-system konnectivity-agent-fnm7j 1/1 Running 0 16h kube-system konnectivity-agent-gvnbl 1/1 Running 0 19h kube-system konnectivity-agent-jj764 1/1 Running 0 19h kube-system konnectivity-agent-vgmwf 1/1 Running 0 19h kube-system kube-proxy-8r85m 1/1 Running 0 15h kube-system kube-proxy-bddf5 1/1 Running 0 19h kube-system kube-proxy-kwskl 1/1 Running 0 16h kube-system kube-proxy-zv6m4 1/1 Running 0 19h kube-system kube-proxy-zw65v 1/1 Running 0 19h kube-system vngcloud-controller-manager-67cf7f868c-jc66k 1/1 Running 0 19h kube-system vngcloud-csi-controller-746b67bcb8-dn5d7 7/7 Running 0 19h kube-system vngcloud-csi-controller-746b67bcb8-hqm24 7/7 Running 0 19h kube-system vngcloud-csi-node-24nlb 3/3 Running 0 15h kube-system vngcloud-csi-node-fgxpg 3/3 Running 0 19h kube-system vngcloud-csi-node-q9npf 3/3 Running 0 19h kube-system vngcloud-csi-node-tw5sv 3/3 Running 0 19h kube-system vngcloud-csi-node-z2sk9 3/3 Running 0 16h kube-system vngcloud-ingress-controller-0 1/1 Running 0 19h\\n\\n```\\n\\nBước 2: Triển khai nginx trên cluster vừa khởi tạo:\\n\\nThực hiện khởi tạo tệp tin nginx-deployment.yaml với nội dung tương tự bên dưới:\\n\\nbash apiVersion: apps/v1 kind: Deployment metadata: name: nginx-app spec: selector: matchLabels: app: nginx replicas: 20 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80\\n\\nThực hiện triển khai deployment này qua lệnh:\\n\\nbash kubectl apply -f nginx-deployment.yaml\\n\\nBước 3: Kiểm tra các pod nginx đã được triển khai và địa chỉ IP được gán cho mỗi pod\\n\\nThực hiện kiểm tra các pod qua lệnh:\\n\\nbash kubectl get pods -o wide\\n\\nBạn có thể quan sát bên dưới, các pod nginx được gán các IP 172.16.x.x thỏa mãn điều kiện Calico CIDR 172.16.0.0/16 mà chúng tôi đã chỉ định bên trên:\\n\\nbash NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-app-7c79c4bf97-2xbwd 1/1 Running 0 49s 172.16.197.136 vks-cluster01-nodegroup01-22e98 <none> <none> nginx-app-7c79c4bf97-5hcds 1/1 Running 0 49s 172.16.197.137 vks-cluster01-nodegroup01-22e98 <none> <none> nginx-app-7c79c4bf97-5hgwp 1/1 Running 0 49s 172.16.197.138 vks-cluster01-nodegroup01-22e98 <none> <none> nginx-app-7c79c4bf97-5l79h 1/1 Running 0 49s 172.16.83.4 vks-cluster01-nodegroup01-36911 <none> <none> nginx-app-7c79c4bf97-5q2f4 1/1 Running 0 49s 172.16.226.196 vks-cluster01-nodegroup01-9102e <none> <none> nginx-app-7c79c4bf97-5szc6 1/1 Running 0 49s 172.16.83.6 vks-cluster01-nodegroup01-36911 <none> <none> nginx-app-7c79c4bf97-9272q 1/1 Running 0 49s 172.16.167.71 vks-cluster01-nodegroup-998b1-14f64 <none> <none> nginx-app-7c79c4bf97-cgwrj 1/1 Running 0 49s 172.16.67.195 vks-cluster01-nodegroup-536d9-452f1 <none> <none> nginx-app-7c79c4bf97-fhlg4 1/1 Running 0 49s 172.16.167.70 vks-cluster01-nodegroup-998b1-14f64 <none> <none> nginx-app-7c79c4bf97-fj865 1/1 Running 0 49s 172.16.83.3 vks-cluster01-nodegroup01-36911 <none> <none> nginx-app-7c79c4bf97-gh6hj 1/1 Running 0 49s 172.16.167.69 vks-cluster01-nodegroup-998b1-14f64 <none> <none> nginx-app-7c79c4bf97-hx2rn 1/1 Running 0 49s 172.16.83.5 vks-cluster01-nodegroup01-36911 <none> <none> nginx-app-7c79c4bf97-jv26j 1/1 Running 0 49s 172.16.167.68 vks-cluster01-nodegroup-998b1-14f64 <none> <none> nginx-app-7c79c4bf97-km7p4 1/1 Running 0 49s 172.16.226.198 vks-cluster01-nodegroup01-9102e <none> <none> nginx-app-7c79c4bf97-lrh2r 1/1 Running 0 49s 172.16.167.67 vks-cluster01-nodegroup-998b1-14f64 <none> <none> nginx-app-7c79c4bf97-lvj6g 1/1 Running 0 49s 172.16.67.196 vks-cluster01-nodegroup-536d9-452f1 <none> <none> nginx-app-7c79c4bf97-nhhdk 1/1 Running 0 49s 172.16.226.197 vks-cluster01-nodegroup01-9102e <none> <none> nginx-app-7c79c4bf97-qr2lm 1/1 Running 0 49s 172.16.67.198 vks-cluster01-nodegroup-536d9-452f1 <none> <none> nginx-app-7c79c4bf97-x4ztb 1/1 Running 0 49s 172.16.226.195 vks-cluster01-nodegroup01-9102e <none> <none> nginx-app-7c79c4bf97-xrqwx 1/1 Running 0 49s 172.16.67.197 vks-cluster01-nodegroup-536d9-452f1 <none> <none>\\n\\nBạn cũng có thể thực hiện xem mô tả chi tiết mỗi pod để kiểm tra thông tin pod này qua lệnh:\\n\\nbash kubectl describe pod nginx-app-7c79c4bf97-2xbwd'),\n",
       " Document(metadata={'source': './../../data/vks/network/lam-viec-voi-application-load-balancer-alb/ingress-for-an-application-load-balancer.md'}, page_content='Ingress for an Application Load Balancer\\n\\nĐể cho tài nguyên Ingress (Ingress Yaml file) có thể hoạt động được, cluster phải có 1 VNGCloud LoadBalancer Controller đang chạy. Không giống các loại Controller khác được chạy như là 1 phần của kube-controller-manager. VNGCloud LoadBalancer Controller không được tự động khởi động cùng với cluster. Hãy làm theo hướng dẫn sau đây để cài đặt VNGCloud LoadBalancer Controller cũng như làm việc với Ingress Yaml file.\\n\\nChuẩn bị\\n\\nTạo một Kubernetes cluster trên VNGCloud, hoặc sử dụng một cluster đã có. Lưu ý: đảm bảo bạn đã tải xuống cluster configuration file sau khi cluster được khởi tạo thành công và truy cập vào cluster của bạn.\\n\\nKhởi tạo hoặc sử dụng một service account đã tạo trên IAM và gắn policy: vLBFullAccess, vServerFullAccess. Để tạo service account bạn truy cập tại đây và thực hiện theo các bước sau:\\n\\nChọn \"Create a Service Account\", điền tên cho Service Account và nhấn Next Step để gắn quyền cho Service Account\\n\\nTìm và chọn Policy: vLBFullAccess và Policy: vServerFullAccess, sau đó nhấn \"Create a Service Account\" để tạo Service Account, Policy: vLBFullAccess vàPolicy: vServerFullAccess do VNG Cloud tạo ra, bạn không thể xóa các policy này.\\n\\nSau khi tạo thành công bạn cần phải lưu lại Client_ID và Secret_Key của Service Account để thực hiện bước tiếp theo.\\n\\nThay đổi thông tin Security Group để cho phép các ALB có thể kết nối được tới các Node trong Node Group của bạn. Bạn cần thay đổi chúng trên vServer Portal khi:\\n\\nSecurity Group được gắn vào Cluster/Node Group của bạn khác với thông số mặc định mà chúng tôi đã tạo.\\n\\nBạn cần thay đổi mức độ bảo mật cho Cluster của mình hoặc bạn cần mở thêm cổng cho các dịch vụ cụ thể hoạt động trên Cluster. Chi tiết tham khảo tại đây.\\n\\nKhởi tạo Service Account và cài đặt VNGCloud LoadBalancer Controller\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nKhi bạn thực hiện khởi tạo Cluster theo hướng dẫn bên trên, nếu bạn chưa bật option Enable vLB Native Integration Driver, mặc định chúng tôi sẽ không cài sẵn plugin này vào Cluster của bạn. Bạn cần tự thực hiện Khởi tạo Service Account và cài đặt VNGCloud LoadBalancer Controller theo hướng dẫn bên dưới. Nếu bạn đã bật option Enable vLB Native Integration Driver, thì chúng tôi đã cài sẵn plugin này vào Cluster của bạn, hãy bỏ qua bước Khởi tạo Service Account, cài đặt VNGCloud LoadBalancer Controller và tiếp tục thực hiện theo hướng dẫn kể từ Deploy một Workload. {% endhint %}\\n\\nDeploy một Workload\\n\\nSau đây là hướng dẫn để bạn deploy service nginx trên Kubernetes.\\n\\nBước 1: Tạo Deployment cho Nginx app.\\n\\nTạo file nginx-service-lb7.yaml với nội dung sau:\\n\\n``` apiVersion: apps/v1 kind: Deployment metadata: name: nginx-app spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.19.1 ports: - containerPort: 80\\n\\napiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app: nginx type: NodePort ports: - protocol: TCP port: 80 targetPort: 80 ```\\n\\nDeploy Deployment này bằng lệch:\\n\\nkubectl apply -f nginx-service-lb7.yaml\\n\\nBước 2: Kiểm tra thông tin Deployment, Service vừa deploy\\n\\nChạy câu lệnh sau đây để kiểm tra Deployment\\n\\nkubectl get svc,deploy,pod -owide\\n\\nNếu kết quả trả về như bên dưới tức là bạn đã deploy Deployment thành công.\\n\\n``` NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1\\n\\nNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-app 1/1 1 1 2m50s nginx nginx:1.19.1 app=nginx\\n\\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-app-7f45b65946-6wlgw 1/1 Running 0 2m49s 172.16.54.3 ng-e0fc7245-0c6e-4336-abcc-31a70eeed71d-972a9\\n\\nTạo Ingress Resource\\n\\n1.Nếu bạn chưa có sẵn một Application Load Balancer nào đã khởi tạo trước đó trên hệ thống vLB.\\n\\nLúc này, khi tạo một Ingress, bạn hãy để trống thông tin Load Balancer ID tại annotation vks.vngcloud.vn/load-balancer-id.\\n\\nVí dụ, giả sử bạn đã deployment một service có tên nginx-service. Lúc này, bạn có thể tạo file nginx-ingress.yaml như sau:\\n\\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-ingress spec: ingressClassName: \"vngcloud\" defaultBackend: service: name: nginx-service port: number: 80 rules: - http: paths: - path: /4 pathType: Exact backend: service: name: nginx-service port: number: 80\\n\\nChạy câu lệnh sau đây để triển khai Ingress\\n\\nkubectl apply -f nginx-ingress.yaml\\n\\nSau khi bạn đã thực hiện triển khai Ingress , Chúng tôi sẽ tự động tạo 1 ALB trên cluster của bạn. ALB này sẽ hiển thị trên vLB Portal, chi tiết truy cập tại đây. ALB này sẽ có thông tin mặc định:\\n\\nThành phần Số lượng Thuộc tính ALB Package 1 VNG ALB_Small Listener 2 1 listener với protocol HTTP và port 80 1 listener với protocol HTTPS và port 443 Pool 1 1 pool default protocol HTTP và algorithm ROUND ROBIN Health Check 1 Sử dụng TCP để health check các member.\\n\\nVí dụ:\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nHiện tại Ingress chỉ hỗ trợ duy nhất TLS port 443 và là điểm kết thúc cho TLS (TLS termination). TLS Secret phải chứa các trường với tên key là tls.crt và tls.key, đây chính là certificate và private key để sử dụng cho TLS. Nếu bạn muốn sử dụng Certificate cho một host, hãy thực hiện tải lên Certificate theo hướng dẫn tại [Upload a certificate] và sử dụng chúng như một annotation. Ví dụ:\\n\\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress annotations: # kubernetes.io/ingress.class: \"vngcloud\" # this annotation is deprecated will cause warning, can use option `ingressClassName` below instead vks.vngcloud.vn/certificate-ids: \"secret-a6d20ec6-f3e5-499a-981b-b1484e340cec\" spec: ingressClassName: \"vngcloud\" defaultBackend: service: name: apache-service port: number: 80 tls: - hosts: - host.example.com rules: - host: host.example.com http: paths: - path: /4 pathType: Exact backend: service: name: nginx-service port: number: 80 {% endhint %}\\n\\n2.Nếu bạn đã có sẵn một Application Load Balancer đã khởi tạo trước đó trên hệ thống vLB và bạn muốn tái sử dụng ALB cho cluster của bạn.\\n\\nLúc này, khi tạo một Ingress, bạn hãy nhập thông tin Load Balancer ID vào annotation vks.vngcloud.vn/load-balancer-id. Ví dụ, trong trường hợp này tôi đã tái sử dụng ALB có ID = lb-2b9d8974-3760-4d60-8203-9671f229fb96:\\n\\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress annotations: # kubernetes.io/ingress.class: \"vngcloud\" # this annotation is deprecated will cause warning, can use option `ingressClassName` below instead. vks.vngcloud.vn/load-balancer-id: \"lb-2b9d8974-3760-4d60-8203-9671f229fb96\" vks.vngcloud.vn/certificate-ids: \"secret-a6d20ec6-f3e5-499a-981b-b1484e340cec\" spec: ingressClassName: \"vngcloud\" defaultBackend: service: name: apache-service port: number: 80 tls: - hosts: - host.example.com rules: - host: host.example.com http: paths: - path: /4 pathType: Exact backend: service: name: nginx-service port: number: 80\\n\\nSau khi bạn đã thực hiện tạo ingress theo hướng dẫn tại Ingress for an Application Load Balancer. Nếu:\\n\\nALB của bạn đang có sẵn 2 listener trong đó:\\n\\n1 listener có cấu hình protocol HTTP và port 80\\n\\n1 listener có cấu hình protocol HTTPS và port 443 thì chúng tôi sẽ sử dụng 2 listener này.\\n\\nALB của bạn chưa có một trong hai hoặc cả 2 listener có cấu hình trên, chúng tôi sẽ tự động khởi tạo chúng.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nNếu ALB của bạn có:\\n\\n1 listener có cấu hình protocol HTTP và port 443\\n\\nHoặc 1 listener có cấu hình protocol HTTPS và portal 80\\n\\nthì khi tạo Ingress sẽ xảy ra lỗi. Lúc này, bạn cần chỉnh sửa lại thông tin listener hợp lệ trên hệ thống vLB và thực hiện tạo lại ingress. {% endhint %}\\n\\n3. Sau khi tạo ingress thành công với một ALB, bạn có thể thực hiện\\n\\nChỉnh sửa cấu hình ingress của bạn theo hướng dẫn cụ thể tại Configure for an Application Load Balancer.\\n\\nHoặc bạn có thể thêm/ sửa/ xóa policy trong ALB của bạn bằng các chỉnh sửa các thông số sau trong tài nguyên ingress (Ingress Yaml file). Ví dụ như bên dưới, tôi đã thực hiện thiết lập 2 rule như sau:\\n\\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-ingress annotations: vks.vngcloud.vn/certificate-ids: \"secret-58542bfb-f410-4095-9e1c-34cd39995c6d, secret-893b67e2-0286-434d-9e11-a8af997aefb8\" spec: ingressClassName: \"vngcloud\" defaultBackend: service: name: nginx-service port: number: 80 tls: - hosts: - \\'*.example.com\\' - \\'example.com\\' rules: - host: example.com http: paths: - path: /1 pathType: Exact backend: service: name: nginx-service1 port: number: 80 - http: paths: - path: /2 pathType: Exact backend: service: name: nginx-service2 port: number: 80\\n\\nCũng giống như các tài nguyên Kubernetes khác, Ingress có cấu trúc gồm các trường thông tin như sau:\\n\\napiVersion: Phiên bản API cho Ingress.\\n\\nkind: Loại tài nguyên, trong trường hợp này là \"Ingress\".\\n\\ningressClassName: bạn cần chỉ định giá trị field này là \"vngcloud\" để sử dụng vngcloud-ingress-controller.\\n\\nmetadata: Thông tin mô tả Ingress, bao gồm tên, annotations.\\n\\nspec: Cấu hình Ingress, bao gồm các rule route traffic theo điều kiện của các incoming request. Tài nguyên Ingress chỉ hỗ trợ các rule để điều hướng HTTP traffic.\\n\\nĐể biết thông tin chung về cách làm việc với tài nguyên Ingress (Ingress Yaml file), hãy xem tại [Configure for an Application Load Balancer]).\\n\\nKiểm tra, chỉnh sửa Ingress resource đã tạo\\n\\nSau khi tạo ingress thành công, bạn có thể xem danh sách ingress qua lệnh\\n\\nkubectl get ingress\\n\\nVí dụ, bên dưới chúng tôi đã tạo thành công nginx-ingress:\\n\\nNAME CLASS HOSTS ADDRESS PORTS AGE nginx-ingress vngcloud * 180.93.181.129 80 103m\\n\\nHoặc xem chi tiết một ingress bằng cách\\n\\nkubectl describe ingress nginx-ingress\\n\\nVí dụ, bên dưới là thông tin chi tiết của nginx-ingress mà tôi đã tạo:\\n\\nName: nginx-ingress Labels: <none> Namespace: default Address: 180.93.181.129 Ingress Class: vngcloud Default backend: nginx-service:80 (172.16.24.202:80) Rules: Host Path Backends ---- ---- -------- * /path1 nginx-service:80 (172.16.24.202:80) Annotations: vks.vngcloud.vn/load-balancer-id: lb-6cdea8fd-4589-410e-933f-c3bc46fa9d25 Events: <none>\\n\\nĐể cập nhật nginx-ingress hiện có, ta có thể thực hiện bằng cách cập nhật Ingress Yaml file như sau:\\n\\nkubectl edit ingress nginx-ingress\\n\\nĐể truy cập vào app nginx, bạn có thể sử dụng Endpoint của Load Balancer mà hệ thống đã tạo.\\n\\nhttp://Endpoint/\\n\\nBạn có thể lấy thông tin Public Endpoint của Load Balancer tại giao diện vLB. Cụ thể truy cập tại\\n\\nVí dụ, bên dưới tôi đã truy cập thành công vào app nginx với địa chỉ : http://180.93.181.129/'),\n",
       " Document(metadata={'source': './../../data/vks/network/lam-viec-voi-application-load-balancer-alb/cau-hinh-cho-mot-application-load-balancer.md'}, page_content='Cấu hình cho một Application Load Balancer\\n\\nTại trang [Ingress for an Application Load Balancer], chúng tôi đã hướng dẫn bạn cách thực hiện cài đặt VNGCloud LoadBalancer Controller và tạo ingress thông qua Ingress Yaml file. Sau đây là chi tiết các ý nghĩa các thông tin bạn có thể thiết lập cho một Ingress\\n\\nAnnotation\\n\\nSử dụng các annotation dưới đây khi thực thiện tạo ingress để tuỳ chỉnh Load Balancer phù hợp với nhu cầu của bạn:\\n\\nAnnotation Bắt buộc/ Không bắt buộc Ý nghĩa vks.vngcloud.vn/load-balancer-id Không bắt buộc Nếu bạn chưa có sẵn một Application Load Balancer đã khởi tạo trước đó trên hệ thống vLB. Lúc này, khi tạo một Ingress, bạn hãy để trống thông tin này. Sau khi bạn đã thực hiện triển khai Ingress theo hướng dẫn tại Ingress for an Application Load Balancer . Chúng tôi sẽ tự động tạo 1 ALB trên cluster của bạn. ALB này sẽ hiển thị trên vLB Portal, chi tiết truy cập tại đây Nếu bạn đã có sẵn một Application Load Balancer đã khởi tạo trước đó trên hệ thống vLB và bạn muốn tái sử dụng ALB cho cluster của bạn. Lúc này, khi tạo một Ingress, bạn hãy nhập thông tin Load Balancer ID vào annotation này. Sau khi bạn đã thực hiện tạo Ingress theo hướng dẫn tại Ingress for an Application Load Balancer . Nếu: ALB của bạn đang có sẵn 2 listener trong đó: 1 listener có cấu hình protocol HTTP và port 80 1 listener có cấu hình protocol HTTPS và port 443 thì chúng tôi sẽ sử dụng 2 listener này. ALB của bạn chưa có một trong hai hoặc cả 2 listener có cấu hình trên, chúng tôi sẽ tự động khởi tạo chúng. Chú ý: Nếu ALB của bạn có: 1 listener có cấu hình protocol HTTP và port 443 Hoặc 1 listener có cấu hình protocol HTTPS và portal 80 thì khi tạo Ingress sẽ xảy ra lỗi. Lúc này, bạn cần chỉnh sửa lại thông tin listener hợp lệ trên hệ thống vLB và thực hiện tạo lại ingress. vks.vngcloud.vn/load-balancer-name Không bắt buộc Annotation vks.vngcloud.vn/load-balancer-name sẽ được sử dụng nếu bạn không sử dụng annotation load-balancer-id . Annotation vks.vngcloud.vn/load-balancer-name chỉ có ý nghĩa khi bạn tạo mới một Ingress resource. Sau khi Ingress resource được tạo thành công, annotation này sẽ tự động bị xóa . Việc sử dụng annotation này sau khi Ingress resource được tạo sẽ không có tác dụng . Khi bạn sử dụng annotation này, nếu bạn chưa có sẵn một Application Load Balancer đã khởi tạo trước đó trên hệ thống vLB. Chúng tôi sẽ tự động tạo 1 ALB trên cluster của bạn. ALB này sẽ hiển thị trên vLB Portal, chi tiết truy cập tại đây Nếu bạn đã có sẵn một Application Load Balancer đã khởi tạo trước đó trên hệ thống vLB và bạn muốn tái sử dụng ALB cho cluster của bạn. Lúc này, bạn hãy nhập thông tin Load Balancer Name vào annotation này. vks.vngcloud.vn/package-id Không bắt buộc Nếu bạn không nhập thông tin này thì mặc định chúng tôi sẽ sử dụng cấu hình ALB Small. Nếu bạn đã có sẵn host vLB đang ACTIVE và bạn muốn tích hợp host này vô cụm K8S của bạn, vui lòng bỏ qua trường thông tin này. vks.vngcloud.vn/tags Không bắt buộc Tag được gắn thêm cho ALB của bạn. vks.vngcloud.vn/scheme Không bắt buộc Mặc định internet-facing , bạn có thể đổi thành internal tùy theo nhu cầu sử dụng. vks.vngcloud.vn/security-groups Không bắt buộc Mặc định sẽ tạo một security group default theo Cluster của bạn. vks.vngcloud.vn/inbound-cidrs Không bắt buộc Mặc định All CIRD: 0.0.0.0/0 vks.vngcloud.vn/healthy-threshold-count Không bắt buộc Mặc định 3 vks.vngcloud.vn/unhealthy-threshold-count Không bắt buộc Mặc định 3 vks.vngcloud.vn/healthcheck-interval-seconds Không bắt buộc Mặc định 30 vks.vngcloud.vn/healthcheck-timeout-seconds Không bắt buộc Mặc định 5 vks.vngcloud.vn/healthcheck-protocol Không bắt buộc Mặc định TCP . Người dùng có thể chọn một trong các giá trị TCP/ HTTP vks.vngcloud.vn/healthcheck-http-method Không bắt buộc Mặc định GET . Người dùng có thể chọn một trong các giá trị GET / POST / PUT vks.vngcloud.vn/healthcheck-path Không bắt buộc Mặc định / vks.vngcloud.vn/healthcheck-http-version Không bắt buộc Mặc định 1.0 . Người dùng có thể chọn một trong các giá trị 1.0, 1.1 vks.vngcloud.vn/healthcheck-http-domain-name Không bắt buộc Mặc định trống vks.vngcloud.vn/healthcheck-port Không bắt buộc Mặc định traffic port vks.vngcloud.vn/success-codes Không bắt buộc Mặc định 200 vks.vngcloud.vn/idle-timeout-client Không bắt buộc Mặc định 50 vks.vngcloud.vn/idle-timeout-member Không bắt buộc Mặc định 50 vks.vngcloud.vn/idle-timeout-connection Không bắt buộc Mặc định 5 vks.vngcloud.vn/pool-algorithm Không bắt buộc Mặc định ROUND_ROBIN . Người dùng có thể chọn một trong các giá trị ROUND_ROBIN / LEAST_CONNECTIONS / SOURCE_IP vks.vngcloud.vn/enable-sticky-session Không bắt buộc Mặc định false . vks.vngcloud.vn/enable-tls-encryption Không bắt buộc Mặc định false vks.vngcloud.vn/target-node-labels Không bắt buộc Mặc định trống vks.vngcloud.vn/certificate-ids Không bắt buộc Mặc định trống vks.vngcloud.vn/is-poc Không bắt buộc Mặc định false. Nếu người dùng chỉ định field này là true, hệ thống sẽ tạo Load Balancer và thực hiện thanh toán bởi số dư ví POC. vks.vngcloud.vn/enable-autoscale Không bắt buộc Mặc định false. Nếu người dùng chỉ định field này là true, hệ thống sẽ tạo Load Balancer với mode autoscale được bật. vks.vngcloud.vn/ignore Không bắt buộc Mặc định false. Nếu người dùng chỉ định field này là false, operator của chúng tối sẽ không quản lý Service và Ingress. Mọi thay đổi đối với resource sẽ bị bỏ qua và LoadBalancer sẽ không được cập nhật. vks.vngcloud.vn/implementation-specific-params Không bắt buộc Mặc định trống Theo mặc định, ALB policy chỉ có 1 rule dành cho host và path, với các thao tác giới hạn như EQUAL_TO (Exact) và START_WITH (Prefix) . Nếu người dùng muốn tạo thêm policy hoặc sử dụng các thao tác khác như ENDS_WITH hoặc REGEX , cần thay đổi pathType thành ImplementationSpecificParams và sử dụng annotation với giá trị JSON để cấu hình. Ví dụ: [{\\n  \"path\": \"/haha\",\\n  \"rules\": [\\n    {\\n      \"type\": \"PATH\",\\n      \"compare\": \"EQUAL_TO\",\\n      \"value\": \"/foo#\"\\n    },\\n    {\\n      \"type\": \"PATH\",\\n      \"compare\": \"REGEX\",\\n      \"value\": \"/foo#anchor\"\\n    }\\n  ],\\n  \"action\": {\\n    \"action\": \"REJECT\",\\n    \"redirectUrl\": \"http://golang.cafe/a\",\\n    \"redirectHttpCode\": 301\\n  }\\n}] vks.vngcloud.vn/header Không bắt buộc Mặc định: {\"http\":[\"X-Forwarded-For\", \"X-Forwarded-Proto\", \"X-Forwarded-Port\"],\"https\":[\"X-Forwarded-For\", \"X-Forwarded-Proto\", \"X-Forwarded-Port\"]} vks.vngcloud.vn/client-certificate-id Không bắt buộc Mặc định trống Client CA là một tính năng bảo mật của Load Balancer giúp xác thực client bằng cách sử dụng client certificate để cho phép các client được ủy quyền truy cập vào ứng dụng hoặc dịch vụ. Truyền một certificate id trong portal vào annotation này sẽ kích hoạt Client CA cho https listener .\\n\\nIngressClassName\\n\\nCác Ingress được cài đặt bởi các VNGCloud LoadBalancer Controller sẽ có thông tin IngressClassName = \"vngcloud\". Bạn không được thay đổi thông tin này.\\n\\nDefaultBackend\\n\\nMột Ingress không có rule nào sẽ gửi tất cả traffic đến 1 service default backend mặc định duy nhất hoặc nếu không có host và path nào khớp với HTTP request trong Ingress Yaml file thì traffic sẽ được route đến service default backend. Ví dụ như bên dưới, chúng tôi đang cấu hình mặc định nếu request không thỏa mãn rule nào trong Ingress yaml file thì sẽ đi vào service name: example-svc-1 với port number 8080\\n\\ndefaultBackend: service: name: example-svc-1 port: number: 8080\\n\\nTLS\\n\\nBạn có thể bảo mật Ingress bằng cách chỉ định 1 Secret có chứa TLS key và certificate. Hiện tại Ingress chỉ hỗ trợ duy nhất TLS port 443 và là điểm kết thúc cho TLS (TLS termination). TLS Secret phải chứa các trường với tên key là tls.crt và tls.key, đây chính là certificate và private key để sử dụng cho TLS. Cụ thể, bạn cần chỉ định:\\n\\nHost: các host được chỉ định sẽ dùng cert.\\n\\nSecretName: tên secret chứa cert.\\n\\nPath types\\n\\nMỗi path (đường dẫn) trong Ingress có một pathType (loại đường dẫn) tương ứng. Có ba pathType được hỗ trợ:\\n\\nExact: Khớp với đường dẫn URL một cách chính xác tuyệt đối và phân biệt chữ hoa chữ thường.\\n\\nPrefix: Khớp dựa trên tiền tố của đường dẫn URL được phân tách bởi dấu /. Việc so khớp (match) là có phân biệt chữ hoa thường và được thực hiện trên từng thành phần của đường dẫn URL. Một thành phần của đường dẫn URL chính là 1 label được phân tách bằng dấu phân cách / trong đường dẫn URL (Nghĩa là đường dẫn URL có thể bao gồm nhiều cấp phân tách nhau bởi dẫu /, mỗi chuỗi đứng giữa 2 dấu / chính là 1 label, mỗi label chính là 1 thành phần của đường dẫn URL). Một request URL được xem như là khớp với 1 trường path (được cấu hình trong đặc tả Ingress) khi toàn bộ giá trị của path (có thể gồm nhiều thành phần phân tách bằng dấu /) khớp với các label đầu tiên (tính từ bên trái của đường dẫn URL). Ví dụ /example1/path1 khớp với /example1/path1/path2, nhưng không khớp với /example1/path1path2\\n\\nVí dụ cụ thể:\\n\\nPath type Path(s) Request path(s) Có match hay không? Exact /example1 /example1 Có Exact /example1 /host1 Không Exact /example1 /example1/ Không Exact /example1/ /example1 Không Prefix / (all paths) Có Prefix /example1 /example1 , /example1/ Có Prefix /example1/ /example1 , /example1/ Có Prefix /example1/host11 /example1/host1 Không Prefix /example1/host1 /example1/host1 Có Prefix /example1/host1/ /example1/host1 Có Prefix /example1/host1 /example1/host1/ Có Prefix /example1/host1 /example1/host1/ccc Có Prefix /example1/host1 /example1/host1xyz Không Prefix / , /example1 /example1/ccc Có Prefix / , /example1 , /example1/host1 /example1/host1 Có Prefix / , /example1 , /example1/host1 /ccc Có Prefix /example1 /ccc Không\\n\\nTrong một số trường hợp, nhiều path bên trong Ingress sẽ khớp với đường dẫn của request URL. Trong những trường hợp đó, quyền ưu tiên sẽ được trao cho path khớp dài nhất đầu tiên. Nếu hai path vẫn có độ dài khớp bằng nhau thì quyền ưu tiên sẽ theo thứ tự rule được tạo trên Ingress Yaml file.\\n\\nIngress rule\\n\\nMỗi HTTP rule chứa các thông tin sau:\\n\\n1 host tùy chọn. Nếu không có host (ta có thể hiểu là 1 tên miền) nào được chỉ định nên rule sẽ được áp dụng cho tất cả HTTP traffic đi vào (inbound) địa chỉ IP đã được chỉ định. Nếu có chỉ định host (ví dụ example1.com) thì rule chỉ áp dụng cho host đó thôi.\\n\\n1 danh sách các đường dẫn (path) (ví dụ /example1/host1), mỗi path có 1 service backend gắn liền với nó được định nghĩa bởi Service Name và Port Number. Cả host và path phải khớp với nội dung của incoming request trước khi bộ cân bằng tải điều hướng traffic đến Services mong muốn.\\n\\n1 backend là tổ hợp của tên Services và Port Number. HTTP và HTTPS request đi đến Ingress và có URL khớp với host và path của rule sẽ được gửi đến danh sách các backend.\\n\\nVí dụ: Host có khớp với Host header theo bảng:\\n\\nHost Host header Có match hay không? *. example1.com example2.example1.com Có *. example1.com baz.example2.example1.com Không *. example1.com example1.com Không'),\n",
       " Document(metadata={'source': './../../data/vks/network/lam-viec-voi-application-load-balancer-alb/README.md'}, page_content='Làm việc với Application Load Balancer (ALB)\\n\\nTổng quan\\n\\nALB là gì?\\n\\nApplication Load Balancer (ALB) là một công cụ trong hạ tầng mạng và máy chủ được sử dụng để phân phối lưu lượng mạng đến nhiều máy chủ hoặc máy ảo để cải thiện hiệu suất và khả năng sẵn sàng của các ứng dụng. ALB hoạt động ở tầng ứng dụng, cho phép phân phối lưu lượng dựa trên nhiều yếu tố như loại yêu cầu, trạng thái của máy chủ và thuật toán phân phối tải. ALB cung cấp khả năng route nâng cao, cho phép điều hướng lưu lượng dựa trên các Host hay Path Header. Nó cũng hỗ trợ tính năng duy trì phiên, giúp duy trì liên tục phiên của người dùng đối với cùng một máy chủ. Điều này rất hữu ích cho các ứng dụng yêu cầu sự nhất quán trong quá trình tương tác của người dùng. Để biết thêm thông tin chi tiết về ALB, vui lòng tham khảo tại [How it works (ALB)]\\n\\nMô hình triển khai\\n\\nNgoài các thành phần cơ bản của một cụm K8S và một ALB mà bạn đã biết, trong mô hình này chúng tôi sử dụng:\\n\\nIngress: là tài nguyên trong Kubernetes được cấu hình để cho các Services có thể truy cập được từ bên ngoài k8s cluster thông qua URL, ngoài ra cũng có thể cân bằng tải traffic, hỗ trợ kết nối SSL/TLS và cung cấp virtual hosting dựa theo tên. Một Ingress không expose tùy tiện các protocol ngoài HTTP và HTTPS. Ingress đóng vai trò như một điểm vào duy nhất cho các request HTTP và HTTPS từ bên ngoài cluster đến các service bên trong. Việc route traffic được kiểm soát bởi các quy tắc (rule) được định nghĩa trong tài nguyên Ingress (Ingress Yaml File). Một Ingress được quản lý bởi VNGCloud Ingress Controller: là một ứng dụng chạy trong cluster và quản lý các tài nguyên Ingress dựa trên Ingress Yaml File do khách hàng định nghĩa'),\n",
       " Document(metadata={'source': './../../data/vks/network/lam-viec-voi-application-load-balancer-alb/gioi-han-va-han-che-alb.md'}, page_content='Giới hạn và hạn chế ALB\\n\\nGiới hạn\\n\\nMột vài lưu ý về giới hạn của việc ingress an ALB vào một cluster:\\n\\nMột cluster có thể có nhiều Ingress, mỗi Ingress có chứa thông tin tài nguyên cho một ALB duy nhất.\\n\\nMột ALB có thể được sử dụng chung cho nhiều Ingress. Từ đó một ALB có thể được sử dụng chung cho nhiều cluster nhưng phải đảm bảo các cluster này có chung VPC.\\n\\nMột ALB có thể bao gồm nhiều listener, nhiều pool, nhiều policy. Các giới hạn về số lượng listener, số lượng pool, số lượng policy vui lòng tham khảo tại [Hạn mức tài nguyên]\\n\\nHạn chế\\n\\nIngress controller manager cần phải cấu hình các annotation để chỉ định các thuộc tính của ALB, như protocol, port,... Các annotation này có thể khác nhau tùy theo nhà cung cấp dịch vụ đám mây, hiện tại với VNGCloud LoadBalancer Controller đang cung cấp danh sách annotation tham khảo tại cau-hinh-cho-mot-application-load-balancer.md. Chúng tôi sẽ nâng cấp thêm phần này trong các phiên bản release kế tiếp.\\n\\nHiện tại, vLB chưa hỗ trợ tính năng strip path ở Load Balancer Layer 7, chúng tôi sẽ sớm tích hợp tính năng này trong tương lai.\\n\\nViệc thay đổi tên hoặc kích thước (Rename, Resize) tài nguyên Load Balancer trên vServer Portal có thể gây ra sự không tương thích với tài nguyên trên Kubernetes Cluster. Điều này có thể dẫn đến việc các tài nguyên không hoạt động trên Cluster, hoặc tài nguyên bị đồng bộ lại hoặc thông tin tài nguyên giữa vServer Portal và Cluster không khớp. Để ngăn chặn vấn đề này, hãy sử dụng kubectlđể quản lý tài nguyên của Cluster.'),\n",
       " Document(metadata={'source': './../../data/vks/thong-bao-va-cap-nhat/README.md'}, page_content='Annoucements and Updates'),\n",
       " Document(metadata={'source': './../../data/vks/thong-bao-va-cap-nhat/release-notes.md'}, page_content='Release notes\\n\\nDec 5, 2024\\n\\nVKS (VNGCloud Kubernetes Service) vừa ra mắt bản cập nhật mới nhất, mang đến nhiều tính năng mới cho người dùng. Dưới đây là những điểm nổi bật của bản cập nhật:\\n\\nTính năng mới:\\n\\nForce-Upgrade, Auto-Upgrade: Tự động nâng cấp phiên bản Kubernetes cho cluster/ node group theo lịch trình hoặc khi phiên bản hiện tại sắp hết hạn.. Chi tiết tham khảo thêm tại đây.\\n\\nOct 23, 2024\\n\\nVKS (VNGCloud Kubernetes Service) vừa ra mắt bản cập nhật mới nhất, mang đến nhiều cải tiến cho người dùng. Dưới đây là những điểm nổi bật của bản cập nhật:\\n\\nCải tiến:\\n\\nHỗ trợ POC/ Stop POC cho Cluster: Người dùng giờ đây có thể thực hiện POC/ Stop POC cho các tài nguyên trên VKS như Server, Volume, Load Balancer, Endpoint. Tính năng này mang đến sự linh hoạt cao cho người dùng khi muốn trải nghiệm với VKS. Chi tiết tham khảo thêm tại đây.\\n\\nNâng cấp Plugin VNGCloud BlockStorage CSI Driver: Các lỗi đã được phát hiện trong các phiên bản trước đã được khắc phục, giúp hệ thống hoạt động mượt mà và tin cậy hơn.\\n\\nTự do lựa chọn/ chỉnh sửa cấu hình có/ không sử dụng plugin VNGCloud LoadBalancer Controller, Plugin VNGCloud Ingress Controller trên cụm VKS hiện có: Khả năng tùy chỉnh cấu hình plugin cho phép người dùng tối ưu hóa cụm VKS theo nhu cầu cụ thể của mình. Điều này giúp tăng tính linh hoạt và đáp ứng các yêu cầu đặc biệt của từng ứng dụng.\\n\\nNgoài ra, trong bản cập nhật này, chúng tôi cũng đã khắc phục một số lỗi nhỏ để mang đến trải nghiệm người dùng tốt hơn.\\n\\nOct 03, 2024\\n\\nVKS (VNGCloud Kubernetes Service) vừa ra mắt bản cập nhật mới nhất, mang đến nhiều tính năng và cải tiến cho người dùng. Dưới đây là những điểm nổi bật của bản cập nhật:\\n\\nTriển khai Region mới:\\n\\nBên cạnh Region HCM03, VKS hiện đã hỗ trợ thêm Region HAN01. Việc bổ sung này giúp khách hàng có thêm lựa chọn trong việc triển khai ứng dụng, đặc biệt hữu ích cho các doanh nghiệp có yêu cầu về vị trí đặt dữ liệu.\\n\\nTính năng mới:\\n\\nNetwork Type: Cilium Overlay, Cilium VPC Native Routing: Ngoài Calico Overlay, bản phát hành lần này chúng tôi đã bổ sung hai loại mạng mới: Cilium Overlay và Cilium VPC Native Routing. Cilium Overlay cho phép bạn xây dựng mạng overlay linh hoạt, trong khi Cilium VPC Native Routing tích hợp chặt chẽ với VPC của VNG Cloud, giúp tối ưu hiệu suất và bảo mật cho ứng dụng của bạn. Chi tiết tham khảo thêm tại đây.\\n\\nCải tiến:\\n\\nMultiple Subnet cho Cluster trên VKS: VKS giờ đây hỗ trợ sử dụng nhiều subnet cho một cluster. Điều này cho phép bạn cấu hình từng node group trong cluster nằm ở các subnet khác nhau trong cùng một VPC, tối ưu hóa việc phân bổ tài nguyên và quản lý mạng.\\n\\nChỉnh sửa Labels/Taints trên cụm VKS hiện có: Với khả năng chỉnh sửa trực tiếp Labels/Taints trên cụm VKS đã triển khai, bạn có thể điều khiển việc lên lịch Pod, áp dụng các chính sách khác nhau cho các Node Group, và tùy chỉnh quy tắc lựa chọn node cho các ứng dụng. Điều này giúp quản lý và phân loại tài nguyên hiệu quả hơn.\\n\\nEnable/Disable lựa chọn sử dụng Private Service Endpoint: Trước đây, khi tạo private cluster trên VKS, việc tạo private service endpoint là bắt buộc. Giờ đây, bạn có thể dễ dàng bật/tắt tính năng này, cho phép các dịch vụ trong cụm VKS giao tiếp qua địa chỉ IP nội bộ, tăng cường bảo mật và giảm thiểu rủi ro tấn công từ bên ngoài.\\n\\nEnable/Disable lựa chọn mã hóa Volume: Tính năng mã hóa Volume cho phép bạn bảo vệ dữ liệu nhạy cảm được lưu trữ trong các Persistent Volume của cụm VKS. Việc này đảm bảo an toàn dữ liệu và tuân thủ các quy định về bảo vệ thông tin. Giờ đây, bạn có thể bật/tắt tính năng mã hóa cho từng Volume theo nhu cầu.\\n\\nAug 28, 2024\\n\\nVKS (VNGCloud Kubernetes Service) giới thiệu bản cập nhật mới nhất cho VKS đã có sẵn, mang đến nhiều tính năng mới cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nTính năng mới:\\n\\nPrivate Cluster: Trước đây, các public cluster trên VKS đang sử dụng địa chỉ Public IP để giao tiếp giữa nodes và control plane. Để nâng cao bảo mật cho cluster của bạn, chúng tôi đã cho ra mắt mô hình private cluster. Tính năng Private Cluster giúp cho cụm K8S của bạn được bảo mật nhất có thể, mọi kết nối hoàn toàn là private từ kết nối giữa nodes tới control plane, kết nối từ client tới control plane, hay kết nối từ nodes tới các sản phẩm dịch vụ khác trong VNG Cloud như: vStorage, vCR, vMonitor, VNGCloud APIs,...Private Cluster là lựa chọn lý tưởng cho các dịch vụ yêu cầu kiểm soát truy cập chặt chẽ, đảm bảo tuân thủ các quy định về bảo mật và quyền riêng tư dữ liệu. Chi tiết 2 mô hình hoạt động của Cluster, bạn có thể tham khảo thêm tại đây và tham khảo các bước khởi tạo một private Cluster tại đây.\\n\\nAug 26, 2024\\n\\nVKS (VNGCloud Kubernetes Service) giới thiệu bản cập nhật mới nhất cho VKS đã có sẵn, mang đến nhiều cải tiến mới cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nCải tiến:\\n\\nKubernetes Version: VKS đã bổ sung các image mới nhằm tối ưu hóa size, feature, network,...so với các image cũ. Việc xây dựng các image này cũng nhằm phục vụ cho cả hai loại cluster là Public và Private mà VKS sắp ra mắt. Cụ thể, trong bản phát hành này, chúng tôi đã bổ sung thêm các image sau:\\n\\nUbuntu-22.kube_v1.27.12-vks.1724605200\\n\\nUbuntu-22.kube_v1.28.8-vks.1724605200\\n\\nUbuntu-22.kube_v1.29.1-vks.1724605200\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nĐể khởi tạo một Private Cluster, bạn cần chọn sử dụng một trong 3 image mới này. Đối với Public Cluster, bạn có thể chọn sử dụng bất kỳ image cũ hoặc mới tùy theo nhu cầu của bạn. {% endhint %}\\n\\nAug 13, 2024\\n\\nVKS (VNGCloud Kubernetes Service) giới thiệu bản cập nhật mới nhất cho VKS đã có sẵn, mang đến nhiều cải tiến mới cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nCải tiến:\\n\\nEvent History: VKS đã bổ sung thêm các sự kiện của Auto Scaling và Auto Healing. Giờ đây, với Event History, bạn có thể theo dõi từng thay đổi xảy ra trong Cluster, từ việc tự động điều chỉnh quy mô (Auto Scaling) cho đến việc tự động phục hồi (Auto Healing). Các event này giúp tăng cường khả năng giám sát và quản lý cụm Kubernetes của bạn.\\n\\nAug 01, 2024\\n\\nVKS (VNGCloud Kubernetes Service) giới thiệu bản cập nhật mới nhất cho VKS đã có sẵn, mang đến nhiều tính năng và cải tiến mới cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nTính năng mới:\\n\\nGiám sát tài nguyên VKS: Người dùng có thể theo dõi trực tiếp tình trạng hoạt động của Cluster, Node, tình trạng sử dụng CPU, RAM, Memory,... của Node thông qua các dashboard trực quan. Để hiển thị dữ liệu lên dashboard, người dùng cần cài đặt vmonitor-metric-agent vào cụm mong muốn thực hiện giám sát. Chi tiết tham khảo thêm tại đây.\\n\\nJuly 25, 2024\\n\\nVKS (VNGCloud Kubernetes Service) giới thiệu bản cập nhật mới nhất cho VKS đã có sẵn, mang đến nhiều cải tiến mới cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nCải tiến:\\n\\nNâng cấp quản lý VKS thông qua Terraform: Người dùng có thể đồng thời điều chỉnh số lượng node (Number of nodes) và thay đổi số lượng node cho autoscale (Minimum/ Maximum node Autoscale) ngay trong quá trình chỉnh sửa cấu hình. Với khả năng điều chỉnh nhiều thông số cùng lúc, việc quản lý cụm Kubernetes trở nên linh hoạt và thuận tiện hơn. Chi tiết tham khảo thêm ví dụ tại đây.\\n\\nJuly 23, 2024\\n\\nVKS (VNGCloud Kubernetes Service) giới thiệu bản cập nhật mới nhất cho VKS đã có sẵn, mang đến nhiều cải tiến mới cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nCải tiến:\\n\\nNâng cấp Plugin VNGCloud LoadBalancer Controller, Plugin VNGCloud Ingress Controller: Các lỗi đã được phát hiện trong các phiên bản trước đã được khắc phục, giúp hệ thống hoạt động mượt mà và tin cậy hơn.\\n\\nJuly 18, 2024\\n\\nVKS (VNGCloud Kubernetes Service) giới thiệu bản cập nhật mới nhất cho VKS đã có sẵn, mang đến nhiều cải tiến mới cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nCải tiến:\\n\\nNâng cấp Plugin VNGCloud BlockStorage CSI Driver: Các lỗi đã được phát hiện trong các phiên bản trước đã được khắc phục, giúp hệ thống hoạt động mượt mà và tin cậy hơn.\\n\\nJuly 17, 2024\\n\\nVKS (VNGCloud Kubernetes Service) giới thiệu bản cập nhật mới nhất cho VKS đã có sẵn, mang đến nhiều cải tiến cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nCải tiến:\\n\\nPrivate Node Group: MTU cho các Node thuộc Private Node Group đã được cập nhật lên 1450. Điều này giúp cải thiện hiệu suất mạng cho các ứng dụng chạy trong Private Node Group.\\n\\nNumber of nodes và AutoScale: Giờ đây bạn có thể chỉnh sửa cả hai thuộc tính này trong cùng một API. Điều này giúp đơn giản hóa việc quản lý Cluster của bạn.\\n\\nJuly 02, 2024\\n\\nVKS (VNGCloud Kubernetes Service) giới thiệu bản cập nhật mới nhất cho VKS đã có sẵn, mang đến nhiều tính năng và cải tiến mới cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nTính năng mới:\\n\\nHỗ trợ Stop POC cho Cluster: Người dùng giờ đây có thể thực hiện Stop POC cho toàn bộ các tài nguyên đang được POC trên một Cluster một cách dễ dàng, thay vì phải thực hiện Stop POC riêng lẻ cho từng resource. Điều này giúp tiết kiệm thời gian và công sức khi chuyển Cluster từ tài nguyên thử nghiệm sang tài nguyên thật. Chi tiết tham khảo thêm tại đây.\\n\\nCải tiến:\\n\\nTrạng thái Node Group: Bổ sung trạng thái \"Degraded\" để người dùng có thể theo dõi tình trạng hoạt động của Node Group một cách chính xác hơn. Trạng thái này sẽ hiển thị khi số node hoạt động ít hơn số replica thực tế.\\n\\nTimeout cho Cluster và Node Group: Đã thêm thời gian chờ (timeout) cho việc tạo Cluster và Node Group, cải tiến này đảm bảo VKS vận hành mượt mà và hiệu quả, đồng thời cung cấp thông tin rõ ràng và kịp thời cho người dùng. Timeout cho việc tạo Cluster là 1 giờ và cho Node Group là 3 giờ. Nếu sau khoảng thời gian này mà Cluster hoặc Node Group của bạn chưa được tạo thành công, chúng tôi sẽ cập nhật trạng thái chúng về ERROR. Lúc này, bạn có thể thực hiện xóa và tạo Cluster, Node Group khác thay thế.\\n\\nKubeConfig Access: Việc truy cập vào tệp tin KubeConfig giờ chỉ được phép khi Cluster đã Active. Cải tiến này giúp người dùng tránh các lỗi cấu hình khi sử dụng Terraform để tự động hóa việc triển khai Kubernetes.\\n\\nJune 27, 2024\\n\\nVKS (VNGCloud Kubernetes Service) giới thiệu bản cập nhật mới nhất cho VKS đã có sẵn, mang đến nhiều cải tiến mới cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nCải tiến:\\n\\nNâng cấp Plugin VNGCloud LoadBalancer Controller, Plugin VNGCloud Ingress Controller: Các lỗi đã được phát hiện trong các phiên bản trước đã được khắc phục, giúp hệ thống hoạt động mượt mà và tin cậy hơn.\\n\\nJune 19, 2024\\n\\nVKS (VNGCloud Kubernetes Service) giới thiệu bản cập nhật mới nhất cho VKS đã có sẵn, mang đến nhiều cải tiến mới cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nCải tiến:\\n\\nNâng cấp tính năng thiết lập kích cỡ PVC (Persistent Volume Claim Size): Người dùng giờ đây có thể chỉ định kích cỡ tối thiểu cho ổ đĩa CSI là 1GB thay vì kích cỡ tối thiểu là 20GB như trước đây. Chi tiết bạn có thể tham khảo thêm tại Volume và Integrate with Container Storage Interface.\\n\\nThay đổi Storage Class mặc định sử dụng cho Cluster: thay đổi mặc định từ ổ đĩa loại SSD - IOPS 200 thành mặc định ổ đĩa loại SSD - IOPS 3000.\\n\\nNâng cấp Plugin VNGCloud LoadBalancer Controller, Plugin VNGCloud Ingress Controller: cải tiến plugin giúp tránh trùng lặp việc đặt tên Load Balancer.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nDo Storage Class mặc định cũ đã được chúng tôi xóa khỏi hệ thống, nếu bạn muốn tiếp tục sử dụng và thực hiện resize storage class này, bạn có thể:\\n\\nTạo Storage Class có tên sc-iops-200-retain với Volume Type mà bạn mong muốn.\\n\\nResize Storage Class thông qua lệnh:\\n\\nkubectl patch pvc sc-iops-200-retain -p \\'{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"50Gi\"\\\\}}\\\\}}\\'\\n\\nChi tiết tham khảo thêm tại Integrate with Container Storage Interface. {% endhint %}\\n\\nJune 12, 2024\\n\\nVKS (VNGCloud Kubernetes Service) giới thiệu bản cập nhật mới nhất cho VKS đã có sẵn, mang đến nhiều tính năng và cải tiến mới cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nTính năng mới:\\n\\nHỗ trợ người dùng làm việc với VKS thông qua Terraform: Người dùng có thể dễ dàng khởi tạo Cluster và Node Group trong VKS bằng Terraform. Chi tiết tham khảo thêm tại đây.\\n\\nCải tiến:\\n\\nNâng cấp Plugin VNGCloud LoadBalancer Controller: Bổ sung Annotation để cấu hình Load Balancer hỗ trợ Proxy Protocol. Chi tiết tham khảo thêm tại đây.\\n\\nMay 30, 2024\\n\\nChúng tôi vô cùng trân trọng thông báo, bản release chính thức (General Availability) của dịch vụ VNGCloud Kubernetes Service đã có sẵn. Với bản release chính thức này, ngoài các tính năng mà chúng tôi đã cung cấp trên các bản release trước đó, phiên bản này sẽ mang đến nhiều tính năng và cải tiến mới cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nTính năng mới:\\n\\nRe-activate : VKS cho phép bạn thực hiện yêu cầu hệ thống tự động khởi tạo lại IAM Service Account mặc định khi bạn xóa nhầm hoặc thay đổi thông tin IAM Service Account mặc định đã khởi tạo trước đó. IAM Service Account mặc định là IAM Service Account được hệ thống VKS tự động khởi tạo khi bạn bắt đầu làm việc với VKS, chúng tôi sẽ sử dụng IAM Service Account này để khởi tạo các resource cho Cluster của bạn.\\n\\nEvent History: VKS sẽ thực hiện hiển thị lịch sử các sự kiện xảy ra khi người dùng làm việc với Cluster hoặc từng Node Group. Đây sẽ là một cách giúp bạn có thể giám sát các hoạt động xảy ra với Cluster của bạn, từ đó hạn chế các hoạt động bất thường xảy ra.\\n\\nVolume: VKS đã tích hợp hiển thị danh sách Volume tại Resource Tab, giúp bạn dễ dàng quản lý các Volume đang được attach vào Cluster của bạn.\\n\\nLoad Balancer: VKS đã tích hợp hiển thị danh sách Load Balancer tại Resource Tab, giúp bạn dễ dàng quản lý các Load Balancer đang được sử dụng cho Cluster của bạn.\\n\\nCải tiến:\\n\\nHiệu năng: VKS đã thực hiện tối ưu hiệu năng khi khởi tạo Cluster. Cụ thể, tại bản Alpha, thời gian từ khi bắt đầu khởi tạo Cluster (với Default Node Group) tới khi Cluster chuyển trạng thái ACTIVE vào khoảng 04:00s tới 04:30s. Hiện tại thời gian này đã được chúng tôi tối ưu về 02:30s tới 03:00s tùy thuộc vào từng Cluster và từng thời điểm mà bạn khởi tạo.\\n\\nGarbage collection of unused containers and images: VKS sẽ tự động xóa các image không được sử dụng khi disk chạm mức giới hạn sử dụng (tỷ lệ usage/ quota >= 85%).\\n\\nNgoài ra, bản GA này được chúng tôi cải thiến một vài vấn đề khác như:\\n\\nThay đổi cách đặt tên Node Name giúp bạn dễ dàng sử dụng và quản lý Cluster của bạn. Cụ thể, tên Node Name sẽ có thêm thông tin Cluster Name, Node Group Name.\\n\\nXóa User Builder trên User\\'s Worker Node.\\n\\nThay đổi cơ chế SSH từ Port 22 qua Port 234.\\n\\nNếu bạn gặp bất kỳ vấn đề với bản phát hành chính thức này, vui lòng liên hệ với bộ phận hỗ trợ của VKS để được trợ giúp.\\n\\nMay 03, 2024\\n\\nBản cập nhật mới nhất cho VKS đã có sẵn, mang đến nhiều tính năng và cải tiến mới cho người dùng. Dưới đây là chi tiết về bản cập nhật:\\n\\nTính năng mới:\\n\\nHỗ trợ tính năng Whitelist: VKS cho phép tạo Private Node Group với chỉ Private IP đồng thời cho phép IP nào kết nối tới Cluster thông qua tính năng Whitelist IP. Chi tiết tham khảo thêm tại Whitelist.\\n\\nCải tiến:\\n\\nTối ưu hóa hệ thống: Giúp hệ thống hoạt động trơn tru và hiệu quả hơn.\\n\\nSửa lỗi: Khắc phục một số lỗi nhỏ để mang đến trải nghiệm người dùng tốt hơn.\\n\\nNếu bạn gặp bất kỳ vấn đề nào sau khi cập nhật, vui lòng liên hệ với bộ phận hỗ trợ của VKS để được trợ giúp.\\n\\nApril 17, 2024\\n\\nChúng tôi vô cùng giới thiệu bản cập nhật mới cho dịch vụ VKS, mang đến cho bạn trải nghiệm quản lý Kubernetes mạnh mẽ và hiệu quả hơn bao giờ hết!\\n\\nĐiểm nổi bật:\\n\\nQuản lý Control Plane hoàn toàn tự động (Fully Managed control plane): VKS sẽ giải phóng bạn khỏi gánh nặng quản lý Control Plane của Kubernetes, giúp bạn tập trung vào việc phát triển ứng dụng.\\n\\nHỗ trợ các phiên bản Kubernetes mới nhất: VKS luôn cập nhật những phiên bản Kubernetes mới nhất (minor version từ 1.27, 1.28, 1.29) để đảm bảo bạn luôn tận dụng được những tính năng tiên tiến nhất.\\n\\nKubernetes Networking: VKS tích hợp Calico CNI, mang lại tính hiệu quả và bảo mật cao.\\n\\nUpgrade seamlessly: VKS hỗ trợ nâng cấp giữa các phiên bản Kubernetes một cách dễ dàng và nhanh chóng, giúp bạn luôn cập nhật những cải tiến mới nhất.\\n\\nScaling & Healing Automatically: VKS tự động mở rộng Node group khi cần thiết và tự động sửa lỗi khi node gặp vấn đề, giúp bạn tiết kiệm thời gian và công sức quản lý.\\n\\nGiảm chi phí và nâng cao độ tin cậy: VKS triển khai Control Plane của Kubernetes ở chế độ sẵn sàng cao và hoàn toàn miễn phí, giúp bạn tiết kiệm chi phí và nâng cao độ tin cậy cho hệ thống.\\n\\nTích hợp Blockstore Native (Container Storage Interface - CSI): VKS cho phép bạn quản lý Blockstore thông qua YAML của Kubernetes, cung cấp lưu trữ bền vững cho container và hỗ trợ các tính năng quan trọng như thay đổi kích thước, thay đổi IOPS và snapshot volume.\\n\\nTích hợp Load Balancer (Network Load Balancer, Application Load Balancer) thông qua các driver được tích hợp sẵn như VNGCloud Controller Mananger, VNGCloud Ingress Controller: VKS cung cấp khả năng quản lý NLB/ALB thông qua YAML của Kubernetes, giúp bạn dễ dàng expose Service trong Kubernetes ra bên ngoài.\\n\\nNâng cao bảo mật: VKS cho phép bạn tạo Private Node Group với chỉ Private IP và kiểm soát quyền truy cập vào cluster thông qua tính năng Whitelist IP, đảm bảo an toàn cho hệ thống của bạn.\\n\\nVới những tính năng đột phá này, VKS hứa hẹn sẽ mang đến cho bạn trải nghiệm quản lý Kubernetes hoàn toàn mới, giúp bạn tối ưu hóa hiệu quả và tiết kiệm chi phí!\\n\\nHãy liên hệ với chúng tôi để được tư vấn và hỗ trợ thêm!'),\n",
       " Document(metadata={'source': './../../data/vks/upgrade-kubernetes-version/manually-upgrade.md'}, page_content='Manually Upgrade\\n\\nUpgrading Control Plane Version\\n\\nHiện tại, hệ thống VKS của chúng tôi đã hỗ trợ bạn nâng cấp Control Plane Version, bạn có thể:\\n\\nNâng cấp Minor Version mới hơn (ví dụ: 1.24 lên 1.25)\\n\\nNâng cấp Patch Version mới hơn (ví dụ: 1.24.2-VKS.100 lên 1.24.5-VKS.200)\\n\\nĐể thực hiện nâng cấp phiên bản Control Plane, bạn có thể thực hiện theo hướng dẫn sau:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn menu Kubernetes Cluster.\\n\\nBước 3: Chọn biểu tượng và chọn Upgrade control plane version để thực hiện nâng cấp version control plane.\\n\\nBước 4: Bạn có thể lựa chọn phiên bản mới cho control plane. Phiên bản mới cần hợp lệ và tương thích với phiên bản hiện tại của cluster. Cụ thể: bạn có thể chọn:\\n\\nNâng cấp Minor Version mới hơn (ví dụ: 1.24 lên 1.25)\\n\\nNâng cấp Patch Version mới hơn (ví dụ: 1.24.2-VKS.100 lên 1.24.5-VKS.200)\\n\\nBước 4: Hệ thống VKS sẽ thực hiện nâng cấp các thành phần Control Plane của Cluster lên phiên bản mới. Sau khi việc nâng cấp hoàn tất, trạng thái (status) Cluster trở về ACTIVE.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nViệc nâng cấp Control Plane Version là không bắt buộc và độc lập với việc nâng cấp Node Group Version. Tuy nhiên Control Plane Version và Node Group Version trong cùng một Cluster không được lệch quá 1 minor version. Bên cạnh đó, hệ thống VKS tự động nâng cấp Control Plane Version khi phiên bản Kubernetes Version hiện tại đang sử dụng cho Cluster của bạn quá thời hạn được nhà cung cấp hỗ trợ.\\n\\nTrong quá trình nâng cấp Control Plane Version, bạn không thể thực hiện các hành động khác trên Cluster của bạn.\\n\\nBên dưới là một vài lưu ý trước, trong và sau quá trình nâng cấp, vui lòng tham khảo thêm:\\n\\nTrước khi thực hiện:\\n\\nSao lưu dữ liệu: Nên sao lưu dữ liệu của cluster trước khi nâng cấp để đảm bảo an toàn trong trường hợp nâng cấp thất bại.\\n\\nKiểm tra phiên bản hiện tại: Truy cập Releases để tham khảo danh sách các phiên bản được hỗ trợ. Chọn phiên bản mới hợp lệ và tương thích với phiên bản hiện tại của cluster.\\n\\nĐảm bảo tính sẵn sàng của cluster: Cluster phải đang ở trạng thái (status) hoạt động (ACTIVE) và tất cả các node phải HEALTHY.\\n\\nNgừng các tác vụ đang chạy: Ngừng các tác vụ đang chạy trên cluster để tránh ảnh hưởng đến quá trình nâng cấp.\\n\\nTrong khi thực hiện:\\n\\nTheo dõi trạng thái (status) cluster: Theo dõi trạng thái (status) cluster trong quá trình nâng cấp. trạng thái (status) cluster sẽ chuyển sang UPDATING và sau khi hoàn tất sẽ trở về ACTIVE.\\n\\nKiểm tra logs hệ thống: Kiểm tra logs hệ thống để phát hiện bất kỳ lỗi hoặc cảnh báo nào trong quá trình nâng cấp.\\n\\nSau khi thực hiện:\\n\\nKiểm tra tính sẵn sàng của cluster: Xác nhận rằng cluster đã được nâng cấp thành công và tất cả các node đang hoạt động bình thường.\\n\\nKiểm tra các ứng dụng: Kiểm tra các ứng dụng đang chạy trên cluster để đảm bảo chúng hoạt động bình thường sau khi nâng cấp.\\n\\nLưu ý:\\n\\nViệc nâng cấp Control Plane Version có thể mất một khoảng thời gian tùy thuộc vào kích thước và độ phức tạp của cluster.\\n\\nTrong một số trường hợp hiếm gặp, việc nâng cấp Control Plane Version có thể thất bại. Nếu điều này xảy ra, hệ thống VKS sẽ tự động rollback cluster về phiên bản hiện tại. {% endhint %}\\n\\nUpgrading Node Group Version\\n\\nHiện tại, hệ thống VKS của chúng tôi đã hỗ trợ bạn nâng cấp Node Group Version, bạn có thể nâng cấp Node Group Version lên:\\n\\nControl Plane Version (Ví dụ nâng cấp từ 1.24 (Node Group version hiện tại) lên 1.25 (Control Plane Version hiện tại), nhưng không thể nâng cấp lên các phiên bản khác.\\n\\nĐể thực hiện nâng cấp phiên bản Node Group Version, bạn có thể thực hiện theo hướng dẫn sau:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Tại màn hình Overview, chọn menu Kubernetes Cluster. Chọn vào một Cluster mà bạn muốn nâng cấp Node Group Version.\\n\\nBước 3: Chọn biểu tượng và chọn Upgrade Node Group Version để thực hiện nâng cấp version node group.\\n\\nBước 4: Bạn có thể lựa chọn phiên bản mới cho tất cả các Node Group. Phiên bản mới cần hợp lệ và tương thích với phiên bản hiện tại của cluster. Cụ thể: bạn có thể chọn:\\n\\nNâng cấp Node Group sao cho về cùng version với Control Plane Version (ví dụ: 1.24 lên 1.25)\\n\\nBước 5: Hệ thống VKS sẽ thực hiện nâng cấp tất cả các Node Group lên version của Control Plane. Sau khi việc nâng cấp hoàn tất, trạng thái (status) Node Group trở về ACTIVE.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nViệc nâng cấp Node Group Version là không bắt buộc và độc lập với việc nâng cấp Control Plane Version. Tuy nhiên tất cả các Node Group trong một Cluster sẽ được nâng cấp trong cùng một lần, cũng như Control Plane Version và Node Group Version trong cùng một Cluster không được lệch quá 1 minor version. Bên cạnh đó, hệ thống VKS tự động nâng cấp Node Group Version khi phiên bản Kubernetes Version hiện tại đang sử dụng cho Cluster của bạn quá thời hạn được nhà cung cấp hỗ trợ.\\n\\nTrong quá trình nâng cấp Node Group Version, bạn không thể thực hiện các hành động khác trên Node Group của bạn.\\n\\nBên dưới là một vài lưu ý trước, trong và sau quá trình nâng cấp, vui lòng tham khảo thêm:\\n\\nTrước khi thực hiện:\\n\\nKiểm tra phiên bản hiện tại: Truy cập Releases để tham khảo danh sách các phiên bản được hỗ trợ. Chọn phiên bản mới hợp lệ và tương thích với phiên bản hiện tại của cluster.\\n\\nĐảm bảo tính sẵn sàng của Node Group: Node Group phải đang ở trạng thái (status) hoạt động (ACTIVE) và tất cả các node phải HEALTHY.\\n\\nNgừng các tác vụ đang chạy: Ngừng các tác vụ đang chạy trên cluster để tránh ảnh hưởng đến quá trình nâng cấp.\\n\\nTrong khi thực hiện:\\n\\nTheo dõi trạng thái (status) Node Group: Theo dõi trạng thái (status) Node Group trong quá trình nâng cấp. trạng thái (status) Node Group sẽ chuyển sang UPDATING và sau khi hoàn tất sẽ trở về ACTIVE.\\n\\nKiểm tra logs hệ thống: Kiểm tra logs hệ thống để phát hiện bất kỳ lỗi hoặc cảnh báo nào trong quá trình nâng cấp.\\n\\nSau khi thực hiện:\\n\\nKiểm tra tính sẵn sàng của Node Group: Xác nhận rằng Node Group đã được nâng cấp thành công và tất cả các node đang hoạt động bình thường.\\n\\nKiểm tra các ứng dụng: Kiểm tra các ứng dụng đang chạy trên cluster để đảm bảo chúng hoạt động bình thường sau khi nâng cấp.\\n\\nLưu ý:\\n\\nViệc nâng cấp Node Group Version có thể mất một khoảng thời gian tùy thuộc vào kích thước và độ phức tạp của Node Group.\\n\\nTrong một số trường hợp hiếm gặp, việc nâng cấp Node Group Version có thể thất bại. Nếu điều này xảy ra, hệ thống VKS sẽ tự động rollback cluster về phiên bản hiện tại. {% endhint %}'),\n",
       " Document(metadata={'source': './../../data/vks/upgrade-kubernetes-version/README.md'}, page_content='Upgrade Kubernetes Version'),\n",
       " Document(metadata={'source': './../../data/vks/upgrade-kubernetes-version/automatically-upgrade.md'}, page_content='Automatically Upgrade\\n\\nTổng quan\\n\\nAutomatically Upgrade Cluster trên VKS là quá trình hệ thống VKS tự động nâng cấp cluster theo upgrade window mà bạn chỉ định, hoặc tự động nâng cấp nhằm cải thiện hiệu suất, bảo mật và khả năng tương thích của hệ thống. Hiện tại, VKS cung cấp 2 loại tự động nâng cấp bao gồm:\\n\\n1. Required Upgrades (Nâng cấp bắt buộc)\\n\\nMục đích: Các nâng cấp bắt buộc nhằm bảo vệ cụm khỏi các rủi ro liên quan đến bảo mật, ổn định và việc sử dụng các phiên bản Kubernetes không còn được hỗ trợ.\\n\\nCụ thể: hệ thống VKS sẽ thực hiện upgrade cluster trong các trường hợp:\\n\\nEnd-of-Life (EOL) Upgrades: Cập nhật Cluster lên các phiên bản mới hơn khi phiên bản hiện tại của Cluster sắp hết hạn.\\n\\nSecurity Upgrades: Vá các lỗ hổng bảo mật, đảm bảo an toàn dữ liệu.\\n\\nStability Upgrades: Sửa các lỗi nghiêm trọng ảnh hưởng đến sự ổn định của hệ thống.\\n\\nThời gian thực hiện:\\n\\nCác nâng cấp này sẽ được hệ thống VKS tự động thực hiện sau 8 giờ tối vào bất kỳ ngày nào nếu cần, nhằm giảm thiểu ảnh hưởng đến bạn. Chúng tôi sẽ thông báo cho bạn về các nâng cấp này trong thời gian sớm nhất.\\n\\n2. Regular Upgrades (Nâng cấp thông thường)\\n\\nMục đích: Các nâng cấp định kỳ theo upgrade window mà bạn chỉ định nhằm giúp cluster của bạn luôn cập nhật với các phiên bản mới nhất, bao gồm cả minor version và patch version.\\n\\nCụ thể:\\n\\nMinor Version Upgrades: Cập nhật các tính năng và API mới. Ví dụ, nếu cluster hiện tại đang sử dụng phiên bản 1.28.2, hệ thống sẽ tự động nâng cấp lên phiên bản 1.29.6.\\n\\nPatch Version Upgrades: Vá lỗi nhỏ và cải thiện hiệu suất. Ví dụ, nếu cluster hiện tại đang sử dụng phiên bản 1.29.1, hệ thống sẽ tự động nâng cấp lên phiên bản 1.29.2.\\n\\nThời gian thực hiện:\\n\\nBạn có thể tự cấu hình lịch trình nâng cấp thông qua VKS Portal theo hướng dẫn dưới đây. Sau khi bạn chọn lịch auto-upgrade qua VKS Portal, hệ thống sẽ bắt đầu thực hiện nâng cấp sau ít nhất 6 ngày kể từ ngày hiện tại. Thời gian này giúp bạn có đủ thời gian để chuẩn bị và kiểm tra trước khi nâng cấp diễn ra.\\n\\nHệ thống sẽ lặp lại cho các tuần kế tiếp kể từ lần nâng cấp trước đó và tuân theo các ngày đã được bạn chọn.\\n\\nTrước mỗi lần nâng cấp, hệ thống sẽ gửi một email thông báo cho bạn trước 6 ngày tính tới thời điểm chạy auto-upgrade thực tế. Trong email, chúng tôi sẽ nêu rõ thời gian cụ thể mà việc auto-upgrade sẽ diễn ra.\\n\\nNếu chọn nhiều ngày trong tuần (như Thứ Hai, Thứ Năm), hệ thống sẽ tính toán chu kỳ nâng cấp cho các ngày đã chọn, không ảnh hưởng đến các ngày khác.\\n\\nVui lòng tham khảo 2 ví dụ bên dưới để hiểu rõ cách hệ thống VKS thực hiện chạy Regular Upgrade.\\n\\n{% hint style=\"info\" %} Chú ý:\\n\\nHệ thống VKS sẽ cố gắng thực hiện nâng cấp theo lịch trình mà bạn đã cấu hình qua VKS Portal. Tuy nhiên, tùy thuộc vào tải của hệ thống, một số lần nâng cấp có thể bị hoãn hoặc không thực hiện đúng như lịch. Khi đó, hệ thống sẽ tự động chuyển lịch nâng cấp sang thời điểm thích hợp tiếp theo, chính là chu kỳ lặp lại trong tuần tiếp theo. {% endhint %}\\n\\nCác bước thực hiện\\n\\nBên dưới là hướng dẫn thực hiện cập nhật Upgrade Policy trên hệ thống VKS:\\n\\nBước 1: Truy cập vào https://vks.console.vngcloud.vn/overview\\n\\nBước 2: Chọn Create a Cluster.\\n\\nBước 3: Tại màn hình khởi tạo Cluster, chúng tôi đã thiết lập thông tin cho Cluster và một Default Node Group. Tại mục Cluster Upgrade policy, bạn có thể chọn Enable/ Disable Automatic upgrade. Trong đó:\\n\\nEnable Automatic upgrade, lúc này:\\n\\nCác nâng cấp Required Upgrades sẽ được thực hiện tự động sau 8 PM bất kỳ ngày nào bởi hệ thống VKS khi cần.\\n\\nCác nâng cấp Regular Upgrades sẽ được thực hiện theo lịch trình do bạn thiết lập.\\n\\nDisable Automatic upgrade, lúc này:\\n\\nChỉ các nâng cấp Required Upgrades mới được thực hiện tự động sau 8 PM bất kỳ ngày nào bởi hệ thống VKS khi cần.\\n\\nBước 4: Nếu bạn chọn Enable Automatic upgrade, vui lòng chọn thời gian hệ thống có thể thực hiện nâng cấp. Cụ thể, bạn cần:\\n\\nChọn một hoặc nhiều ngày trong tuần mà hệ thống VKS có thể thực hiện auto-upgrade (ví dụ: Monday, Tuesday, ...).\\n\\nChọn một mốc thời gian cụ thể mà bạn mong muốn hệ thống VKS thực hiện auto-upgrade (ví dụ: 20:00 (08:00 PM - theo múi giờ UTC+07:00)\\n\\nBước 5: Chọn Create Kubernetes cluster/ Update Cluster. Hãy chờ vài phút để chúng tôi khởi tạo/ chỉnh sửa Cluster của bạn, trạng thái của Cluster lúc này là Creating/ Updating.\\n\\nBước 6: Khi trạng thái Cluster là Active, tức là hệ thống VKS đã hoàn thành việc tạo/ chỉnh sửa cluster này.\\n\\nQuy tắc hoạt động của hệ thống\\n\\nRequired Upgrade\\n\\nTrên hệ thống VKS, required upgrades bao gồm End-of-Life (EOL) Upgrades, Security Upgrades, Stability Upgrades. Required Upgrades sẽ được thực hiện sau 8:00 PM vào bất kỳ ngày nào cần thiết. Những nâng cấp này không tuân theo lịch trình cố định mà được kích hoạt khi hệ thống phát hiện cần đảm bảo:\\n\\nCluster đang sử dụng phiên bản Kubernetes vẫn nằm trong danh sách được hỗ trợ.\\n\\nTránh các vấn đề nghiêm trọng có thể gây rủi ro đến bảo mật hoặc dữ liệu.\\n\\nTrước khi thực hiện Required Upgrade, hệ thống sẽ gửi email thông báo chi tiết cho bạn, bao gồm:\\n\\nThông tin Cluster/ Node Group dự kiến được nâng cấp.\\n\\nPhiên bản hiện tại và Phiên bản nâng cấp dự kiến.\\n\\nNgày và giờ dự kiến thực hiện nâng cấp,...\\n\\nSau khi nâng cấp hoàn tất, hệ thống sẽ gửi email xác nhận tình trạng cluster và các thay đổi liên quan.\\n\\nRiêng đối với trường hợp Force Upgrade, khi phát hiện ra Kubernetes version của bạn sắp hết hạn hỗ trợ. Chúng tôi sẽ gửi email thông báo cho bạn trước 60 ngày, 30 ngày, 7 ngày và 1 ngày. Trong thời gian này, bạn có thể thực hiện nâng cấp thủ công phiên bản Kubernetes theo hướng dẫn tại đây. Sau thời gian này, nếu bạn không thực hiện nâng cấp thủ công, chúng tôi sẽ tiến hành force upgrade lên phiên bản Kubernetes được hỗ trợ gần nhất. Tùy thuộc vào workload của bạn mà quá trình nâng cấp có thể gây gián đoạn dịch vụ trong khoảng thời gian khác nhau. Vui lòng tham khảo mục bên dưới để biết chi tiết.\\n\\nRegular Upgrades\\n\\nRegular Upgrades bao gồm các nâng cấp Minor và Patch nhằm cải thiện hiệu năng, tính năng mới, và sửa các lỗi nhỏ trong hệ thống Kubernetes. Bạn có thể tự cấu hình lịch Regular Upgrades thông qua VKS Portal. Hệ thống sẽ cố gắng thực hiện nâng cấp trong cluster theo ngày, giờ mà bạn đã chỉ định.\\n\\nTrước khi thực hiện Regular Upgrade, hệ thống sẽ gửi email thông báo chi tiết cho bạn, bao gồm:\\n\\nThông tin Cluster/ Node Group dự kiến được nâng cấp.\\n\\nPhiên bản hiện tại và Phiên bản nâng cấp dự kiến.\\n\\nNgày và giờ dự kiến thực hiện nâng cấp,...\\n\\nSau khi nâng cấp hoàn tất, hệ thống sẽ gửi email xác nhận tình trạng cluster và các thay đổi liên quan.\\n\\nTrước thời điểm hệ thống thực hiện tự động nâng cấp, nếu bạn thực hiện thay đổi Upgrade Window hoặc thực hiện nâng cấp thủ công, hệ thống sẽ bỏ qua lần nâng cấp tự động này. Trong trường hợp đó, bạn có thể bỏ qua email thông báo là chúng tôi gửi này. ùy thuộc vào workload của bạn mà quá trình nâng cấp có thể gây gián đoạn dịch vụ trong khoảng thời gian khác nhau. Vui lòng tham khảo mục bên dưới để biết chi tiết.\\n\\nTối Ưu Hóa Quá Trình Upgrade và Giảm Thiểu Downtime Khi Upgrade Kubernetes Version\\n\\nNâng cấp Kubernetes cluster là một bước quan trọng để duy trì tính ổn định, bảo mật và hiệu suất của hệ thống. Tuy nhiên, quá trình này có thể dẫn đến downtime nếu không được thực hiện đúng cách. Sau đây là chi tiết các nguyên nhân phổ biến gây gián đoạn, đồng thời cung cấp các giải pháp giúp bạn giảm thiểu rủi ro trong quá trình nâng cấp.\\n\\nKhi nâng cấp cluster, hai thành phần chính được nâng cấp:\\n\\nControl Plane (Kubernetes API Server): Thành phần này được thay thế bằng một phiên bản mới. Trong quá trình này, API server sẽ không khả dụng trong vài phút, nhưng workload sẽ không bị ảnh hưởng.\\n\\nWorker Nodes: Các node này được nâng cấp theo cơ chế rolling update, từng node một trong mỗi node group.\\n\\nQuy trình nâng cấp node group:\\n\\nBước 1: Hệ thống VKS thực hiện xác định các node cần nâng cấp.\\n\\nBước 2: Hệ thống VKS thực hiện drain node, tức là di chuyển tất cả các pod đang chạy trên node cũ chưa nâng cấp này sang các node khác.\\n\\nBước 3: Hệ thống sẽ tạo lại node mới với cấu hình đã được thiết lập trên node group và thực hiện join node này vào cụm. Nếu sau khi khởi động lại, node vẫn báo cáo trạng thái \"NotReady\", hệ thống sẽ tiếp tục khởi động lại node cho đến khi node trở lại trạng thái hoạt động bình thường.\\n\\nNếu surge upgrades được kích hoạt, hệ thống sẽ tạo tối đa 10 node mới trước khi bắt đầu nâng cấp các node hiện tại. Điều này giúp đảm bảo workload có đủ tài nguyên chạy trong suốt quá trình nâng cấp.\\n\\nCác Nguyên Nhân Gây Downtime Hoặc Việc Upgrade Không Thành Công và Cách Giải Quyết\\n\\nPods Không Đủ Replicas Hoặc Không Được Triển Khai Qua Deployment/StatefulSet: Nếu pod không có đủ replicas hoặc không được triển khai với Deployment/StatefulSet, khi node chứa pod này bị nâng cấp, dịch vụ sẽ ngừng hoạt động. Giải pháp:\\n\\nĐảm bảo các pod quan trọng luôn có số lượng replicas >=2.\\n\\nSử dụng Deployment hoặc StatefulSet để tự động quản lý pods.\\n\\nPod Disruption Budget (PDB) Cấu Hình Quá Nhỏ: PDB giới hạn số lượng pod có thể bị gián đoạn tại một thời điểm. Nếu cấu hình quá nhỏ, quá trình nâng cấp có thể bị treo. Giải pháp:\\n\\nCấu hình PDB phù hợp với yêu cầu của ứng dụng (ví dụ: cho phép ít nhất 1 pod luôn hoạt động).\\n\\nPersistent Volume (PV) Không Được Cấu Hình Với ReadWriteMany: PV với chế độ ReadWriteOnce chỉ có thể được gắn vào một node duy nhất. Khi node này nâng cấp, PV cần được di chuyển, gây downtime. Giải pháp:\\n\\nSử dụng chế độ ReadWriteMany nếu ứng dụng yêu cầu dữ liệu khả dụng trên nhiều node.\\n\\nPods Thiếu Liveness và Readiness Probes: Nếu không có probes, Kubernetes không thể xác định trạng thái của pod, dẫn đến routing sai hoặc downtime. Giải pháp:\\n\\nCấu hình Liveness Probes để kiểm tra pod có hoạt động đúng cách.\\n\\nCấu hình Readiness Probes để đảm bảo chỉ route traffic đến các pod đã sẵn sàng.\\n\\nWorkload Mất Cân Bằng (Affinity/Anti-Affinity): Quy tắc Affinity/Anti-Affinity quá chặt chẽ có thể khiến Kubernetes không tìm được đủ node để khởi động lại pod. Giải pháp:\\n\\nĐiều chỉnh các quy tắc Affinity/Anti-Affinity sao cho hợp lý.\\n\\nKết hợp với surge upgrades để tăng thêm tài nguyên tạm thời. Surge Upgrades là một tính năng giúp tạo các node mới trước khi nâng cấp các node hiện tại. Điều này đảm bảo workload có tài nguyên dự phòng, giúp tăng tốc quá trình nâng cấp và hạn chế downtime.\\n\\nNgoài ra, để việc upgrade diễn ra thành công, bạn cũng cần xem xét tới 2 vấn đề sau:\\n\\nBilling Problem: Khi thực hiện upgrade, hệ thống VKS sẽ thực hiện drain node cũ, tạo node mới trước khi thực hiện xóa node cũ, do đó, tại thời điểm hệ thống thực hiện upgrade, chi phí của cluster sẽ tạm thời tăng. Nếu số dư credit không đủ, hệ thống không thể tạo node mới, khiến quá trình nâng cấp bị gián đoạn. Giải pháp:\\n\\nTheo dõi thông báo lịch upgrade qua email và chuẩn bị số dư credit đủ để hệ thống thực hiện nâng cấp.\\n\\nResource Quota Problem: Trong trường hợp số lượng node mà bạn có thể tạo do vượt quá quota hiện tại, việc nâng cấp Kubernetes có thể bị gián đoạn vì không thể tạo thêm node mới. Điều này đặc biệt quan trọng khi sử dụng Surge Upgrade, nơi hệ thống cần tạo thêm các node mới trước khi xóa các node cũ để đảm bảo tính liên tục của dịch vụ. Giải pháp:\\n\\nTheo dõi thông báo lịch upgrade qua email và thực hiện yêu cầu nâng cấp quota nếu có.\\n\\nTrong trường hợp bạn muốn chủ động kiểm soát việc nâng cấp Kubernetes Cluster (manually upgrade) và không muốn hệ thống tự động thực hiện nâng cấp, bạn có thể tắt tính năng Regular Upgrade bằng cách bỏ chọn Enable Automatic Upgrade.'),\n",
       " Document(metadata={'source': './../../data/vks/upgrade-kubernetes-version/phien-ban-ho-tro-kubernetes.md'}, page_content='Phiên bản hỗ trợ Kubernetes\\n\\nHiện tại, hệ thống VKS đang cung cấp cho bạn 6 Kubernetes version bao gồm:\\n\\nVersion Ngày hết hạn Tham khảo thêm Vesion 1.29.1 2025-02-28 https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.29.md#v1291 Version 1.28.8 2024-10-28 https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.28.md#v1288 Version 1.27.12 2024-06-28 https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.27.md#v12712 Version 1.29.1-vks.1724605200 2025-02-28 Version 1.28.8-vks.1724605200 2024-10-28 Version 1.27.12-vks.1724605200 2024-06-28'),\n",
       " Document(metadata={'source': './../../data/vks/giam-sat/README.md'}, page_content='Monitoring'),\n",
       " Document(metadata={'source': './../../data/vks/giam-sat/metrics.md'}, page_content='Metrics\\n\\nBạn có thể cài đặt vMonitor Platform Metric Agent vào Kubernetes Cluster để thu thập và đẩy metric về vMonitor Platform site, sau đó sử dụng các tính năng tại vMonitor Platform để quản lý tập trung tài nguyên và theo dõi hoạt động bất thường của Kubernetes Cluster.\\n\\nCài đặt Metric Agent sử dụng Helm\\n\\nCác bước chuẩn bị trước khi cài đặt\\n\\n1. Kiểm tra bạn đã có Metric Quota và quota chưa chạm mức giới hạn, nếu chưa có bạn cần thực hiện mua Quota Metric tại đây.\\n\\n2. Tạo Service Account và gắn policy: vMonitorMetricPush để có đủ quyền đẩy Metric về vMonitor\\n\\nĐể tạo service account bạn truy cập tại đây, sau đó thực hiện các bước sau:\\n\\nChọn \"Create a Service Account\", điền tên cho Service Account và nhấn Next Step để gắn quyền cho Service Account\\n\\nTìm và chọn Policy: vMonitorMetricPush, sau đó nhấn \"Create a Service Account\" để tạo Service Account, Policy: vMonitorMetricPush do VNG Cloud tạo ra chỉ chứa chính xác quyền đẩy metric về hệ thống\\n\\nSau khi tạo thành công bạn cần phải lưu lại Client_ID và Secret_Key để thực hiện bước tiếp theo.\\n\\nCài đặt helm tại Debian/Ubuntu server\\n\\n1. Bạn cần cài đặt Helm trên server có kubeconfig chứa đủ quyền để tương tác Kubernetes Cluster.\\n\\nKiểm tra quyền bằng command kubectl:\\n\\nKube check permission\\n\\n```\\n\\nLệnh dùng để kiểm tra quyền tương tác tất cả resource tại namespace default\\n\\nkubectl auth can-i \\'\\' \\'\\'\\n\\nLệnh dùng để kiểm tra quyền tương tác tất cả resource tại namespace chỉ định, sử dụng lệnh này khi bạn muốn cài đặt agent metric tại namespace khác\\n\\nkubectl auth can-i -n\\n\\nLệnh dùng để kiểm tra quyền tạo clusterrole và clusterrolebinding\\n\\nkubectl auth can-i create clusterrole kubectl auth can-i create clusterrolebinding ```\\n\\nNếu kết quả các command trên là YES thì bạn đã đủ quyền.\\n\\n2. Tiến hành cài đặt Helm\\n\\nThực hiện những command sau:\\n\\ncurl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null sudo apt-get install apt-transport-https --yes echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm\\n\\nKiểm tra Helm đã cài đặt thành công\\n\\n``` helm version\\n\\nKết quả mong muốn của command\\n\\nversion.BuildInfo{Version:\"v3.11.0\", GitCommit:\"472c5736ab01133de504a826bd9ee12cbe4e7904\", GitTreeState:\"clean\", GoVersion:\"go1.18.10\"}\\n\\n```\\n\\nCài đặt Helm tại các hệ điều hành khác\\n\\nTham khảo hướng dẫn cài đặt tại đây: Install Helm Through Package Managers\\n\\n3. Một số lưu ý thêm về Helm: (tham khảo chi tiết tại: helm docs)\\n\\nHelm sẽ dùng kubeconfig để tương tác với cluster, mặc định helm sẽ dùng config ở đường dẫn: \"\\\\~/.kube/config\"\\n\\nNếu cần thay đổi đường dẫn tới kubeconfig ta có thể sử dụng 2 cách:\\n\\nVới mỗi command helm ta thêm option --kubeconfig: ví dụ helm install --kubeconfig \\\\\\n\\nKhai báo biến môi trường: KUBECONFIG\\n\\nCài đặt Metric Agent\\n\\nMặc định khi cài đặt vMonitor Platform Metric Agent sẽ có 2 thành phần:\\n\\nDeployment Agent kube-state-metrics: thu thập metric từng resource của k8s cluster (pod, daemonset, deployment, replicaset, ....)\\n\\nDaemonset Agent: thu thập metrics từng node k8s cluster (CPU, Memory usage, ...)\\n\\n1. Thêm Helm vMonitor Platform Repo\\n\\nhelm repo add vmonitor-platform https://vngcloud.github.io/helm-charts-vmonitor helm repo update\\n\\n2. Cài đặt chart\\n\\nKiểm tra và xóa các resource liên quan trước khi cài đặt để tránh đụng độ\\n\\n```\\n\\nGet Clusterrole vmonitor metric agent\\n\\nkubectl get clusterrole | grep vmonitor-metric-agent\\n\\nThực hiện xóa resource nếu có tồn tại\\n\\nkubectl delete clusterrole vmonitor-metric-agent ```\\n\\nCài đặt tại namespace default (thêm cờ -n \\\\ để cài đặt agent tại namespace khác)\\n\\nhelm install vmonitor-metric-agent vmonitor-platform/vmonitor-metric-agent \\\\ --set vmonitor.iamClientID=<YOUR_CLIENT_ID_XXXXXXXXXXXXXXXXXXX> \\\\ --set vmonitor.iamClientSecret=<YOUR_CLIENT_SECRET_XXXXXXXXXXXXXXX> \\\\ --set vmonitor.clusterName=<CLUSTER_NAME>\\n\\n\\\\\\n\\n\\\\\\n\\nKiểm tra việc install agent thành công\\n\\n```\\n\\nChạy command và đảm bảo output là các pods ở trạng thái running\\n\\nkubectl get pod | grep \"vmonitor-metric-agent\" ```\\n\\nNếu các pods agent không ở trạng thái running, sử dụng command tương ứng để kiểm tra lỗi\\n\\n```\\n\\nChạy command nếu pod ở trạng thái Pending\\n\\nkubectl describe pod\\n\\nChạy command nếu pod ở trạng thái CrashLoopBackOff and Error\\n\\nkubectl logs\\n\\nSau đó kiểm tra logs xuất hiện tại agent\\n\\n```\\n\\nSau khi cài đặt hoàn tất, metric theo dõi kubernetes đã được đẩy về vMonitor Platform site , bạn có thể tiến hành lên vMonitor để vẽ các dashboard, widget.\\n\\nGỡ cài đặt Metric Agent\\n\\nThực hiện command sau để xóa các k8s resources liên quan đã cài đặt:\\n\\nhelm uninstall vmonitor-metric-agent\\n\\nCài đặt Metric Agent không sử dụng kube-state-metrics\\n\\nCài đặt tại namespace default (thêm cờ -n \\\\ để cài đặt agent tại namespace khác)\\n\\nhelm install vmonitor-metric-agent vmonitor-platform/vmonitor-metric-agent \\\\ --set vmonitor.iamClientID=<YOUR_CLIENT_ID_XXXXXXXXXXXXXXXXXXX> \\\\ --set vmonitor.iamClientSecret=<YOUR_CLIENT_SECRET_XXXXXXXXXXXXXXX> \\\\ --set vmonitor.clusterName=<CLUSTER_NAME> \\\\ --set vmonitor.kubeStateMetricsEnabled=false \\\\ --set kubeStateMetricsAgent.enabled=false')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stackops/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"  # Uses psycopg3!\n",
    "collection_name = \"my_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PGVector(\n",
    "    embeddings=embeddings_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6cd951a9-a131-4b94-8b43-2a88b7ae8b13',\n",
       " 'a0514bc6-9c38-40b7-8f59-8e22ee811e2c',\n",
       " '10d6d3ba-605d-411d-b598-5fc5d194746e',\n",
       " 'cd2a2755-9af3-42b1-93f9-9ac49adde916',\n",
       " 'e29cdd2f-6c9d-4aeb-b016-8f15b15b6aaa',\n",
       " '86b537ad-4750-416a-abae-cf3f0b9cb5a3',\n",
       " '51d97ba2-b0f9-4c63-8eb4-45bc32643861',\n",
       " '04ec882a-6be6-47bf-a369-b6fca977847d',\n",
       " 'aadd5009-cda6-49a4-8438-27168a2e754d',\n",
       " '99f45bea-b6cf-45aa-8142-188e048a923f',\n",
       " '8c24f43a-7649-43d8-82b0-7c2800fbfabd',\n",
       " '8408eaa5-e04a-4529-97a9-5178a2ba379f',\n",
       " 'f9bcd683-e16b-4330-b1eb-4afa24da3984',\n",
       " '4ea3dc88-b98e-4fdb-bad1-7a62ccae4591',\n",
       " '29d38724-bb4b-4ced-ad7e-75979b0019c6',\n",
       " 'cfdebc1f-f9dd-4b8f-b6d0-2cb1e958898c',\n",
       " '1a019301-b0b4-405b-b953-fbb537cd708e',\n",
       " 'a27c96e8-fc3d-4906-92a6-34c0a5ee07e8',\n",
       " '9f6e7fb1-5d78-4e93-b868-5934cf1d165d',\n",
       " '32e8b293-1158-40b6-884d-e4198eab513e',\n",
       " '56e54a59-0b6c-4e0b-b076-3573b6b2e862',\n",
       " '914f04f9-e3a7-447c-bbc5-dce012cbc515',\n",
       " 'cbfb32c9-2f91-48f5-86fc-fd430214a608',\n",
       " '86015644-4143-4641-89be-23368b95b2b5',\n",
       " '9d83ea1f-8bb5-4333-90f7-e878f10bdd89',\n",
       " '46cb2728-1e27-4cdc-948a-e1b5b49321f8',\n",
       " '2119d83b-9a84-483c-b9ef-bab8959ac66a',\n",
       " '8800f515-77af-479c-a244-c7a3c4574871',\n",
       " '7f0f65c6-08ed-425b-b533-521f929c59a6',\n",
       " 'cd7cb1a5-e72d-4c81-9b3e-bf38ec79518e',\n",
       " '6c1ca684-75fa-4b26-8633-9f37d0343c27',\n",
       " '034e677b-5b71-410e-af99-d82bc71701d1',\n",
       " '75aa3e52-37a8-4fc4-97a1-3ce0b73ea1dc',\n",
       " '99c4ae71-3e8a-4efc-bac7-272a4fd1db05',\n",
       " 'f1de77db-8d5a-41de-bbe6-85041b12aa55',\n",
       " 'ef94fc84-2523-4170-9b37-fbfdc9b4b813',\n",
       " '2b2996d4-e1d5-4788-a61f-5b30add3fa01',\n",
       " 'bf1b9382-29cf-48ac-8711-692146e0e41b',\n",
       " 'f1bf6c68-24d7-487b-a061-867eb0134c00',\n",
       " 'da048461-d2bc-4007-b086-f94fce7d9d9c',\n",
       " 'aecaa5ea-a020-43c4-85ca-5a9d05ecad0a',\n",
       " '382273b4-8215-4a4f-86a9-830383d0d1d5',\n",
       " '1a6b2c47-674f-46b8-8bdf-089da3970a6c',\n",
       " '00297274-2359-4f1e-92f9-21eba501d7b2',\n",
       " '305d716d-6b8d-4422-892d-d2338d83c6a4',\n",
       " '5c0f492e-939e-452d-9d02-13455331bfe4',\n",
       " '3b2b6581-cb62-4d4d-a413-4decc717fb35',\n",
       " '903a4a95-5ecd-40a5-a234-b5216ad1b250',\n",
       " 'b023a6be-cb15-49c5-a681-8222025d5ac7',\n",
       " '66735913-eeaf-48f5-afae-546befeb40e0',\n",
       " '6750abfe-f33e-40a0-85e2-f779bfb53d3d',\n",
       " 'cad07e22-ba26-47b1-9462-b1bf3b5aedc7',\n",
       " 'dc7df4ff-e48c-4c72-9acf-316605c752fd',\n",
       " '96bc4d0d-75cd-4862-bb9e-76a3dde87050',\n",
       " '84f105c6-918c-4718-951d-ef2896fbb27a',\n",
       " 'c7b30c68-4e1b-43b7-999e-d4b459cf35ce',\n",
       " '9195961f-e8d2-4997-8df9-2af5cff43ad5',\n",
       " '7043baa0-52b7-4443-bbd3-cc457cbce38c',\n",
       " '2ee65a23-513d-4363-b8ee-f911a2119e95',\n",
       " '8c511031-1686-4748-9379-0ed71896922d',\n",
       " '6ccacd14-e69f-45d6-bcfc-090c09878ca5',\n",
       " '7235879d-d729-4007-9705-4827f3aad0d7',\n",
       " '2c931895-79fe-4162-88cb-6b36c3b61c77',\n",
       " 'ef5532c7-a79f-473f-88a1-4321f7ad2bf3',\n",
       " 'fccf7ea5-869a-434d-8d68-4df493ed62a0']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"VKS là gì?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='VKS là gì?\n",
      "\n",
      "VKS (VNGCloud Kubernetes Service) là một dịch vụ được quản lý trên VNGCloud giúp bạn đơn giản hóa quá trình triển khai và quản lý các ứng dụng dựa trên container. Kubernetes là một nền tảng mã nguồn mở được phát triển bởi Google, được sử dụng rộng rãi để quản lý và triển khai các ứng dụng container trên môi trường phân tán.\n",
      "\n",
      "Những điểm nổi bật của VKS\n",
      "\n",
      "Quản lý Control Plane hoàn toàn tự động (Fully Managed control plane): VKS sẽ giải phóng bạn khỏi gánh nặng quản lý Control Plane của Kubernetes, giúp bạn tập trung vào việc phát triển ứng dụng.\n",
      "\n",
      "Hỗ trợ các phiên bản Kubernetes mới nhất: VKS luôn cập nhật những phiên bản Kubernetes mới nhất (minor version từ 1.27, 1.28, 1.29) để đảm bảo bạn luôn tận dụng được những tính năng tiên tiến nhất.\n",
      "\n",
      "Kubernetes Networking: VKS tích hợp Calico CNI, mang lại tính hiệu quả và bảo mật cao.\n",
      "\n",
      "Upgrade seamlessly: VKS hỗ trợ nâng cấp giữa các phiên bản Kubernetes một cách dễ dàng và nhanh chóng, giúp bạn luôn cập nhật những cải tiến mới nhất.\n",
      "\n",
      "Scaling & Healing Automatically: VKS tự động mở rộng Node group khi cần thiết và tự động sửa lỗi khi node gặp vấn đề, giúp bạn tiết kiệm thời gian và công sức quản lý.\n",
      "\n",
      "Giảm chi phí và nâng cao độ tin cậy: VKS triển khai Control Plane của Kubernetes ở chế độ sẵn sàng cao và hoàn toàn miễn phí, giúp bạn tiết kiệm chi phí và nâng cao độ tin cậy cho hệ thống.\n",
      "\n",
      "Tích hợp Blockstore Native (Container Storage Interface - CSI): VKS cho phép bạn quản lý Blockstore thông qua YAML của Kubernetes, cung cấp lưu trữ bền vững cho container và hỗ trợ các tính năng quan trọng như thay đổi kích thước, thay đổi IOPS và snapshot volume.\n",
      "\n",
      "Tích hợp Load Balancer (Network Load Balancer, Application Load Balancer) thông qua các driver được tích hợp sẵn như VNGCloud Controller Mananger, VNGCloud Ingress Controller: VKS cung cấp khả năng quản lý NLB/ALB thông qua YAML của Kubernetes, giúp bạn dễ dàng expose Service trong Kubernetes ra bên ngoài.\n",
      "\n",
      "Nâng cao bảo mật: VKS cho phép bạn tạo Private Node Group với chỉ Private IP và kiểm soát quyền truy cập vào cluster thông qua tính năng Whitelist IP, đảm bảo an toàn cho hệ thống của bạn.\n",
      "\n",
      "Ngoài ra, VKS còn có các ưu điểm sau:\n",
      "\n",
      "Dễ sử dụng: VKS cung cấp giao diện đơn giản và dễ sử dụng.\n",
      "\n",
      "Chi phí hợp lý: VKS cung cấp mức giá cạnh tranh cho các dịch vụ của mình.\n",
      "\n",
      "Region\n",
      "\n",
      "Hiện tại, trên VKS, chúng tôi đang cung cấp cho bạn 2 cơ sở hạ tầng riêng biệt được đặt tại Hà Nội và Hồ Chí Minh. Bạn có thể lựa chọn sử dụng VKS trên mỗi region tùy thuộc vào vị trí và nhu cầu thực tế của bạn. Đối với 2 farm HCM03, HAN01, các thông số cụ thể cho mỗi region được chúng tôi cung cấp như sau:\n",
      "\n",
      "Farm Domain HCM03 https://vks.console.vngcloud.vn HAN01 https://vks-han-1.console.vngcloud.vn' metadata={'source': './../../data/vks/vks-la-gi.md'}\n",
      "page_content='VKS\n",
      "\n",
      "VKS (VNGCloud Kubernetes Service) là một dịch vụ được quản lý trên VNGCloud giúp bạn đơn giản hóa quá trình triển khai và quản lý các ứng dụng dựa trên container. Kubernetes là một nền tảng mã nguồn mở được phát triển bởi Google, được sử dụng rộng rãi để quản lý và triển khai các ứng dụng container trên môi trường phân tán.\n",
      "\n",
      "{% embed url=\"https://www.youtube.com/watch?v=t272u0uU8dU\" %}' metadata={'source': './../../data/vks/README.md'}\n",
      "page_content='Sử dụng VKS với Terraform\n",
      "\n",
      "Terraform là gì?\n",
      "\n",
      "Terraform là một cơ sở hạ tầng nguồn mở dưới dạng công cụ mã cho phép người dùng quản lý cơ sở hạ tầng của họ một cách dễ dàng và hiệu quả trên các nền tảng đám mây khác nhau, chẳng hạn như VNG Cloud, AWS, Google Cloud và Azure. Máy chủ Terraform đề cập đến phiên bản của công cụ Terraform đang chạy trên một máy chủ hoặc máy cụ thể. Đây là nơi mã cơ sở hạ tầng được viết và thực thi, cho phép người dùng tạo, sửa đổi và hủy tài nguyên trên nền tảng đám mây.\n",
      "\n",
      "Bản thân Terraform không có giao diện người dùng đồ họa, thay vào đó người dùng tương tác với nó bằng giao diện dòng lệnh. Terraform yêu cầu tài khoản và khóa của nhà cung cấp đám mây được định cấu hình cùng với tệp cấu hình Terraform để thực thi cơ sở hạ tầng dưới dạng mã. Ngoài ra, Terraform có thể hoạt động trong môi trường nhóm nơi nhiều người dùng có thể cộng tác trên cùng một cơ sở mã cơ sở hạ tầng, khiến nó trở thành một công cụ mạnh mẽ và linh hoạt để quản lý cơ sở hạ tầng đám mây.\n",
      "\n",
      "Các bước thực hiện\n",
      "\n",
      "Để khởi tạo một Cluster Kubernetes bằng Terraform, bạn cần thực hiện các bước sau:\n",
      "\n",
      "Truy cập IAM Portal tại đây, thực hiện tạo Service Account với quyền hạn VKS Full Access. Cụ thể, tại trang IAM, bạn có thể:\n",
      "\n",
      "Chọn \"Create a Service Account\", điền tên cho Service Account và nhấn Next Step để gắn quyền cho Service Account.\n",
      "\n",
      "Tìm và chọn Policy: VKSFullAccess sau đó nhấn \"Create a Service Account\" để tạo Service Account, Policy: VKSFullAccess do VNG Cloud tạo ra, bạn không thể xóa các policy này.\n",
      "\n",
      "Sau khi tạo thành công bạn cần phải lưu lại Client_ID và Secret_Key của Service Account để thực hiện bước tiếp theo.\n",
      "\n",
      "Truy cập VKS Portal tại đây, thực hiện Activate dịch vụ VKS ở tab Overview. Hãy chờ đợi tới khi chúng tôi khởi tạo thành công tài khoản VKS của bạn.\n",
      "\n",
      "Cài đặt Terraform:\n",
      "\n",
      "Tải xuống và cài đặt Terraform cho hệ điều hành của bạn từ https://developer.hashicorp.com/terraform/install.\n",
      "\n",
      "Khởi tạo cấu hình Terraform:\n",
      "\n",
      "Tạo tệp variable.tf và khai báo thông tin Service Account trong file này.\n",
      "\n",
      "Tạo tệp main.tf và định nghĩa các tài nguyên Kubernetes Cluster mà bạn muốn tạo.\n",
      "\n",
      "Ví dụ:\n",
      "\n",
      "Tệp variable.tf:bạn cần thay thế Client ID và Client Secret đã khởi tạo ở bước 1 ở file này.\n",
      "\n",
      "variable \"client_id\" { type = string default = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" } variable \"client_secret\" { type = string default = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" }\n",
      "\n",
      "Trên file main.tf, bạn cần có thể thêm resource để tạo Cluster/ Node Group:\n",
      "\n",
      "Tạo Cluster my-vks-cluster và Node Group my-nodegroup độc lập:\n",
      "\n",
      "```markup resource \"vngcloud_vks_cluster\" \"primary\" { name = \"my-cluster\" cidr = \"172.16.0.0/16\" vpc_id = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" }\n",
      "\n",
      "resource \"vngcloud_vks_cluster_node_group\" \"primary\" { name= \"my-nodegroup\" ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" cluster_id= vngcloud_vks_cluster.primary.id } ```\n",
      "\n",
      "Tạo Cluster với Default Node Group\n",
      "\n",
      "markup resource \"vngcloud_vks_cluster\" \"primary\" { name = \"my-cluster\" cidr = \"172.16.0.0/16\" vpc_id = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" node_group { name= \"my-nodegroup\" ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" } }\n",
      "\n",
      "{% hint style=\"info\" %} Chú ý:\n",
      "\n",
      "Chúng tôi khuyên bạn nên tạo và quản lý các Cluster, Node Group dưới dạng resource riêng biệt, như trong ví dụ bên dưới. Điều này cho phép bạn thêm hoặc xóa các Node Group mà không cần tạo lại toàn bộ Cluster. Nếu bạn khai báo trực tiếp Node Group Default trong tài nguyên vngcloud_vks_cluster, bạn không thể xóa chúng mà không tạo lại chính Cluster đó.\n",
      "\n",
      "Trong file main.tf, để khởi tạo một cluster với một node group thành công, bạn bắt buộc cần nhập thông tin của 4 field sau:\n",
      "\n",
      "vpc_id = \"net-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" subnet_id = \"sub-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" ssh_key_id= \"ssh-xxxxxxxx-xxxx-xxxxx-xxxx-xxxxxxxxxxxx\" {% endhint %}\n",
      "\n",
      "Các ví dụ tham khảo\n",
      "\n",
      "Example Usage 1 - Create a Cluster with Network type CALICO OVERLAY and a Node Group with AutoScale Mode\n",
      "\n",
      "``` terraform { required_providers { vngcloud = { source = \"vngcloud/vngcloud\" version = \"1.2.7\" } } } resource \"vngcloud_vks_cluster\" \"primary\" { name = \"cluster-demo\" description = \"Cluster create via terraform\" version = \"v1.29.1\" cidr = \"172.16.0.0/16\" enable_private_cluster = false network_type = \"CALICO\" vpc_id = \"net-70ef12d4-d619-43fc-88f0-1c1511683123\" subnet_id = \"sub-0725ef54-a32e-404c-96f2-34745239c123\" enabled_load_balancer_plugin = true enabled_block_store_csi_plugin = true }\n",
      "\n",
      "resource \"vngcloud_vks_cluster_node_group\" \"primary\" { cluster_id = vngcloud_vks_cluster.primary.id name = \"nodegroup1\" num_nodes = 3 auto_scale_config { min_size = 0 max_size = 5 } upgrade_config { strategy = \"SURGE\" max_surge = 1 max_unavailable = 0 } image_id = \"img-108b3a77-ab58-4000-9b3e-190d0b4b07fc\" flavor_id = \"flav-9e88cfb4-ec31-4ad4-8ba5-243459f6d123\" disk_size = 50 disk_type = \"vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018\" enable_private_nodes = false ssh_key_id= \"ssh-f923c53c-cba7-4131-9f86-175d04ae2123\" security_groups = [\"secg-faf05344-fbd6-4f10-80a2-cda08d15ba5e\"] labels = { \"test\" = \"terraform\" } taint { key = \"key1\" value = \"value1\" effect = \"PreferNoSchedule\" } } ```\n",
      "\n",
      "Example Usage 2 - Create a Cluster with Network type CILIUM OVERLAY and a Node Group with AutoScale Mode\n",
      "\n",
      "``` terraform { required_providers { vngcloud = { source = \"vngcloud/vngcloud\" version = \"1.2.7\" } } } resource \"vngcloud_vks_cluster\" \"primary\" { name = \"cluster-demo\" description = \"Cluster create via terraform\" version = \"v1.29.1\" cidr = \"172.16.0.0/16\" enable_private_cluster = false network_type = \"CILIUM_OVERLAY\" vpc_id = \"net-70ef12d4-d619-43fc-88f0-1c1511683123\" subnet_id = \"sub-0725ef54-a32e-404c-96f2-34745239c123\" enabled_load_balancer_plugin = true enabled_block_store_csi_plugin = true }\n",
      "\n",
      "resource \"vngcloud_vks_cluster_node_group\" \"primary\" { cluster_id = vngcloud_vks_cluster.primary.id name = \"nodegroup1\" num_nodes = 3 auto_scale_config { min_size = 0 max_size = 5 } upgrade_config { strategy = \"SURGE\" max_surge = 1 max_unavailable = 0 } image_id = \"img-108b3a77-ab58-4000-9b3e-190d0b4b07fc\" flavor_id = \"flav-9e88cfb4-ec31-4ad4-8ba5-243459f6d123\" disk_size = 50 disk_type = \"vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018\" enable_private_nodes = false ssh_key_id= \"ssh-f923c53c-cba7-4131-9f86-175d04ae2123\" security_groups = [\"secg-faf05344-fbd6-4f10-80a2-cda08d15ba5e\"] labels = { \"test\" = \"terraform\" } taint { key = \"key1\" value = \"value1\" effect = \"PreferNoSchedule\" } } ```\n",
      "\n",
      "Example Usage 3 - Create a Cluster with Network type CILIUM VPC Native Routing and a Node Group with AutoScale Mode\n",
      "\n",
      "``` terraform { required_providers { vngcloud = { source = \"vngcloud/vngcloud\" version = \"1.2.7\" } } } resource \"vngcloud_vks_cluster\" \"primary\" { name = \"cluster-demo\" description = \"Cluster create via terraform\" version = \"v1.29.1\" enable_private_cluster = false network_type = \"CILIUM_NATIVE_ROUTING\" vpc_id = \"net-70ef12d4-d619-43fc-88f0-1c1511683123\" subnet_id = \"sub-0725ef54-a32e-404c-96f2-34745239c123\" secondary_subnets = [\"10.200.27.0/24\", \"10.200.28.0/24\"] node_netmask_size = 25 enable_service_endpoint = false enabled_load_balancer_plugin = true enabled_block_store_csi_plugin = true }\n",
      "\n",
      "resource \"vngcloud_vks_cluster_node_group\" \"primary\" { cluster_id = vngcloud_vks_cluster.primary.id name = \"nodegroup1\" num_nodes = 3 auto_scale_config { min_size = 0 max_size = 5 } upgrade_config { strategy = \"SURGE\" max_surge = 1 max_unavailable = 0 } image_id = \"img-108b3a77-ab58-4000-9b3e-190d0b4b07fc\" flavor_id = \"flav-9e88cfb4-ec31-4ad4-8ba5-243459f6d123\" subnet_id = \"sub-cddd7ffa-be05-4698-9b3d-794e1adfcbce\" secondary_subnets = [\"10.200.27.0/24\", \"10.200.28.0/24\"] disk_size = 50 disk_type = \"vtype-61c3fc5b-f4e9-45b4-8957-8aa7b6029018\" enable_private_nodes = false ssh_key_id= \"ssh-f923c53c-cba7-4131-9f86-175d04ae2123\" security_groups = [\"secg-faf05344-fbd6-4f10-80a2-cda08d15ba5e\"] labels = { \"test\" = \"terraform\" } taint { key = \"key1\" value = \"value1\" effect = \"PreferNoSchedule\" } } ```\n",
      "\n",
      "{% hint style=\"info\" %} Chú ý:\n",
      "\n",
      "Để lấy image_id bạn mong muốn sử dụng, bạn có thể truy cập vào VKS Portal, chọn menu System Image và lấy ID mà bạn mong muốn hoặc lấy thông tin này tại đây.\n",
      "\n",
      "Để lấy flavor_id bạn mong muốn sử dụng cho Node group của bạn, vui lòng lấy ID tại đây. {% endhint %}\n",
      "\n",
      "Khởi chạy Terraform command\n",
      "\n",
      "Sau khi hoàn tất các thông tin trên, thực hiện chạy lệnh bên dưới:\n",
      "\n",
      "terraform init\n",
      "\n",
      "Sau đó, bạn để xem những thay đổi sẽ được áp dụng trên những resource mà terraform đang quản lý bạn có thể chạy:\n",
      "\n",
      "terraform plan\n",
      "\n",
      "Cuối cùng bạn chọn chạy dòng lệnh:\n",
      "\n",
      "terraform apply\n",
      "\n",
      "Chọn YES để thực hiện việc khởi tạo Cluster, Node Group thông qua Terraform\n",
      "\n",
      "Kiểm tra Cluster vừa tạo trên giao diện VNG Cloud Portal\n",
      "\n",
      "Sau khi khởi tạo thành công Terraform, bạn có thể lên VKS Portal để xem thông tin Cluster vừa tạo.\n",
      "\n",
      "Tham khảo thêm về cách sử dụng Terraform để làm việc với VKS tại đây.\n",
      "\n",
      "Một số lưu ý khi sử dụng VKS với Terraform:\n",
      "\n",
      "Khi sử dụng Terraform để khởi tạo Cluster và Node Group trên hệ thống VKS, nếu bạn thay đổi một trong các field sau, hệ thống sẽ tự động xóa Node Group/ Cluster và thực hiện khởi tạo lại Node Group/ Cluster theo thông số mới tương ứng. Việc xóa sẽ được thực hiện trước khi tạo Node Group/ Cluster mới.\n",
      "\n",
      "Đỗi với resource vngcloud_vks_cluster, các field khi bạn thay đổi hệ thống sẽ xóa Cluster và tạo lại bao gồm:\n",
      "\n",
      "name\n",
      "\n",
      "description\n",
      "\n",
      "enable_private_cluster\n",
      "\n",
      "network_type\n",
      "\n",
      "vpc_id\n",
      "\n",
      "subnet_id\n",
      "\n",
      "cidr\n",
      "\n",
      "enabled_load_balancer_plugin\n",
      "\n",
      "enabled_block_store_csi_plugin\n",
      "\n",
      "node_group\n",
      "\n",
      "secondary_subnets\n",
      "\n",
      "node_netmask_size\n",
      "\n",
      "Đỗi với resource vngcloud_vks_cluster_node_group, các field khi bạn thay đổi hệ thống sẽ xóa Cluster và tạo lại bao gồm:\n",
      "\n",
      "cluster_id\n",
      "\n",
      "name\n",
      "\n",
      "flavor_id\n",
      "\n",
      "disk_size\n",
      "\n",
      "disk_type\n",
      "\n",
      "enable_private_nodes\n",
      "\n",
      "ssh_key_id\n",
      "\n",
      "secondary_subnets\n",
      "\n",
      "enabled_encryption_volume\n",
      "\n",
      "subnet_id\n",
      "\n",
      "Để chỉ định hệ thống tạo cluster/node group mới rồi mới thực hiện xóa cluster/ node group cũ, bạn có thể thêm tham số lifecycle { create_before_destroy = true }vào file main.tf của bạn. Cụ thể:\n",
      "\n",
      "Đỗi với resource vngcloud_vks_cluster\n",
      "\n",
      "``` resource \"vngcloud_vks_cluster\" \"example\" { # ...\n",
      "\n",
      "lifecycle { create_before_destroy = true } } ```\n",
      "\n",
      "Đỗi với resource vngcloud_vks_cluster_node_group\n",
      "\n",
      "``` resource \"vngcloud_vks_cluster_node_group\" \"example\" { # ...\n",
      "\n",
      "lifecycle { create_before_destroy = true } } ```' metadata={'source': './../../data/vks/su-dung-vks-voi-terraform.md'}\n",
      "page_content='Bắt đầu với VKS\n",
      "\n",
      "Bạn có thể sử dụng các hướng dẫn sau đây để bắt đầu làm việc với VKS. Trong quá trình sử dụng, nếu gặp vấn đề gì với dịch vụ, bạn hãy vui lòng liên hệ với VNG Cloud qua email support@vngcloud.vn hoặc hotline 19001549.' metadata={'source': './../../data/vks/bat-dau-voi-vks/README.md'}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
